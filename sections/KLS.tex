In this section, we present the original context and motivation for the construction of stochastic 
localizations: the KLS conjecture. The KLS conjecture is a conjecture which roughly states that all 
log-concave measures on \(\mathbb{R}^n\) are concentrated in a way similar to a Gaussian measure. 
To tackle this problem, Eldan in \cite{Eldan_2013} introduced stochastic localizations  
in order to reduce the KLS conjecture to a seemingly weaker conjecture (up to a logarithmic constant): 
the thin-Shell conjecture. More recently, stochastic localization was applied by Chen in \cite{Chen_2020} 
to provide an almost constant lower bound for the KLS conjecture. We will now describe 
a proof of Eldan's original reduction of the KLS conjecture to the thin-Shell conjecture.

The method presented in this section is due to Lee and Vempala \cite{Lee_2016} and reformulated in 
the language of concentration by Eldan \cite{Eldan_notes}.

\subsection{Concentration}

Let us quickly introduce some preliminary definitions required to state the aforementioned conjectures. 

\begin{definition}[Concentration, \cite{Eldan_notes}]
  Let \(\mu\) be a measure on \(\mathbb{R}^n\), then \(\mu\) is said to be \(C\)-(inversely)-concentrated if
  for all \(1\)-Lipschitz function \(\phi : \mathbb{R}^n \to \mathbb{R}\), 
  \begin{equation}
    \text{Var}_\mu[\phi] = \text{Var}_{X \sim \mu}[\phi(X)] \le C^2.
  \end{equation}
  We denote the least possible such \(C\) by \(C^\mu_{\text{con}}\).
\end{definition}

Heuristically, the concentration is a measure of the variance of \(\mu\) in each direction and 
in particular equals the variance in the 1-dimensional case. This is illustrated by the following proposition. 

\begin{proposition}\label{prop:concentration}
  Let \(X\) be a \(\mathbb{R}^n\)-valued random variable. Then for all \(K\)-Lipschitz function 
  \(\phi : \mathbb{R}^n \to \mathbb{R}\),
  \[\text{Var}[\phi(X)] \le K^2 \text{Var}[X].\]
\end{proposition}
\begin{proof}
  We first prove the proposition in the case that \(\mathbb{E}[X] = 0\). 

  Let \(X'\) be a i.i.d. copy of \(X\) on the same probability space. Then for all \(K\)-Lipschitz 
  function \(\phi\), we have 
  \begin{align*}
    2 \text{Var}[\phi(X)] & = \text{Var}[\phi(X) - \phi(X')] & (\text{i.i.d.})\\
      & = \mathbb{E}[(\phi(X) - \phi(X'))^2] - \mathbb{E}[\phi(X) - \phi(X')]^2 & \\
      & = \mathbb{E}[(\phi(X) - \phi(X'))^2] & (\text{identically distributed}) \\
      & \le K^2 \mathbb{E}[\|X - X'\|^2] & (\text{as \(\phi\) is \(K\)-Lipschitz}) \\
      & = K^2 \mathbb{E}[X^T X + X'^T X' - X^T X' - X'^T X] & \\
      & = 2K^2 \text{Var}[X] - 2K^2 \text{Cov}(X, X') = 2K^2 \text{Var}[X]. & (\text{independence})
  \end{align*}
  implying \(\text{Var}[\phi(X)] \le K^2 \text{Var}[X]\) as claimed.

  For general \(X\), by defining \(\phi'(x) := \phi(x + \mathbb{E}[X])\), we can apply the 0 mean 
  case to \(X - \mathbb{E}X\) and \(\phi'\) to obtain
  \[\text{Var}[\phi(X)] = \text{Var}[\phi'(X - \mathbb{E}[X])] \le 
    K^2 \text{Var}[X - \mathbb{E}[X]] = K^2 \text{Var}[X]\]
  as required.
\end{proof}

With this proposition in mind, it is clear that for \(\mathbb{R}\)-valued random variables \(X\), 
its law \(\mu\) has concentration \(C^\mu_{\text{con}} = \text{Var}[X]\). 

We note that the definition we are presenting here is slightly non-standard. However, utilizing the 
following remarkable result due to Milman, we show that this definition is equivalent to the following 
definitions in a specific sense.

\begin{definition}[Exponential concentration, \cite{Milman_2018}]
  Given a measure \(\mu\) on \(\mathbb{R}^n\), we say \(\mu\) has exponential concentration if 
  there exists some \(c, D > 0\) such that for all 1-Lipschitz function 
  \(\phi : \mathbb{R}^n \to \mathbb{R}, t > 0\), we have
  \begin{equation}
    \mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) \le c e^{-Dt}.
  \end{equation}
  Fixing \(c = 1\), we denote the largest possible \(D\) as \(D^\mu_{\text{exp}}\).
\end{definition}

\begin{definition}[First-moment concentration, \cite{Milman_2018}]
  Again, given \(\mu\) a measure on \(\mathbb{R}^n\), we say \(\mu\) has first-moment concentration 
  if there exists some \(D > 0\) such that for all 1-Lipschitz function 
  \(\phi : \mathbb{R}^n \to \mathbb{R}\), we have
  \begin{equation}
    \mathbb{E}_\mu[|\phi - \mathbb{E}_\mu[\phi]|] \le \frac{1}{D}.
  \end{equation}
  We denote the largest possible \(D\) by \(D^\mu_{\text{FM}}\).
\end{definition}

It is clear that exponential concentration implies first-moment concentration. Indeed, if \(\mu\) 
has exponential concentration with constant \(D\) (taking \(c = 1\)), then by the tail probability formula,
\begin{align*}
  \mathbb{E}_\mu[|\phi - \mathbb{E}_\mu[\phi]|] 
  & = \int_0^\infty \mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) \dd t \le \int_0^\infty e^{-Dt} \dd t 
   = \frac{1}{D}.
\end{align*}

On the other hand, Milman showed that for log-concave measures on \(\mathbb{R}^n\), exponential concentration 
and first-moment concentration are equivalent in the following sense. 

\begin{theorem}[Milman, \cite{Milman_2008}]\label{thm:milman}
  For all log-concave measure \(\mu\) on \(\mathbb{R}^n\), \(\mu\) has exponential concentration 
  if and only if \(\mu\) has first-moment concentration. Furthermore, 
  \(D^\mu_{\text{exp}} \simeq D^\mu_{\text{FM}}\) where we write \(A \simeq B\) if there exists 
  universal constants \(C_1, C_2 > 0\) such that \(C_1 A \le B \le C_2 A\).
\end{theorem}

With this theorem in mind, we establish the following correspondence.

\begin{proposition}\label{thm:equiv}
  For all measures \(\mu\) on \(\mathbb{R}^n\), we have 
  \[\text{Exponentially concentrated \(\implies\) Concentrated \(\implies\) First-moment concentrated}\]
  and \(D^\mu_{\text{exp}} \le \sqrt{2} (C^\mu_{\text{con}})^{-1}\) and 
  \((2C^\mu_{\text{con}})^{-1} \le D^\mu_{\text{FM}}\).
  Hence, if \(\mu\) is log-concave, 
  \(D^\mu_{\text{exp}} \simeq D^\mu_{\text{FM}} \simeq (C^\mu_{\text{con}})^{-1}\).
\end{proposition}
\begin{proof}
  Assume first that \(\mu\) is \(C\)-concentrated. Then by the Chebyshev inequality, we have 
  \[\mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) 
      \le \frac{1}{t^2}\text{Var}_\mu[\phi] 
      \le \frac{C^2}{t^2},\]
  for all 1-Lipschitz \(\phi\). 
  Thus, by tail probability,
  \begin{align*}\mathbb{E}_\mu[|\phi - \mathbb{E}_\mu[\phi]|] 
    & = \int_0^\infty \mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) \dd t\\ 
    & \le \inf_{a > 0} \left\{\int_0^a \mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) \dd t + C^2 \int_a^\infty \frac{1}{t^2} \dd t\right\}\\ 
    & \le \inf_{a > 0} \left\{a + \frac{C^2}{a}\right\} = 2C,
  \end{align*}
  implying \(\mu\) is first-moment concentrated with respect to the constant \((2C)^{-1}\).

  On the other hand, if \(\mu\) is exponential concentration with some constant \(D\), then 
  again by the tail probability, 
  \[\text{Var}_\mu[\phi] = \int_0^\infty \mu((\phi - \mathbb{E}_\mu[\phi])^2 \ge t) \dd t
      \le \int_0^\infty e^{-D \sqrt{t}} \dd t = \frac{2}{D^2}\]
  implying \(\mu\) is \(\sqrt{2} D^{-1}\)-concentrated.
\end{proof}

\subsection{Example: concentration of the Gaussian}

Given a class of measures, it is in general \textit{not} true that the concentration coefficient of 
measures of said class is dimension invariant. However, this turns out to be the case for the Gaussian 
measures.

\begin{theorem}[Concentration of Gaussian measures]\label{thm:gaussian_conc}
  Denoting \(\gamma^n\) the standard Gaussian measure on \(\mathbb{R}^n\), \(\gamma^n\) is \(C\)-concentrated 
  for some constant \(C\) which is independent of \(n\). That is, for all 1-Lipschitz 
  \(\phi : \mathbb{R}^n \to \mathbb{R}\), \(\text{Var}_{\gamma^n}[\phi] \le C^2\).
\end{theorem}

This fact motivate the KLS conjecture which hypothesized that 
this invariance holds for a larger class of measures known as the log-concave measures. We will 
for completeness give a brief proof (based on theorem 1.7.1 in \cite{Bogachev_1998}) of the above theorem taking \(C = \pi / 2\) 
(although one can further show Gaussian measures are 1-concentrated).

To prove this theorem we first observe the following elementary property of the Gaussian measure.

\begin{lemma}\label{lem:gaussian_inner}
  For all \(x \in \mathbb{R}^n\), we have 
  \(\mathbb{E}_{\gamma^n}[|\langle x, \cdot \rangle|^2] = \|x\|^2\).
\end{lemma}
\begin{proof}
  Defining \(f_x := \langle x, \cdot \rangle\), we have 
  \(\mathbb{E}_{\gamma^n}[|\langle x, \cdot \rangle|^2] = \mathbb{E}_{f_x^* \gamma^n}[|\cdot |^2]\) where 
  \(f_x^* \gamma^n \sim \mathcal{N}(0, \|x\|^2)\) as \(f_x^*\) is linear. Hence, the result follows as 
  \(\mathbb{E}_{f_x^* \gamma^n}[|\cdot |^2] = \text{Var}_{f_x^* \gamma^n}[\text{id}] = \|x\|^2\).
\end{proof}

With this in mind, fixing a smooth 1-Lipschitz function \(\phi : \mathbb{R}^n \to \mathbb{R}\) (we can assume 
smoothness since any 1-Lipschitz function can be uniformly approximated by smooth 1-Lipschitz functions), we will now attempt 
to bound \(\int \int |\phi(x) - \phi(y)|^2 \gamma^n(\dd x) \gamma^n(\dd y)\) by first bounding it by 
the integral of a inner product. In particular, for all \(x, y \in \mathbb{R}^n\), we observe
\begin{align*}
  |\phi(x) - \phi(y)| 
  & = \left|\int_0^{\pi / 2}\partial_\theta \phi(x \sin \theta + y \cos \theta) \dd \theta\right|
    \le \int_0^{\pi / 2}|\partial_\theta \phi(x \sin \theta + y \cos \theta)| \dd\theta\\
  & = \int_0^{\pi / 2} |\langle \nabla \phi(x \sin \theta + y \cos \theta), x \cos \theta - y \sin\theta\rangle| \dd \theta.
\end{align*}
Then, by rescaling \(\dd \theta\), we may apply Jensen's inequality resulting in
\begin{align*}
  |\phi(x) - \phi(y)|^2 
  & \le \left(\int_0^{\pi / 2} |\langle \nabla \phi(x \sin \theta + y \cos \theta), x \cos \theta - y \sin\theta\rangle| \dd \theta\right)^2\\
  & \le \frac{\pi}{2}\int_0^{\pi / 2} |\langle \nabla \phi(x \sin \theta + y \cos \theta), x \cos \theta - y \sin\theta\rangle|^2 \dd \theta
\end{align*}
Thus, we have 
\begin{align*}
  & \int \int |\phi(x) - \phi(y)|^2 \gamma^n(\dd x) \gamma^n(\dd y) \\
  & \le \frac{\pi}{2} \int_0^{\pi / 2} \dd \theta \int \int 
    |\langle \nabla \phi(x \sin \theta + y \cos \theta), x \cos \theta - y \sin\theta\rangle|^2
    \gamma^n(\dd x) \gamma^n(\dd y).
\end{align*}
Now, by substituting \(u = x \sin \theta + y\cos \theta, v = x \cos \theta - y \sin \theta\) (which Jacobian 
has determinant 1), we have
\begin{align*}
  \int \int |\phi(x) - \phi(y)|^2 \gamma^n(\dd x) \gamma^n(\dd y) 
  & \le \frac{\pi}{2} \int_0^{\pi / 2} \dd \theta \int \int |\langle \nabla \phi(u), v \rangle|^2 \gamma^n(\dd u) \gamma^n(\dd v)\\
  & \le \frac{\pi}{2}\int_0^{\pi / 2} \dd \theta \int \|\nabla \phi(u)\|^2 \gamma^n(\dd u)
\end{align*}
where the second inequality is due to lemma \ref{lem:gaussian_inner}. Hence, as \(\phi\) is 1-Lipschitz, 
\(\|\nabla \phi(u)\| \le 1\) for all \(u\) and thus, 
\[\int \int |\phi(x) - \phi(y)|^2 \gamma^n(\dd x) \gamma^n(\dd y) 
  \le \frac{\pi}{2}\int_0^{\pi / 2} \dd \theta \int \dd \gamma^n
  = \frac{\pi}{2} \cdot \frac{\pi}{2} = \frac{\pi^2}{4}.\]
The immediate consequence of this bound is that there exists some \(z \in \mathbb{R}^n\) such that 
\(\int |\phi(x) - z|^2 \gamma^n(\dd x) \le \pi^2 / 4\). Indeed, if no such \(z\) exists, 
then \(\int |\phi(x) - \phi(y)|^2 \gamma^n(\dd x) > \pi^2 / 4\) for all \(y\), implying
\(\int \int |\phi(x) - \phi(y)|^2 \gamma^n(\dd x) \gamma^n(\dd y) > \pi^2 / 4\) which 
contradicts the above bound. Hence, choosing such a \(z\), we conclude the proof of 
theorem~\ref{thm:gaussian_conc} since
\[\text{Var}_{\gamma^n}[\phi] = \min_{w \in \mathbb{R}^n} \int |\phi(x) - w|^2 \gamma^n(\dd x)
  \le \int |\phi(x) - z|^2 \gamma^n(\dd x) \le \left(\frac{\pi}{2}\right)^2\]
as required.

\subsection{The KLS and thin-shell conjecture}

As alluded to in the previous section, the KLS conjecture suggests that any log-concave measure on 
\(\mathbb{R}^n\) admits concentration roughly similar to that of the Gaussian measure. However, unlike the Gaussian, as the 
concentration of measures is not invariant under linear functions, it is clear that the KLS 
conjecture would not hold without a suitable normalization. This leads us to the following formulation 
of the KLS conjecture.

\begin{conjecture}[Kannan-Lovász-Simonovitz, \cite{Eldan_notes}]\label{conj:KLS}
  Denoting \(\mathscr{M}^n_{\text{con}}\) the set of all log-concave probability measures \(\mu\) on 
  \(\mathbb{R}^n\) satisfying \(\text{Var}_\mu[T] \le 1\) for all 1-Lipschitz linear maps 
  \(T : \mathbb{R}^n \to \mathbb{R}\), there exists a \textit{universal} constant \(C\) 
  (i.e. does not depend on any parameter and in particular is independent of the dimension \(n\)) such that for all 
  \(\mu \in \mathscr{M}^n_{\text{con}}\), \(\mu\) is \(C\)-concentrated.
\end{conjecture}

Assuming the normalization condition, we remark a trivial bound on the concentration of \(\mu\):
Taking \(\pi_i : \mathbb{R}^n \to \mathbb{R}\) to be the projection into the \(i\)-th coordinate, 
as \(\pi_i\) is linear and 1-Lipschitz for all \(i\), we have \(\text{Var}_{X \sim \mu}[X_i] \le 1\).
Thus, 
\begin{align*}
  \text{Var}_{X \sim \mu}[X] & = \mathbb{E}_{X \sim \mu}[X^T X] - \mathbb{E}_{X \sim \mu}[X]^T \mathbb{E}_{X \sim \mu}[X]\\ 
    & = \sum_{i = 1}^n (\mathbb{E}_{X \sim \mu}[X_i^2] - (\mathbb{E}_{X \sim \mu}X_i)^2)
      = \sum_{i = 1}^n \text{Var}_\mu[\pi_i] \le n.
\end{align*}
Hence, by proposition~\ref{prop:concentration}, we have that \(\text{Var}_\mu[\phi] \le n\) for all 
1-Lipschitz \(\phi : \mathbb{R}^n \to \mathbb{R}\) implying \(\mu\) is \(\sqrt{n}\)-concentrated.

\begin{conjecture}[Thin-shell, \cite{Eldan_2013}]
  Taking \(\mathscr{M}^n_{\text{con}}\) as above, there exists a universal constant \(C\) such that 
  for all \(\mu \in \mathscr{M}^n_{\text{con}}\), we have
  \[\sqrt{\text{Var}_\mu[\|\cdot\|]} \le C.\]
\end{conjecture}

Since the norm function is 1-Lipschitz, it is clear that the thin-shell conjecture is 
weaker than that of the KLS conjecture. On the other hand, as we shall describe in the following subsections, 
as a consequence of the theory of stochastic localization, Eldan \cite{Eldan_2013} provides a reduction 
of the KLS conjecture to the thin-shell conjecture up to logarithmic factors. 

\begin{theorem}[Eldan, \cite{Eldan_2013}]\label{thm:KLS_to_TS}
  Denoting \(\mathscr{M}^n_{\text{con}}\) as above, we define 
  \[C^n_{\text{con}} := \inf \left\{C \mid \text{\(\forall \mu \in \mathscr{M}^n_{\text{con}},\) 
    \(\mu\) is \(C\)-concentrated}\right\},\]
  and 
  \[C^n_{\text{TS}} := \inf \left\{C \mid \text{\(\forall \mu \in \mathscr{M}^n_{\text{con}},\) 
      \(\sqrt{\text{Var}_\mu[\|\cdot\|]} \le C\)}\right\}
      = \sup_{\mu \in \mathscr{M}^n_{\text{con}}} \sqrt{\text{Var}_\mu[\|\cdot\|]},\]
  we have,
  \[C^n_{\text{TS}} \le C^n_{\text{con}} \lesssim C^n_{\text{TS}} \log n.\]
\end{theorem}

% We remark that while the constants in theorem~\ref{thm:KLS_to_TS} depends on the dimension \(n\), the 
% KLS conjecture is reduced to the thin-shell conjecture in the sense that it suffices to show 
% \(\sup_n C^n_{\text{TS}} \log n < \infty\). Then, the universal bound for the KLS conjecture is obtained 
% by taking the constant \(C = \sup_n C^n_{\text{con}} \le \sup_n C^n_{\text{TS}} \log n < \infty\).

The stochastic localization scheme has been wildly successful in making progress towards the KLS conjecture. 
Modifying the original arguments by Eldan, Lee and Vempala \cite{Lee_2016} obtained the bound 
\(C_{\text{con}}^n \lesssim n^{-1 / 4}\). Further modifying their arguments, a recent breakthrough by 
Chen \cite{Chen_2020} improves the bound providing the following theorem. 

\begin{theorem}[Chen, \cite{Chen_2020}]
  \(\log C_{\text{con}}^n \lesssim \sqrt{\log n \log \log n}\) and so \(C_{\text{con}}^n = n^{-o(1)}\).
\end{theorem}


\subsubsection{Equivalent formulation of the KLS conjecture}

While we have formulated the KLS conjecture using the language of concentration, the conjecture itself 
was originally formulated as an isoperimetric problem. For completeness of this exposition, we shall 
briefly present these equivalent formulations here.  

The isoperimetric problem is the problem in finding 
the set of unit volume with minimum surface area. In the case of the \(\mathbb{R}^n\) equipped with the 
Lebesgue measure, we have known since the ancient Greeks \cite{iso_hist} that the solution is the unit ball. 
With this in mind, it is natural for us to generalize the problem for arbitrary measures. 

\begin{definition}[Minkowski's boundary measure]
  Given a measure \(\mu\) on \(\mathbb{R}^n\) and a Borel set \(A \subseteq <\mathbb{R}^n\), the Minkowski's 
  boundary measure of \(A\), 
  \[\mu^+(\partial A) := \liminf_{\epsilon \downarrow 0} \frac{\mu(A_\epsilon) - \mu(A)}{\epsilon}.\]
  where \(A_\epsilon := \{x \in \mathbb{R}^n \mid \text{dist}(x, A) \le \epsilon\}\) is the 
  \(\epsilon\)-thickening of some Borel set \(A\). 
\end{definition}

The isoperimetric problem for the measure \(\mu\) then becomes the problem of finding the set \(A\) 
satisfying \(\mu(A) = 1\) with minimum \(\mu^+(\partial A)\). 

\begin{definition}[Cheeger's inequality, \cite{Milman_2008}]
  Given a measure \(\mu\) on \(\mathbb{R}^n\), we say \(\mu\) satisfy Cheeger's inequality 
  if there exists some \(D\) such that for all \(A\),
  \[\mu(A) \wedge \mu(A^c) \le D\mu^+(A).\]
  We call the largest such \(D\) the inverse Cheeger's constant (or the inverse isoperimetric constant) and 
  denote it by \(D^\mu_{\text{C}}\).
\end{definition}

With these definitions, the KLS conjecture can be equivalently reformulated as the following.

\begin{conjecture}[KLS, \cite{Eldan_2013}]
  Denoting \(\mathscr{M}^n_{\text{iso}}\) the set of all log-concave and isotropic probability measures \(\mu\) on 
  \(\mathbb{R}^n\), there exists a \textit{universal} constant \(D\) such that for all 
  \(\mu \in \mathscr{M}^n_{\text{iso}}\), \(\mu\) satisfy the Cheeger's inequality with constant \(D\).
\end{conjecture}

The equivalence of the reformulation follows by completing theorem~\ref{thm:equiv} with two additional 
equivalences.

\begin{theorem}[Milman, \cite{Milman_2008}]\label{thm:milman2}
  For all log-concave measure \(\mu\) on \(\mathbb{R}^n\), the following are equivalent
  \begin{itemize}
    \item \(\mu\) has exponential concentration with constant \(D^\mu_\text{exp}\).
    \item \(\mu\) has first-moment concentration with constant \(D^\mu_\text{FM}\).
    \item \(\mu\) satisfy the Cheeger's inequality with constant \(D^\mu_\text{C}\).
    \item \(\mu\) satisfy the Poincaré inequality: there exists some \(D > 0\) such that for all 
      smooth \(\phi : \mathbb{R}^n \to \mathbb{R}\) satisfying \(\int \phi \dd\mu = 0\), we have
      \[D \cdot \text{Var}_\mu[\phi] \le \mathbb{E}_\mu[\|\nabla \phi\|^2] = \int \|\nabla \phi\|^2 \dd\mu.\]
      We denote the largest such \(D\) by \(D^\mu_{\text{P}}\).
  \end{itemize}
  Furthermore, \(D^\mu_{\text{exp}} \simeq D^\mu_{\text{FM}} \simeq D^\mu_{\text{P}} \simeq D^\mu_{\text{C}}\).
\end{theorem}

With this theorem and proposition \ref{thm:equiv} in mind, it is clear that the KLS conjecture can be 
instead formulated with any of these inequalities instead. We also showcase one of these formulations here 
using the Poincaré inequality:

\begin{conjecture}[KLS, \cite{Eldan_2013}]
  Denoting \(\mathscr{M}^n_{\text{iso}}\) as above, there exist a \textit{universal} constant \(D\) 
  such that for all \(\mu \in \mathscr{M}^n_{\text{iso}}\), \(\mu\) satisfy the Poincaré inequality 
  with constant \(D\).
\end{conjecture}

We remark that isotropic measures satisfy the normalization condition in conjecture~\ref{conj:KLS}. Indeed, if 
\(T : \mathbb{R}^n \to \mathbb{R}\) is a 1-Lipschitz linear function, i.e. is of the form \(v \mapsto w^T v + d\) 
for some \(w \in S^{n - 1}\) and \(d \in \mathbb{R}\), then we have
\[\text{Var}_{\mu}[T] = \text{Var}_{X \sim \mu}\left[\sum_{i = 1}^n w_i X_i + d\right] 
    = \sum_{i, j = 1}^n w_i w_j \text{Cov}_{X \sim \mu}(X_i, X_j) = \sum_{i = 1}^n w_i^2 = 1,\]
as \(\text{Cov}_{X \sim \mu}(X) = \text{id}\).

