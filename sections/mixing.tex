An application of stochastic localizations is used to prove mixing bounds for Markov processes. 
Focusing on the framework established by Chen and Eldan in \cite{Chen_2022}, we will present a method to 
compute a bound on the mixing time for a special class of Markov processes; namely the Markov chains 
associated with stochastic localizations. 
As a specific example, we will in this section also describe the application of this framework to the 
Glauber dynamics and as a result, allow us to recover the main theorem on spectral independence as 
presented by Anari, Liu and Oveis in \cite{Anari_2020}.

\subsection{Mixing bounds}

The motivation for Markov mixing bounds fundamentally comes from sampling. Suppose we wish to sample 
from some probability distribution $\mu$. A common method to achieve this to through the use of the 
Markov chain Monte Carlo (MCMC):
\begin{theorem}\label{thm:markov_conv}
  Given \((X_n)\) an irreducible positively recurrent homogenous Markov process on \(\mathcal{X}\) with
  stationary distribution \(\mu\), for any \(\phi : \mathcal{X} \to \mathbb{R}\) integrable, 
  \[\lim_{n \to \infty} \frac{1}{n} \sum_{k = 1}^n \phi(X_k) = \int \phi \dd \mu\]
  almost everywhere.  
\end{theorem}
With this theorem in mind, MCMC allows us to sample \(\mu\) by sampling from \((X_n)\) instead. It is in general 
not difficult to come up with such Markov processes, although it is often difficult to show its rate of convergence. 
This motivates the notion of mixing bounds which quantifies the time for which the Markov process takes before 
its law is approximately stationary.

\begin{definition}[Total variation mixing time]
  Given a probability measure \(\nu \in \mathcal{M}(\mathcal{X})\), a Markov kernel \(K\) with stationary 
  distribution \(\mu\) and some \(\epsilon > 0\), the \(\epsilon\)-total variation mixing time is defined as 
  \[t_{\text{mix}}(P, \epsilon, \mu) := \inf \{t \ge 0 \mid \|P^t\nu - \mu\|_{\text{TV}} < \epsilon\},\]
  Furthermore, we denote 
  \[t_{\text{mix}}(P, \epsilon) = \sup_{x \in \mathcal{X}} t_{\text{mix}}(P, \epsilon, \delta_x)\]
  the worst mixing time starting at a point.
\end{definition}

A standard method of analyzing the mixing times of Markov chains is through the use of the spectral gap. 

\begin{definition}[Spectral gap, \cite{Levin_2017}]
  The spectral gap of a Markov kernel \(K\) is defined to be 
  \[\mathfrak{gap}(K) := 1 - \sup\{\lambda \mid \lambda \text{ is an eigenvalue of } K, \lambda \neq 1\}.\]
\end{definition}

It is not difficult to show that 
\begin{equation}\label{eq:gap}
  \mathfrak{gap}(K) = 
    \inf_{\substack{\phi : \mathcal{X} \to \mathbb{R}\\ \int \phi \dd \mu = 0}} 1 
      - \frac{\int \phi K \phi \dd \mu}{\int \phi^2 \dd \mu}
    = \inf_{\phi : \mathcal{X} \to \mathbb{R}} 
      \frac{1}{2 \text{Var}_\mu[\phi]} \int (\phi(x) - \phi(y))^2 K(x, \dd y) \mu(\dd x),
\end{equation}
where \(\mu\) is the stationary measure of \(K\) and \(K\phi = \int \phi(y) K(\cdot, \dd y)\). We will take 
equation~\eqref{eq:gap} to be the defining property of the spectral gap in the case \(K\) is defined on 
a general state space.

\begin{theorem}[\cite{Levin_2017}]\label{thm:levin}
  Given a reversible and irreducible Markov chain with kernel \(K\) on the state space \(\mathcal{X}\) 
  with stationary distribution \(\mu\), denoting \(\mu_{\min} = \inf_{x \in \mathcal{X}} \mu(x)\), we have 
  \[t_{\text{mix}}(K, \epsilon) \le \left\lceil\frac{1}{\mathfrak{gap}(K)}
    \left(\frac{1}{2}\log\left(\frac{1}{\mu_{\min}}\right) + \log\left(\frac{1}{2\epsilon}\right)\right)\right\rceil.\]
\end{theorem}

We remark that this inequality is only meaningful whenever \(\mu_{\min} > 0\) and thus, this theorem is 
only meaningful for Markov chains on finite state space (and we can replace \(\inf\) with \(\min\)).

% A similar bound can also be established using the modified log-Sobolev inequality (MLSI).

% \begin{definition}[Entropy, \cite{Liu_2020}]
%   Given \(\phi : \mathcal{X} \to \mathbb{R}_{\ge 0}\) and a measure \(\mu\), we define the entropy of \(\phi\) 
%   with respect to \(\mu\) to be 
%   \[\text{Ent}_\mu[\phi] := \mathbb{E}_\mu\left[\phi \log\left(\frac{1}{\mathbb{E}_\mu[\phi]} \phi\right)\right]
%    = \int \phi \log \phi \dd \mu - \int \phi \dd \mu \log\left(\int \phi \dd \mu\right).\]
% \end{definition}

% \begin{definition}\todo{TODO}
  
% \end{definition}

\subsubsection{Ising model and Glauber dynamics}

\begin{definition}[Ising model]
  Given a graph \(G = (V, E)\), \(\beta > 0\) and \(h \in \mathbb{R}\), the Ising model on \(G\) 
  with inverse temperature \(\beta\) and external field \(h\) is the probability measure \(\mu_{\beta, h}\) on 
  \(\{-1, 1\}^V\) defined such that for all \(\sigma \in \{-1, 1\}^V\),
  \[\mu_{\beta, h}(\sigma) := 
    \frac{1}{Z} \exp\left\{\beta \sum_{xy \in E} \sigma_x \sigma_y + h \sum_{x \in V} \sigma_x\right\}\]
  where \(Z > 0\) is the normalizing constant.
\end{definition}

Heuristically, the Ising model measures the probability that a graph is in a specific configuration of 
up and down spins in which neighboring vertices are more likely to have the same spin. This ``likeliness'' 
is controlled by \(\beta\) in which a larger \(\beta\) means that neighboring vertices are more likely to
align. 

As illustrated by theorem~\ref{thm:markov_conv}, in order to sample from the Ising model, we can 
construct a Markov chain which has the Ising model as its stationary distribution. One such Markov chain 
is known as the Glauber dynamics.

\begin{definition}[Glauber dynamics]
  Given a measure \(\mu\) of \(\{-1, 1\}^n\), the Glauber dynamics of \(\mu\) is the Markov chain 
  with kernel 
  \begin{equation}\label{eq:glauber_ker}
    K(\sigma^1, \sigma^2) := 
      \mathbf{1}_{\{\|\sigma^1 - \sigma^2\|_1 = 1\}}\frac{1}{n}\frac{\mu(\sigma^2)}{\mu(\sigma^1) + \mu(\sigma^2)}
    + \mathbf{1}_{\{\sigma^1 = \sigma^2\}} \frac{1}{n}\sum_{\|\tilde \sigma - \sigma^1\|_1 = 1}
    \frac{\mu(\sigma^1)}{\mu(\sigma^1) + \mu(\tilde \sigma)},
  \end{equation}
  where we define \(\|\sigma^1 - \sigma^2\|_1 := \frac{1}{2}\sum_{i = 1}^n |\sigma_i^1 - \sigma_i^2|\) 
  for all \(\sigma^1, \sigma^2 \in \{-1, 1\}^n\).
\end{definition}

Parsing this definition, we see that the Glauber dynamics is the Markov chain such that, starting at a 
configuration \(\sigma_1 \in \{-1, 1\}^n\), the configuration at the next time step either remains the 
same or change at one vertex. Furthermore, the probability of this occurring is weighted according to 
\(\mu\). One may easily compute that the Glauber dynamics of the Ising model \(\mu_{\beta, h}\) indeed 
has \(\mu_{\beta, h}\) as its stationary distribution.

Recently, a framework known as spectral independence has been developed by Anari, Liu and Charan in 
\cite{Anari_2020} which provides a bound for the spectral gap (and consequently a mixing bound by 
theorem~\ref{thm:levin}) for the Glauber dynamics. 

\begin{definition}[Pairwise influence matrix, \cite{Anari_2020}]
  Given a measure \(\mu\) on \(\{-1, 1\}^n\), we define the pairwise influence matrix \(\Psi(\mu)\) 
  such that it has entries 
  \[\Psi(\mu)_{i, j} := \mathbb{P}_{X \sim \mu}[X_i = 1 \mid X_j = 1] - \mathbb{P}_{X \sim \mu}[X_i = 1 \mid X_j = -1],\]
  for all \(i \neq j\) and we set \(\Psi(\mu)_{i, i} = 0\) for all \(i\).
\end{definition}

\begin{definition}[Spectral independence, \cite{Anari_2020}]
  A measure \(\mu\) on \(\{-1, 1\}^n\) is said to be \(\eta\)-spectrally independent if 
  the maximum eigenvalue of \(\Psi(\mu)\) is bounded above by \(\eta\).

  In addition, we say \(\mu\) is \((\eta_0, \cdots, \eta_{n - 2})\)-spectrally independent if defining 
  \(\mu_k\) to be the law of \(X\) conditioned on \(X_{i_1}, \cdots, X_{i_k}\) for some \(X \sim \mu\) and 
  any \(\{i_1, \cdots, i_k\} \subseteq \{1, \cdots, n\}\), \(\mu_k\) is \(\eta_k\)-spectrally independent 
  for each \(k = 0, \cdots, n - 2\).
\end{definition}

\begin{theorem}[\cite{Anari_2020}]\label{thm:Anari}
  If \(\mu\) is a measure on \(\{-1, 1\}^n\) which is \((\eta_0, \cdots, \eta_{n - 2})\)-spectrally 
  independent, then the Glauber dynamics of \(\mu\) has spectral gap bound 
  \[\mathfrak{gap}(K) \ge \frac{1}{n} \prod_{k = 0}^{n - 2}\left(1 - \frac{\eta_k}{n - k - 1}\right).\]
\end{theorem}

As an application of stochastic localizations, we will in section~\ref{sec:Glauber_loc} recover a similar
bound by using the theories established above.

\subsection{Dynamics of stochastic localizations}

As alluded to previously, one may associate a Markov process at each time step of a stochastic 
localization process for which the original process is stationary. We will in this section define 
these Markov processes and show that the Glauber dynamics can be constructed using this method. 

\begin{definition}[Markov process associated with a stochastic localization, \cite{Chen_2022}]
  Let \((\mu_t)_{t \ge 0}\) be a prelocalization of \(\mu\) such that \(\mu_t\) is absolutely 
  continuous (almost everywhere) with respect to \(\mu\) for all \(t\). For all \(\tau > 0\), we define the dynamics 
  associated with \((\mu_t)_t\) at \(\tau\) to be the Markov process with kernel 
  \[K(x, A) := \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}(x) \mu_\tau(A)\right]\]
  for all \(x \in \mathcal{X}, A \in \Sigma\).
\end{definition}

As hinted by the notation, rather than a deterministic time \(\tau\), \(\tau\) can also be taken to be an 
appropriate stopping time. In this case, the theorems below will remain to hold by invoking the optional stopping 
theorem whenever necessary.

This is indeed a kernel since for each \(x \in \mathcal{X}\), as \(\mathbb{E}_\mathbb{P}[\mu_\tau] = \mu\) 
by \ref{L2},
\[K(x, \Omega) = \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}(x)\right] 
  = \frac{\dd}{\dd \mu}\mathbb{E}_\mathbb{P}[\mu_\tau](x) = \frac{\dd \mu}{\dd \mu}(x) = 1\]
where the third equality follows by the uniqueness of the Radon-Nikodym derivative as for all \(A \in \Sigma\), 
we have
\[\int_A \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}\right] \dd \mu = 
  \mathbb{E}_\mathbb{P}\left[\int_A \frac{\dd \mu_\tau}{\dd \mu} \dd \mu \right] = 
  \mathbb{E}_\mathbb{P}[\mu_\tau](A).\]

\begin{proposition}
  The Markov process associated with a prelocalization \((\mu_t)_{t \ge 0}\) of \(\mu\) is reversible and 
  has stationary distribution \(\mu\). 
\end{proposition}
\begin{proof}
  Taking \(\phi : \mathcal{X}^2 \to \mathbb{R}\) integrable, we have by Fubini's theorem
  \begin{equation}\label{eq:markov_assoc}
    \begin{split}
      \int_{\mathcal{X}^2} \phi(x, y) K(x, \dd y) \mu(\dd x) & = 
      \int \phi(x, y) \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}(x) \mu_\tau(\dd y)\right]\mu(\dd x)\\
      & = \mathbb{E}_\mathbb{P}\left[\int \phi(x, y) \mu_\tau(\dd y) \frac{\dd \mu_\tau}{\dd \mu}(x) \mu(\dd x)\right]\\
      & = \mathbb{E}_\mathbb{P}\left[\int \phi(x, y) \mu_\tau(\dd y) \mu_\tau(\dd x)\right].
    \end{split}
  \end{equation}
  Similarly, by the same calculation,   
  \[\int_{\mathcal{X}^2} \phi(x, y) K(y, \dd x) \mu(\dd y) = \mathbb{E}_\mathbb{P}\left[\int \phi(x, y) \mu_\tau(\dd y) \mu_\tau(\dd x)\right].\]
  Thus, 
  \[\int_{\mathcal{X}^2} \phi(x, y) K(x, \dd y) \mu(\dd x) = \int_{\mathcal{X}^2} \phi(x, y) K(y, \dd x) \mu(\dd y)\]
  for any integrable \(\phi : \mathcal{X}^2 \to \mathbb{R}\) implying \(K\) is reversible.

  On the other hand, for all \(A \in \Sigma\), we compute using the martingale property
  \begin{align*} 
    K\mu(A) & = \int K(x, A) \mu(\dd x) = \int \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}(x) \mu_\tau(A)\right] 
    = \mathbb{E}_\mathbb{P}\left[\mu_\tau(A) \int \frac{\dd \mu_\tau}{\dd \mu} \dd \mu\right] \\
    & = \mathbb{E}_\mathbb{P}[\mu_\tau(A) \mu_\tau(\Omega)] = \mathbb{E}_\mathbb{P}[\mu_\tau(A)] = \mu(A)
  \end{align*}
  implying \(\mu\) is the stationary measure of \(K\).
\end{proof}

\begin{proposition}\label{prop:gap_assoc}
  Taking \(K\) to be the kernel of the Markov process associated with a prelocalization \((\mu_t)_{t \ge 0}\)
  of \(\mu\) at time \(\tau\), we have 
  \[\mathfrak{gap}(K) = \inf_{\phi : \mathcal{X} \to \mathbb{R}} \frac{\mathbb{E}[\text{Var}_{\mu_\tau}[\phi]]}{\text{Var}_\mu[\phi]}.\]
\end{proposition}
\begin{proof}
  By equation~\eqref{eq:markov_assoc} (where we take \(\phi(x, y) = \phi(x)\phi(y)\)), we have 
  \begin{equation}
    \int_{\mathcal{X}^2} \phi(x)\phi(y) K(x, \dd y) \mu(\dd x) = \mathbb{E}_\mathbb{P}\left[\left(\int_\mathcal{X} \phi \dd\mu_\tau\right)^2\right],
  \end{equation}
  for any integrable \(\phi : \mathcal{X} \to \mathbb{R}\). On the other hand, we observe 
  \[\int_{\mathcal{X}^2} \phi(y)^2 K(x, \dd y) \mu(\dd x) = \int (K \phi^2)(x) \mu(\dd x) = 
      \int \phi(x)^2 (K\mu)(\dd x) = \int \phi(x)^2 \mu(\dd x)\]
  as \(\mu\) is the stationary measure of \(K\). Thus, for any integrable \(\phi : \mathcal{X} \to \mathbb{R}\),
  by substituting the above two equations, we have
  \begin{align*}
    \hspace{1cm} & \frac{1}{2\text{Var}_\mu[\phi]} \int_{\mathcal{X}^2}(\phi(x) - \phi(y))^2 K(x, \dd y) \mu(\dd x) \\
    = & \frac{1}{2\text{Var}_\mu[\phi]} \left(\int\phi(x)^2\mu(\dd x) 
      - 2\int \phi(x)\phi(y) K(x, \dd y) \mu(\dd x) + \int \phi(y)^2 K(x, \dd y) \mu(\dd x)\right) \\
    = & \frac{1}{\text{Var}_\mu[\phi]}\left(\int\phi^2 \dd \mu 
      - \mathbb{E}_\mathbb{P}\left[\left(\int_\mathcal{X} \phi \dd\mu_\tau\right)^2\right]\right) \\
    = & \frac{1}{\text{Var}_\mu[\phi]} 
        \mathbb{E}_\mathbb{P}\left[\int\phi^2 \dd \mu_\tau - \left(\int_\mathcal{X} \phi \dd\mu_\tau\right)^2\right]
      = \frac{\mathbb{E}[\text{Var}_{\mu_\tau}[\phi]]}{\text{Var}_\mu[\phi]}.
  \end{align*}
  Hence, recalling the equivalent form of the spectral gap as described by \eqref{eq:gap}, the result follows 
  by taking infimum on both sides.
\end{proof}

% This proposition has a nice intuitive interpretation. By recalling that the limit of a stochastic 
% localization as \(t \to \infty\) is a Dirac measure, we may imagine a stochastic localization zooms in 
% (in \(t\)) towards a region containing the massive point. Then, the spectral gap of the 
% associated Markov process at time \(\tau\) is simply the smallest proportion of the local variation around this 
% zoomed in region at time \(\tau\) to that of the global variation. This is achieved by a test function 
% with minimal variation within this region and maximal variation outside of it.

With regards to theorem~\ref{thm:levin}, in the case \(\mu\) has full support on the finite state space 
\(\mathcal{X}\) (e.g. in the setting of Glauber dynamics) the above proposition provides a method for 
computing an upper bound for the mixing time. In particular, should the prelocalization \((\mu_k)_k\)
satisfy 
\[\mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k] \ge (1 - \epsilon)\text{Var}_{\mu_k}[\phi]\]
for given \(\epsilon > 0, k = 0, \cdots, m - 1\) and any integrable function \(\phi : \mathcal{X} \to \mathbb{R}\), we have the 
telescoping product
\[\frac{\mathbb{E}[\text{Var}_{\mu_m}[\phi]]}{\text{Var}_\mu[\phi]} 
  = \mathbb{E}\left[\prod_{k = 0}^{m - 1}\frac{\mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k]}{\text{Var}_{\mu_k}[\phi]}\right]
  \ge (1 - \epsilon)^m.\]
Hence, we have the bound 
\[\mathfrak{gap}(K)^{-1} \le (1 - \epsilon)^{-m}\]
which immediately provides an upper bound for the mixing time of \(K\) in light of theorem~\ref{thm:levin}.
This motivates the following definition.

\begin{definition}[Discrete time approximate conservation of variance (ACV), \cite{Chen_2022}]
  A prelocalization process \((\mu_k)_k\) is said satisfy conserve \((\kappa_k)\)-variance 
  up to time \(m\) if for any integrable function \(\phi : \mathcal{X} \to \mathbb{R}\), \(0 \le k < m\),
  \[\mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_t] \ge (1 - \kappa_k)\text{Var}_{\mu_k}[\phi].\]
\end{definition}

By the same computation above, if \((\mu_k)_k\) conserves \((\kappa_k)\)-variance up to time \(m\), 
then it associated dynamics at time \(m\) has a spectral gap of at least \(\prod_{k = 0}^{m - 1} (1 - \kappa_k)\).

\subsubsection{ACV of discrete time linear-tilt localization}

In the case of the discrete time linear-tilt localization \((\mu_k)_k\) of some \(\mu\), we may compute 
an explicit value \((\eta_k)\) for which \((\mu_k)_k\) conserves \((\eta_k)\)-variance. Indeed, 
given integrable \(\phi : \mathcal{X} \to \mathbb{R}\), we have
\begin{equation}\label{eq:var_bound}
  \begin{split}
    \mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k] 
    & = \mathbb{E}\left[\int\phi^2 \dd \mu_{k + 1} - \left(\int \phi \dd \mu_{k + 1}\right)^2\ \middle\vert\ \mu_k \right]\\
    & = \int \phi^2 \dd \mu_k - \mathbb{E}\left[\left(\int \phi(x) 
          (1 + \langle x - \bar{\mu}_k, Z_k\rangle)\mu_k(\dd x)\right)^2\ \middle\vert\ \mu_k\right]\\
    & = \int \phi^2 \dd \mu_k - \left(\int \phi \dd \mu_k\right)^2 -
          \mathbb{E}\left[\left(\int \phi(x) \langle x - \bar{\mu}_k, Z_k\rangle\mu_k(\dd x)\right)^2\ \middle\vert\ \mu_k\right]\\
    & = \text{Var}_{\mu_k}[\phi] - 
          \text{Var}\left[\int \phi(x) \langle x - \bar{\mu}_k, Z_k\rangle\mu_k(\dd x)\ \middle\vert \mu_k\right]
  \end{split}
\end{equation}
where the third and last equality follows as \(\mathbb{E}[Z_k \mid \mu_k] = 0\). Then, denoting 
\[C_k := \text{Cov}[Z_k \mid \mu_k] \text{ and } U_k := \int \phi(x)(x - \bar{\mu}_k) \mu_k(\dd x),\]
equation~\eqref{eq:var_bound} becomes
\[\mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k] = \text{Var}_{\mu_k}[\phi] - \|C_k^{1 / 2}U_k\|^2.\]
With this reduction, our goal is reduced to bounding \(\|C_k^{1 / 2}U_k\|^2\) by an expression of 
the form \(\eta_k \text{Var}_{\mu_k}[\phi]\) where \(\eta_k\) is independent of \(\phi\). Indeed, 
taking \(\theta\) to be the normalized vector \(C^{1/ 2}U_k / \|C_k^{1 / 2}U_k\|\), we have
\begin{align*}
  \|C_k^{1 / 2} U_k\|^2 & = \langle C_k^{1 / 2} U_k, \theta\rangle^2 
    = \left(\int \phi(x)\langle C_k^{1 / 2}(x - \bar{\mu}_k), \theta \rangle \mu_k(\dd x)\right)^2\\
  & = \left(\int \left(\phi(x) - \int \phi \dd\mu_k\right) \langle C_k^{1 / 2}(x - \bar{\mu}_k), \theta \rangle \mu_k(\dd x)\right)^2\\
  & \le \text{Var}_{\mu_k}[\phi]\int \langle C_k^{1 / 2}(x - \bar{\mu}_k), \theta \rangle^2 \mu_k(\dd x)
    \le \text{Var}_{\mu_k}[\phi]\|C_k^{1/ 2}\text{Cov}(\mu_k) C_k^{1 / 2}\|_{\text{op}}
\end{align*}
where the inequality is due to Cauchy-Schwarz. Hence, for any discrete linear-tilt 
localization \((\mu_k)_k\), we have
\begin{equation}\label{eq:linear-tilt_ACV}
  \mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k] \ge 
    (1 - \|C_k^{1 / 2}\text{Cov}(\mu_k)C_k^{1 / 2}\|_{\text{op}})\text{Var}_{\mu_k}[\phi]
\end{equation}
for any integrable \(\phi : \mathcal{X} \to \mathbb{R}\) and thus, conserve \((1 - \eta_k)\)-variance 
where we define \(\eta_k := \|C_k^{1 / 2}\text{Cov}(\mu_k)C_k^{1 / 2}\|_{\text{op}}\). 

\subsubsection{ACV of linear-tilt localization driven by a Wiener process}

In the setting of the linear-tilt localization driven by a Wiener process, the computation above remains 
to work with slight modifications. Namely, rather than considering the discrete differences of the 
variance, we instead compute and attempt to bound the time derivative of the variance from below. 
This computation turns out to be very useful with regards to the KLS conjecture which will be discussed 
in section~\ref{sec:KLS}.

\begin{definition}[Approximate conservation of variance]
  A prelocalization process \((\mu_t)_{t \ge 0}\) satisfies approximate \((\kappa_t)\)-variance conservation if 
  for all integrable \(\phi : \mathcal{X} \to \mathbb{R}\) and \(t \ge 0\), 
  \[\frac{\dd \text{Var}_{\mu_t}[\phi]}{\text{Var}_{\mu_t}[\phi]} \ge -\kappa_t \dd t + N_t,\]
  for some martingale \((N_t)\).
\end{definition}

% \begin{proposition}
%   Given the linear-tilt localization \((\mu_t)_{t \ge 0}\) of \(\mu\) driven by a Wiener process as defined 
%   in section~\ref{sec:construct}, \((\mu_t)\) satisfies approximate \(\|A_t\|_{\text{op}}\)-variance 
%   conservation where \(A_t\) is the covariance matrix of \(\mu_t\).
% \end{proposition}

% To prove this proposition, we first recall the following consequence of Itô's formula: if \((X_t)\) 
% is an Itô process with no drift term, taking \(f(x) := x^2\), we have  
% \[\dd f(X_t) = 2 X_t \dd X_t + \dd[] [X]_t.\]
% Hence, taking expectation on both sides, we obtain \(\text{Var}[X_t] = [X]_t\). With this 
% in mind, by defining the martingale \((M_t)\) as in corollary~\ref{cor:lim_mart} for the given 
% \(\phi\), we have 
% \[\text{Var}_{\mu_t}[\phi] = \text{Var}[M_t] = [M]_t,\]
% and so, it suffices to analyze \(\dd[] [M]_t\).

% \begin{proof}
%   With the above remarks in mind, we attempt to bound \(\dd[] [M]_t\) by an expression of the form 
%   \(\eta_t \text{Var}_{\mu_t}[\phi]\dd t\). Observe that
%   \begin{align*}
%     \dd M_t & = d \int \phi(x) F_t(x) \mu(\dd x) = \int \phi(x) \langle x - a_t, \dd W_t \rangle \mu_t(\dd x)\\
%     & = \left\langle \int \phi(x)(x - a_t)\mu_t(\dd x), \dd W_t\right\rangle
%   \end{align*}
%   so, by considering the component-wise quadratic variation, we have
%   \begin{equation}\label{eq:diff_qvar}
%     \dd[] [M]_t = \left\| \int \phi(x)(x - a_t)\mu_t(\dd x) \right\|^2 \dd t.
%   \end{equation}
%   The remainder of the calculation is quite similar to the discrete case. Indeed, denoting \(\theta\) the vector 
%   \(\int \phi(x)(x - a_t)\mu_t(\dd x)\) normalized to have norm 1, we observe,
%   \begin{equation}\label{eq:red_a}
%     \begin{split}
%       \dd[] [M]_t & = \left\langle \theta, \int \phi(x)(x - a_t)\mu_t(\dd x)\right\rangle^2 \dd t
%         = \left\langle \theta, \int \left(\phi(x) - \int \phi \dd \mu_t\right)(x - a_t)\mu_t(\dd x)\right\rangle^2 \dd t\\
%       & = \left(\int \left(\phi(x) - \int \phi \dd \mu_t\right) \langle \theta, x - a_t\rangle \mu_t(\dd x)\right)^2 \dd t\\
%       & \le \left(\int \left(\phi(x) - \int \phi \dd \mu_t\right)^2 \mu_t(\dd x)\right) \left(\int \langle \theta, x - a_t\rangle^2 \mu_t(\dd x)\right) \dd t\\
%       & = \text{Var}_{\mu_t}[\phi] \left(\int \langle\theta, (x - a_t)^{\otimes 2} \theta\rangle \mu_t(\dd x)\right) \dd t
%         = \text{Var}_{\mu_t}[\phi] \langle\theta, A_t \theta\rangle\dd t\\ 
%       & \le \text{Var}_{\mu_t}[\phi] \|A_t\|_{\text{op}} \dd t.
%     \end{split}
%   \end{equation}
%   where again the inequality follows by the Cauchy-Schwarz inequality and \(A_t\) is the covariance 
%   matrix of \(\mu_t\).
% \end{proof}

\subsection{Glauber dynamics as an associated Markov process}\label{sec:Glauber_loc}

As alluded to in section~\ref{sec:stoch_loc}, Glauber dynamics can be constructed as an associated Markov 
chain of the coordinate by coordinate localization. Namely, taking \(\mu \in \mathcal{M}(\{-1, 1\}^n)\)
and \(\tau = n - 1\), we will show the associate Markov kernel 
\[\mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\right] = K(\sigma^1, \sigma^2)\]
for all \(\sigma^1, \sigma^2 \in \{-1, 1\}^n\) where \(K(\sigma^1, \sigma^2)\) as defined by equation~\eqref{eq:glauber_ker}.

First, taking \(\sigma^1, \sigma^2 \in \{-1, 1\}^n\) such that \(\|\sigma^1 - \sigma^2\| = 1\), say \(\sigma^1\) 
and \(\sigma^2\) differs at the \(m\)-th coordinate, we have 
\[\mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\right] = \frac{1}{n}\sum_{k = 1}^n \mathbb{E}
    \left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\ \middle\vert\ Y_n = k\right].\]
Now, if \(Y_n = k \neq m\), the configuration is fixed at the \(m\)-th coordinate. However, as \(\sigma^1\) and 
\(\sigma^2\) differs at the \(m\)-th coordinate, either \(\mu_{n - 1}(\sigma^1) = 0\) or \(\mu_{n - 1}(\sigma^2) = 0\).
Thus, all but the \(m\)-th term in the above sum vanishes and we have the kernel equals
\begin{align*}
  & = \frac{1}{n} \mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\ \middle\vert\ Y_n = m\right]\\
  & = \frac{1}{n} \mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}
    \mathbf{1}_{\{\text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}\}} \middle\vert\ Y_n = m\right]\\
  & = \frac{\mathbb{P}(\text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\})}{n} 
    \mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}
    \middle\vert\ Y_n = m, \text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}\right]\\
\end{align*}
Then, substituting in the following equalities:
\[\mathbb{P}(\text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}) = \mu(\sigma^1) + \mu(\sigma^2),\]
and
\begin{align*}
  & \mathbb{E}[\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2) \mid Y_n = m, \text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}]\\
  = & \mathbb{E}[\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2) \mid \text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}]\\
  = & \frac{\mu(\sigma^1)}{\mu(\sigma^1) + \mu(\sigma^2)} \frac{\mu(\sigma^2)}{\mu(\sigma^1) + \mu(\sigma^2)}
\end{align*}
we have 
\[\mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\right] =
  \frac{\mu(\sigma^1) + \mu(\sigma^2)}{n\mu(\sigma^1)}
  \frac{\mu(\sigma^1)}{\mu(\sigma^1) + \mu(\sigma^2)} 
  \frac{\mu(\sigma^2)}{\mu(\sigma^1) + \mu(\sigma^2)} = 
  \frac{1}{n} \frac{\mu(\sigma^2)}{\mu(\sigma^1) + \mu(\sigma^2)}\]
which is precisely the kernel of the Glauber dynamics of two neighboring configurations. 
On the other hand, if \(\|\sigma^1 - \sigma^2\| > 1\), then 
\[\mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\right] = 0\] 
as \(\mu_{n - 1}\) have fixed all but one coordinate. Thus, the dynamics associated with the coordinate 
is precisely the Glauber dynamics (we don't need to check the case \(\sigma^1 = \sigma^2\) as they are 
kernels and so \(K(\sigma, \sigma) = 1 - \sum_{\tilde \sigma \neq \sigma}K(\sigma, \tilde \sigma)\)).

Now, as the coordinate by coordinate localization is a discrete linear-tilt localization as presented 
by section~\ref{sec:discrete_tilt}, we have by equation~\eqref{eq:linear-tilt_ACV} that the spectral gap of 
the Glauber dynamics is bounded below by
\begin{equation}\label{eq:gap_glauber}
  \mathfrak{gap}(K) \ge \prod_{k = 0}^{n - 2} (1 - \|C_k^{1 / 2} \text{Cov}(\mu_k) C_k^{1 / 2}\|_{\text{op}}),
\end{equation}
where \(C_k := \text{Cov}[Z_k \mid \mu_k]\) with \(Z_k\) defined by equation~\eqref{eq:def_Z}. Furthermore, 
by observing from definition of \(Z_k\) that \((Z_k)_i (Z_k)_j = 0\) for all \(i \neq j\), 
\(C_k\) has only non-zero entries on its diagonal for which 
\begin{align*}
  (C_k)_{i, i} &= (\text{Cov}[Z_k \mid \mu_k, Y_k = i])_{i, i} \mathbb{P}(Y_k = i)\\
  & = \frac{1}{n - k}\left(\frac{1}{(1 + (\bar{\mu}_k)_i)^2}\frac{1 + (\bar{\mu}_k)_i}{2} + 
    \frac{1}{(1 - (\bar{\mu}_k)_i)^2}\frac{1 - (\bar{\mu}_k)_i}{2}\right)\\
  & = \frac{1}{n - k}\frac{1}{1 - (\bar{\mu}_k)_i^2} 
    = \frac{1}{n - k}(\text{Cov}(\mu_k)_{i, i})^{-1}.
\end{align*}
Thus, by recalling that the correlation matrix of a measure \(\nu\) is defined with entries 
\[\text{Cor}(\mu)_{i, j} = \frac{\text{Cov}(\mu)_{i, j}}{\sqrt{\text{Cov}(\mu)_{i, i}}\sqrt{\text{Cov}(\mu)_{j, j}}},\]
we have \(C_k^{1 / 2} \text{Cov}(\mu_k) C_k^{1 / 2} = \frac{1}{n - k}\text{Cor}(\mu_k)\) and 
equation~\eqref{eq:gap_glauber} becomes
\begin{equation}
  \mathfrak{gap}(K) \ge \prod_{k = 0}^{n - 2} \left(1 - \frac{\|\text{Cor}(\mu_k)\|_{\text{op}}}{n - k}\right).
\end{equation}

This inequality allows us to directly recover a bound for the spectral gap should \(\mu\) be spectrally 
independent. Indeed, denoting the maximum eigenvalue of a matrix \(A\) by \(\rho(A)\), as \(\text{Cor}(\mu_k)\) is symmetric, we 
have \(\|\text{Cor}(\mu_k)\|_{\text{op}} = \rho(\text{Cor}(\mu_k))\). Furthermore, by observing 
\(\Psi(\mu_k) = \text{Cov}(\mu_k) \text{diag}(\text{Cov}(\mu_k))^{-1} - \text{Id}_n\), we have 
\[\|\text{Cor}(\mu_k)\|_{\text{op}} = \rho(\text{Cor}(\mu_k)) = \rho(\text{Cov}(\mu_k) \text{diag}(\text{Cov}(\mu_k))^{-1})
  = \rho(\Psi(\mu_k) + \text{Id}_n) = \rho(\Psi(\mu_k)) + 1.\]
Thus, if \(\mu\) is a measure on \(\{-1, 1\}^n\) which is \((\eta_0, \cdots, \eta_{n - 2})\)-spectrally 
independent, we have \(\rho(\Psi(\mu_k)) \le \eta_k\) for all \(k = 0, \cdots, n - 2\) and so, by 
equation~\eqref{eq:gap_glauber}, we have
\begin{equation}\label{eq:gap_glauber_final}
  \mathfrak{gap}(K) \ge \prod_{k = 0}^{n - 2}\left(1 - \frac{\eta_k + 1}{n - k}\right).
\end{equation}
This bound and theorem~\ref{thm:Anari} are said to be equivalent as both equations provide polynomial 
lower bounds for the spectral gap in terms of the spectral independence coefficients.

\subsection{Overview of MLSI}