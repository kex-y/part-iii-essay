In this section, we will present the original context and motivation for the construction of stochastic 
localizations: the KLS conjecture. The KLS conjecture is a conjecture which roughly states that all 
log-concave measures on \(\mathbb{R}^n\) are concentrated in a way similar to a Gaussian measure. 
To tackle this problem, Eldan in \cite{Eldan_2013} introduced stochastic localizations and applied to 
in order to reduce the KLS conjecture to a seemingly weaker conjecture (up to a logarithmic constant): 
the thin-Shell conjecture. More recently, also using stochastic localization, Chen \cite{Chen_2020} 
was able to provide an almost constant lower bound on for the KLS conjecture. We will now describe 
a proof of Eldan's original reduction of the KLS conjecture to the thin-Shell conjecture.
The method presented in this section is due to Lee and Vempala \cite{Lee_2016} and reformulated in 
the language of concentration by Eldan \cite{Eldan_notes}.

\subsection{Concentration}

Let us quickly introduce some preliminary definitions required to state the aforementioned conjectures. 

\begin{definition}[Concentration, \cite{Eldan_notes}]
  Let \(\mu\) be a measure on \(\mathbb{R}^n\), then \(\mu\) is said to be \(C\)-(inversely)-concentrated if
  for all \(1\)-Lipschitz function \(\phi : \mathbb{R}^n \to \mathbb{R}\), 
  \begin{equation}
    \text{Var}_\mu[\phi] = \text{Var}_{X \sim \mu}[\phi(X)] \le C^2.
  \end{equation}
  We denote the least possible such \(C\) by \(C^\mu_{\text{con}}\).
\end{definition}

Heuristically, the concentration measures the relation between \(\mu\) and the Euclidean metric by 
providing a numerical control for the variance of its norm. This is perhaps best illustrated by the following proposition.

\begin{proposition}
  Let \(X\) be a \(\mathbb{R}^n\)-valued random variable. Then for all \(K\)-Lipschitz function 
  \(\phi : \mathbb{R}^n \to \mathbb{R}\),
  \[\text{Var}[\phi(X)] \le K^2 \text{Var}[\|X\|^2].\]
\end{proposition}
\begin{proof}
  WLOG. by subtracting its expectation from \(X\), we may assume \(\mathbb{E}[X] = 0\).
  Let \(X'\) be a i.i.d. copy of \(X\) on the same probability space. Then for all \(K\)-Lipschitz 
  function \(\phi\), we have 
  \begin{align*}
    2 \text{Var}[\phi(X)] & = \text{Var}[\phi(X) - \phi(X')] & (\text{i.i.d.})\\
      & = \mathbb{E}[(\phi(X) - \phi(X'))^2] - \mathbb{E}[\phi(X) - \phi(X')]^2 & \\
      & = \mathbb{E}[(\phi(X) - \phi(X'))^2] & (\text{identically distributed}) \\
      & \le K^2 \mathbb{E}[\|X - X'\|^2] & (\text{as \(\phi\) is \(K\)-Lipschitz}) \\
      & = K^2 \mathbb{E}[X^T X + X'^T X' - X^T X' - X'^T X] & \\
      & = 2K^2 \text{Var}[\|X\|^2] - 2K^2 \text{Cov}(X, X') = 2K^2 \text{Var}[\|X\|^2]. & (\text{independence})
  \end{align*}
  implying \(\text{Var}[\phi(X)] \le K^2 \text{Var}[\|X\|^2]\) as claimed.
\end{proof}

With this proposition in mind, it is clear that for \(\mathbb{R}\)-valued random variables \(X\), 
its law \(\mu\) has concentration \(C^\mu_{\text{con}} = \text{Var}[X]\). Furthermore, by 
considering the projection maps, it follows that the standard Gaussian measure on \(\mathbb{R}^n\) 
is 1-concentrated.

We note that the definition we are presenting here is slightly non-standard. However, utilising a 
remarkable result due to Milman, we show that this definition is equivalent to the following 
definitions in a specific sense.

\begin{definition}[Exponential concentration, \cite{Milman_2018}]
  Given a measure \(\mu\) on \(\mathbb{R}^n\), we say \(\mu\) has exponential concentration if 
  there exists some \(c, D > 0\) such that for all 1-Lipschitz function 
  \(\phi : \mathbb{R}^n \to \mathbb{R}, t > 0\), we have
  \begin{equation}
    \mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) \le c e^{-Dt}.
  \end{equation}
  Fixing \(c = 1\), we denote the largest possible \(D\) as \(D^\mu_{\text{exp}}\).
\end{definition}

\begin{definition}[First-moment concentration, \cite{Milman_2018}]
  Again, given \(\mu\) a measure on \(\mathbb{R}^n\), we say \(\mu\) has first-moment concentration 
  if there exists some \(D > 0\) such that for all 1-Lipschitz function 
  \(\phi : \mathbb{R}^n \to \mathbb{R}\), we have
  \begin{equation}
    \mathbb{E}_\mu[|\phi - \mathbb{E}_\mu[\phi]|] \le \frac{1}{D}.
  \end{equation}
  We denote the largest possible \(D\) by \(D^\mu_{\text{FM}}\).
\end{definition}

It is clear that exponential concentration implies first-moment concentration. Indeed, if \(\mu\) 
has exponential concentration with constant \(D\) (taking \(c = 1\)), then by the tail probability formula,
\begin{align*}
  \mathbb{E}_\mu[|\phi - \mathbb{E}_\mu[\phi]|] 
  & = \int_0^\infty \mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) \dd t \le \int_0^\infty e^{-Dt} \dd t 
   = \frac{1}{D}.
\end{align*}

On the other hand, Milman showed that for log-concave measures on \(\mathbb{R}^n\), exponential concentration 
and first-moment concentration are equivalent in the following sense. 

\begin{theorem}[Milman, \cite{Milman_2008}]\label{thm:milman}
  For all log-concave measure \(\mu\) on \(\mathbb{R}^n\), \(\mu\) has exponential concentration 
  if and only if \(\mu\) has first-moment concentration. Furthermore, 
  \(D^\mu_{\text{exp}} \simeq D^\mu_{\text{FM}}\) where we write \(A \simeq B\) if there exists 
  universal constants \(C_1, C_2 > 0\) such that \(C_1 A \le B \le C_2 A\).
\end{theorem}

With this theorem in mind, we establish the following correspondence.

\begin{proposition}\label{thm:equiv}
  For all measures \(\mu\) on \(\mathbb{R}^n\), we have 
  \[\text{Exponentially concentrated \(\implies\) Concentrated \(\implies\) First-moment concentrated}\]
  and \(D^\mu_{\text{exp}} \le \sqrt{2} (C^\mu_{\text{con}})^{-1}\) and 
  \((2C^\mu_{\text{con}})^{-1} \le D^\mu_{\text{FM}}\).
  Hence, if \(\mu\) is log-concave, 
  \(D^\mu_{\text{exp}} \simeq D^\mu_{\text{FM}} \simeq (C^\mu_{\text{con}})^{-1}\).
\end{proposition}
\begin{proof}
  Assume first that \(\mu\) is \(C\)-concentrated. Then by the Chebyshev inequality, we have 
  \[\mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) 
      \le \frac{1}{t^2}\text{Var}_\mu[\phi] 
      \le \frac{C^2}{t^2},\]
  for all 1-Lipschitz \(\phi\). 
  Thus, by tail probability,
  \begin{align*}\mathbb{E}_\mu[|\phi - \mathbb{E}_\mu[\phi]|] 
    & = \int_0^\infty \mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) \dd t\\ 
    & \le \inf_{a > 0} \left\{\int_0^a \mu(|\phi - \mathbb{E}_\mu[\phi]| \ge t) \dd t + C^2 \int_a^\infty \frac{1}{t^2} \dd t\right\}\\ 
    & \le \inf_{a > 0} \left\{a + \frac{C^2}{a}\right\} = 2C,
  \end{align*}
  implying \(\mu\) is first-moment concentrated with respect to the constant \((2C)^{-1}\).

  On the other hand, if \(\mu\) is exponential concentration with some constant \(D\), then 
  again by the tail probability, 
  \[\text{Var}_\mu[\phi] = \int_0^\infty \mu((\phi - \mathbb{E}_\mu[\phi])^2 \ge t) \dd t
      \le \int_0^\infty e^{-D \sqrt{t}} \dd t = \frac{2}{D^2}\]
  implying \(\mu\) is \(\sqrt{2} D^{-1}\)-concentrated.
\end{proof}

\subsection{Example: concentration of the Gaussian}


\subsection{The KLS and thin-shell conjecture}

Informally, the KLS conjecture suggests that any log-concave measure on \(\mathbb{R}^n\) admits 
the same concentration as that of the Gaussian measure. However, unlike the Gaussian, as the 
concentration of measures is not invariant under linear functions, it is clear that the KLS 
conjecture would not hold without a suitable normalization. This leads us to the following formulation 
of the KLS conjecture.

\begin{conjecture}[Kannan-Lovász-Simonovitz, \cite{Eldan_notes}]\label{conj:KLS}
  Denoting \(\mathscr{M}^n_{\text{con}}\) the set of all log-concave probability measures \(\mu\) on 
  \(\mathbb{R}^n\) satisfying \(\text{Var}_\mu[T] \le 1\) for all 1-Lipschitz linear maps 
  \(T : \mathbb{R}^n \to \mathbb{R}\), there exists a \textit{universal} constant \(C\) such that for all 
  \(\mu \in \mathscr{M}^n_{\text{con}}\), \(\mu\) is \(C\)-concentrated.
\end{conjecture}

We remark that \(C\) is universal in the sense that it does not depend on any parameter and in 
particular is independent of the dimension \(n\).

\begin{conjecture}[Thin-shell, \cite{Eldan_2013}]
  Taking \(\mathscr{M}^n_{\text{con}}\) as above, there exists a universal constant \(C\) such that 
  for all \(\mu \in \mathscr{M}^n_{\text{con}}\), we have
  \[\sqrt{\text{Var}_\mu[\|\cdot\|]} \le C.\]
\end{conjecture}

As the norm function is 1-Lipschitz, it is \textit{a priori} that the thin-shell conjecture is 
weaker than that of the KLS conjecture. On the other hand, as we shall describe in the next section, 
as a consequence of the stochastic localization scheme, Eldan \cite{Eldan_2013} provides a reduction 
of the KLS conjecture to the thin-shell conjecture up to logarithmic factors. 

\begin{theorem}[Eldan, \cite{Eldan_2013}]\label{thm:KLS_to_TS}
  Denoting \(\mathscr{M}^n_{\text{con}}\) as above, we define 
  \[C^n_{\text{con}} := \inf \left\{C \mid \text{\(\forall \mu \in \mathscr{M}^n_{\text{con}},\) 
    \(\mu\) is \(C\)-concentrated}\right\},\]
  and 
  \[C^n_{\text{TS}} := \inf \left\{C \mid \text{\(\forall \mu \in \mathscr{M}^n_{\text{con}},\) 
      \(\sqrt{\text{Var}_\mu[\|\cdot\|]} \le C\)}\right\}
      = \sup_{\mu \in \mathscr{M}^n_{\text{con}}} \sqrt{\text{Var}_\mu[\|\cdot\|]},\]
  we have,
  \[C^n_{\text{TS}} \le C^n_{\text{con}} \lesssim C^n_{\text{TS}} \log n.\]
\end{theorem}

% We remark that while the constants in theorem~\ref{thm:KLS_to_TS} depends on the dimension \(n\), the 
% KLS conjecture is reduced to the thin-shell conjecture in the sense that it suffices to show 
% \(\sup_n C^n_{\text{TS}} \log n < \infty\). Then, the universal bound for the KLS conjecture is obtained 
% by taking the constant \(C = \sup_n C^n_{\text{con}} \le \sup_n C^n_{\text{TS}} \log n < \infty\).

The stochastic localization scheme has been wildly successful in making progress towards the KLS conjecture. 
Modifying the original arguments by Eldan, Lee and Vempala \cite{Lee_2016} obtained the bound 
\(C_{\text{con}}^n \lesssim n^{-1 / 4}\). Further modifying their arguments, a recent breakthrough by 
Chen \cite{Chen_2020} improves the bound providing the following theorem. 

\begin{theorem}[Chen, \cite{Chen_2020}]
  \(\log C_{\text{con}}^n \lesssim \sqrt{\log n \log \log n}\) and so \(C_{\text{con}}^n = n^{-o(1)}\).
\end{theorem}


\subsubsection{Equivalent formulation of the KLS conjecture}

While we have formulated the KLS conjecture using the language of concentration, the conjecture itself 
is originally formulated as an isoperimetric problem. The isoperimetric problem is the problem in finding 
the set of unit volume with minimum surface area. In the case of the \(\mathbb{R}^n\) equipped with the 
Lebesgue measure, we have known since the ancient Greeks \cite{iso_hist} that the solution is the unit ball. 
With this in mind, it is natural for us to generalize the problem for arbitrary measures. 

\begin{definition}[Minkowski's boundary measure]
  Given a measure \(\mu\) on \(\mathbb{R}^n\) and a Borel set \(A \subseteq <\mathbb{R}^n\), the Minkowski's 
  boundary measure of \(A\), 
  \[\mu^+(\partial A) := \liminf_{\epsilon \downarrow 0} \frac{\mu(A_\epsilon) - \mu(A)}{\epsilon}.\]
  where \(A_\epsilon := \{x \in \mathbb{R}^n \mid \text{dist}(x, A) \le \epsilon\}\) is the 
  \(\epsilon\)-thickening of some Borel set \(A\). 
\end{definition}

The isoperimetric problem for the measure \(\mu\) then becomes the problem of finding the set \(A\) 
satisfying \(\mu(A) = 1\) with minimum \(\mu^+(\partial A)\). 

\begin{definition}[Cheeger's inequality, \cite{Milman_2008}]
  Given a measure \(\mu\) on \(\mathbb{R}^n\), we say \(\mu\) satisfy Cheeger's inequality 
  if there exists some \(D\) such that for all \(A\),
  \[\mu(A) \wedge \mu(A^c) \le D\mu^+(A).\]
  We call the largest such \(D\) the inverse Cheeger's constant (or the inverse isoperimetric constant) and 
  denote it by \(D^\mu_{\text{C}}\).
\end{definition}

With this, the KLS conjecture can be reformulated as the following.

\begin{conjecture}[KLS, \cite{Eldan_2013}]
  Denoting \(\mathscr{M}^n_{\text{iso}}\) the set of all log-concave and isotropic 
  (i.e. \(\mathbb{E}_{X \sim \mu}[X] = 0\) and \(\text{Cov}_{X \sim \mu}(X) = \text{id}\)) probability measures \(\mu\) on 
  \(\mathbb{R}^n\), there exists a \textit{universal} constant \(D\) such that for all 
  \(\mu \in \mathscr{M}^n_{\text{iso}}\), \(\mu\) satisfy the Cheeger's inequality with constant \(D\).
\end{conjecture}

The equivalence of the reformulation follows by completing theorem~\ref{thm:equiv} with two additional 
equivalences.

\begin{theorem}[Milman, \cite{Milman_2008}]\label{thm:milman2}
  For all log-concave measure \(\mu\) on \(\mathbb{R}^n\), the following are equivalent
  \begin{itemize}
    \item \(\mu\) has exponential concentration with constant \(D^\mu_\text{exp}\).
    \item \(\mu\) has first-moment concentration with constant \(D^\mu_\text{FM}\).
    \item \(\mu\) satisfy the Cheeger's inequality with constant \(D^\mu_\text{C}\).
    \item \(\mu\) satisfy the Poincaré inequality: there exists some \(D > 0\) such that for all 
      differentiable \(\phi : \mathbb{R}^n \to \mathbb{R}\) satisfying \(\int \phi \dd\mu = 0\), we have
      \[D \cdot \text{Var}_\mu[\phi] \le \int \|\nabla \phi\|^2 \dd\mu.\]
      We denote the largest such \(D\) by \(D^\mu_{\text{P}}\).
  \end{itemize}
  Furthermore, \(D^\mu_{\text{exp}} \simeq D^\mu_{\text{FM}} \simeq D^\mu_{\text{P}} \simeq D^\mu_{\text{C}}\).
\end{theorem}

With this theorem and proposition \ref{thm:equiv} in mind, it is clear that the KLS conjecture can be 
instead formulated with any of these inequalities instead. We showcase one of these formulations here 
using the Poincaré inequality:

\begin{conjecture}[KLS, \cite{Eldan_2013}]
  Denoting \(\mathscr{M}^n_{\text{iso}}\) as above, there exist a \textit{universal} constant \(D\) 
  such that for all \(\mu \in \mathscr{M}^n_{\text{iso}}\), \(\mu\) satisfy the Poincaré inequality 
  with constant \(D\).
\end{conjecture}

We remark that isotropic measures satisfy the normalization condition in \ref{conj:KLS}. Indeed, if 
\(T : \mathbb{R}^n \to \mathbb{R}\) is a 1-Lipschitz linear function, i.e. is of the form \(v \mapsto w^T v + d\) 
for some \(w \in S^{n - 1}\) and \(d \in \mathbb{R}\), then we have
\[\text{Var}_{\mu}[T] = \text{Var}_{X \sim \mu}\left[\sum_{i = 1}^n w_i X_i + d\right] 
    = \sum_{i, j = 1}^n w_i w_j \text{Cov}_{X \sim \mu}(X_i, X_j) = \sum_{i = 1}^n w_i^2 = 1,\]
as \(\text{Cov}_{X \sim \mu}(X) = \text{id}\).

