An application of stochastic localizations is used to prove mixing bounds for Markov processes. 
Focusing on the framework established by Chen and Eldan in \cite{Chen_2022}, we will present their prescribed 
method to bound the mixing time for a special class of Markov processes: the Markov chains 
associated with stochastic localizations. 
As a specific example, we will in this section also describe Chen and Eldan's \cite{Chen_2022} application 
of this framework to the Glauber dynamics and as a result, recover a version of the main theorem on spectral independence as 
presented by Anari, Liu and Oveis Gharan in \cite{Anari_2020}.

% It is not difficult to see that 
% \begin{equation}\label{eq:gap}
%   \mathfrak{gap}(K) = 
%     \inf_{\substack{\phi : \mathcal{X} \to \mathbb{R}\\ \int \phi \dd \mu = 0}} 1 
%       - \frac{\int \phi K \phi \dd \mu}{\int \phi^2 \dd \mu}
%     = \inf_{\phi : \mathcal{X} \to \mathbb{R}} 
%       \frac{1}{2 \text{Var}_\mu[\phi]} \int (\phi(x) - \phi(y))^2 K(x, \dd y) \mu(\dd x),
% \end{equation}
% where \(\mu\) is the stationary measure of \(K\) and \(K\phi = \int \phi(y) K(\cdot, \dd y)\). We will take 
% Equation~\eqref{eq:gap} to be the defining property of the spectral gap in the case \(K\) is defined on 
% a general state space. Nonetheless, as the inequality in Theorem~\ref{thm:levin} is only meaningful whenever 
% \(\mu_{\min} > 0\), this theorem is only applicable for Markov chains on finite state space 
% (and we can replace \(\inf\) with \(\min\)).

\subsection{Ising model and Glauber dynamics}

Before moving directly to establishing the framework, let us first quickly discuss the Ising model and the 
main result of this section. The Ising model is a well-known measure in mathematical physics and is often used to model magnetism and 
gas behavior. While fundamentally, the Ising model is constructed using Bernoulli random variables, as these 
random variables interacts with one another, sampling the Ising model directly has exponential computational 
complexity increasing with the number of particles. Hence, to sample from the Ising model, one often instead 
applies MCMC to the Glauber dynamics which is a Markov chain with stationary measure being the Ising model.
We will in this small section introduce these definitions and recall a known bound from \cite{Anari_2020} 
for the spectral gap and consequently the mixing time of the Glauber dynamics.

Heuristically, the Ising model measures the probability that a graph is in a specific configuration of 
up and down spins in which neighboring vertices are more likely to have the same spin. This ``likeliness'' 
is controlled by a parameter \(\beta\) in which a larger \(\beta\) means that neighboring vertices are more likely to
align. 

\begin{definition}[Ising model, \cite{Chen_2022}]
  Given a graph \(G = (V, E)\), \(\beta > 0\) and \(h \in \mathbb{R}\), the Ising model on \(G\) 
  with inverse temperature \(\beta\) and external field \(h\) is the probability measure \(\mu_{\beta, h}\) on 
  \(\{-1, 1\}^V\) defined such that for all \(\sigma \in \{-1, 1\}^V\),
  \[\mu_{\beta, h}(\sigma) := 
    \frac{1}{Z} \exp\left\{\beta \sum_{xy \in E} \sigma_x \sigma_y + h \sum_{x \in V} \sigma_x\right\}\]
  where \(Z > 0\) is the normalizing constant.
\end{definition}

% As illustrated by Theorem~\ref{thm:markov_conv}, in order to sample from the Ising model, we can 
% construct a Markov chain which has the Ising model as its stationary distribution. One such Markov chain 
% is known as the Glauber dynamics.

To sample from the Ising model, one method is by starting from an arbitrary configuration 
\(\sigma_1 \in \{-1, 1\}^n\), and at each time step, update the configuration according to the description 
of the Ising model. Namely, the configuration at the next time step either remains the 
same or change at one vertex. Furthermore, the probability of this occurring is weighted according to a given 
measure \(\mu\). This procedure results in a Markov chain which turns out to have the Ising model as its stationary distribution. 
This Markov chain is known as the Glauber dynamics. Formally, it is defined as the following.

\begin{definition}[Glauber dynamics, \cite{Chen_2022}]
  Given a measure \(\mu\) of \(\{-1, 1\}^n\), the Glauber dynamics of \(\mu\) is the Markov chain 
  with kernel 
  \begin{equation}\label{eq:glauber_ker}
    K(\sigma^1, \sigma^2) := 
      \mathbf{1}_{\{\|\sigma^1 - \sigma^2\|_1 = 1\}}\frac{1}{n}\frac{\mu(\sigma^2)}{\mu(\sigma^1) + \mu(\sigma^2)}
    + \mathbf{1}_{\{\sigma^1 = \sigma^2\}} \frac{1}{n}\sum_{\|\tilde \sigma - \sigma^1\|_1 = 1}
    \frac{\mu(\sigma^1)}{\mu(\sigma^1) + \mu(\tilde \sigma)},
  \end{equation}
  where we define \(\|\sigma^1 - \sigma^2\|_1 := \frac{1}{2}\sum_{i = 1}^n |\sigma_i^1 - \sigma_i^2|\) 
  for all \(\sigma^1, \sigma^2 \in \{-1, 1\}^n\).
\end{definition}

Recently, Anari, Liu and Oveis Gharan in \cite{Anari_2020} were able to 
provide a bound for the spectral gap for a special class of Glauber dynamics satisfying a property known 
as spectral independence. We quickly recall their results here.

\begin{definition}[Pairwise influence matrix, \cite{Anari_2020}]
  Given a measure \(\mu\) on \(\{-1, 1\}^n\), we define the pairwise influence matrix \(\Psi(\mu)\) 
  such that it has entries 
  \[\Psi(\mu)_{i, j} := \mathbb{P}_{X \sim \mu}[X_i = 1 \mid X_j = 1] - \mathbb{P}_{X \sim \mu}[X_i = 1 \mid X_j = -1],\]
  for all \(i \neq j\) and we set \(\Psi(\mu)_{i, i} = 0\) for all \(i\).
\end{definition}

\begin{definition}[Spectral independence, \cite{Anari_2020}]
  A measure \(\mu\) on \(\{-1, 1\}^n\) is said to be \(\eta\)-spectrally independent if 
  the maximum eigenvalue of \(\Psi(\mu)\) is bounded above by \(\eta\).

  In addition, we say \(\mu\) is \((\eta_0, \ldots, \eta_{n - 2})\)-spectrally independent if defining 
  \(\mu_k\) to be the law of \(X\) conditioned on \(X_{i_1}, \ldots, X_{i_k}\) for some \(X \sim \mu\) and 
  any \(\{i_1, \ldots, i_k\} \subseteq \{1, \ldots, n\}\), \(\mu_k\) is \(\eta_k\)-spectrally independent 
  for each \(k = 0, \ldots, n - 2\).
\end{definition}

\begin{theorem}[\cite{Anari_2020}]\label{thm:Anari}
  If \(\mu\) is a measure on \(\{-1, 1\}^n\) which is \((\eta_0, \ldots, \eta_{n - 2})\)-spectrally 
  independent, then the Glauber dynamics of \(\mu\) has spectral gap bound 
  \[\mathfrak{gap}(K) \ge \frac{1}{n} \prod_{k = 0}^{n - 2}\left(1 - \frac{\eta_k}{n - k - 1}\right).\]
\end{theorem}

As an application of stochastic localizations, we will in section~\ref{sec:Glauber_loc} recover a similar 
bound by using the theories established above. In particular, assuming the same assumptions as Theorem~\ref{thm:Anari},
we will show that 
\[\mathfrak{gap}(K) \ge \prod_{k = 0}^{n - 2}\left(1 - \frac{\eta_k + 1}{n - k}\right).\]

\subsection{Dynamics of stochastic localizations}

As alluded to in the introduction, one may associate a Markov process at each time step of a stochastic 
localization process for which the original process is stationary. We will in this section define 
these Markov processes and show that the Glauber dynamics can be constructed using this method. 

\begin{definition}[Markov process associated with a stochastic localization, \cite{Chen_2022}]
  Let \((\mu_t)_{t \ge 0}\) be a prelocalization of \(\mu\) such that \(\mu_t\) is absolutely 
  continuous with respect to \(\mu\) for all \(t\). Then, for all \(\tau > 0\), we define the dynamics 
  associated with \((\mu_t)_t\) at \(\tau\) to be the Markov process with kernel 
  \[K(x, A) := \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}(x) \mu_\tau(A)\right]\]
  for all \(x \in \mathcal{X}, A \in \Sigma\).
\end{definition}

As hinted by the notation, rather than a deterministic time \(\tau\), \(\tau\) can also be taken to be an 
appropriate stopping time. In this case, the theorems below will remain to hold by invoking the optional stopping 
theorem whenever necessary.

This construction indeed results in a kernel since for each \(x \in \mathcal{X}\), as \(\mathbb{E}_\mathbb{P}[\mu_\tau] = \mu\) 
by \eqref{L2},
\[K(x, \Omega) = \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}(x)\right] 
  = \frac{\dd}{\dd \mu}\mathbb{E}_\mathbb{P}[\mu_\tau](x) = \frac{\dd \mu}{\dd \mu}(x) = 1\]
where the third equality follows by the uniqueness of the Radon-Nikodym derivative. In particular, 
for all \(A \in \Sigma\), we have
\[\int_A \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}\right] \dd \mu = 
  \mathbb{E}_\mathbb{P}\left[\int_A \frac{\dd \mu_\tau}{\dd \mu} \dd \mu \right] = 
  \mathbb{E}_\mathbb{P}[\mu_\tau](A).\]

\begin{proposition}
  The Markov process associated with a prelocalization \((\mu_t)_{t \ge 0}\) of \(\mu\) is reversible and 
  has stationary distribution \(\mu\). 
\end{proposition}
\begin{proof}
  Taking \(\phi : \mathcal{X}^2 \to \mathbb{R}\) integrable, we have by Fubini's theorem
  \begin{equation}\label{eq:markov_assoc}
    \begin{split}
      \int_{\mathcal{X}^2} \phi(x, y) K(x, \dd y) \mu(\dd x) & = 
      \int \phi(x, y) \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}(x) \mu_\tau(\dd y)\right]\mu(\dd x)\\
      & = \mathbb{E}_\mathbb{P}\left[\int \phi(x, y) \mu_\tau(\dd y) \frac{\dd \mu_\tau}{\dd \mu}(x) \mu(\dd x)\right]\\
      & = \mathbb{E}_\mathbb{P}\left[\int \phi(x, y) \mu_\tau(\dd y) \mu_\tau(\dd x)\right].
    \end{split}
  \end{equation}
  Similarly, by the same calculation,   
  \[\int_{\mathcal{X}^2} \phi(x, y) K(y, \dd x) \mu(\dd y) = \mathbb{E}_\mathbb{P}\left[\int \phi(x, y) \mu_\tau(\dd y) \mu_\tau(\dd x)\right].\]
  Thus, 
  \[\int_{\mathcal{X}^2} \phi(x, y) K(x, \dd y) \mu(\dd x) = \int_{\mathcal{X}^2} \phi(x, y) K(y, \dd x) \mu(\dd y)\]
  for any integrable \(\phi : \mathcal{X}^2 \to \mathbb{R}\) implying \(K\) is reversible.

  On the other hand, for all \(A \in \Sigma\), we compute using the martingale property
  \begin{align*} 
    K\mu(A) & = \int K(x, A) \mu(\dd x) = \int \mathbb{E}_\mathbb{P}\left[\frac{\dd \mu_\tau}{\dd \mu}(x) \mu_\tau(A)\right] 
    = \mathbb{E}_\mathbb{P}\left[\mu_\tau(A) \int \frac{\dd \mu_\tau}{\dd \mu} \dd \mu\right] \\
    & = \mathbb{E}_\mathbb{P}[\mu_\tau(A) \mu_\tau(\Omega)] = \mathbb{E}_\mathbb{P}[\mu_\tau(A)] = \mu(A)
  \end{align*}
  implying \(\mu\) is the stationary measure of \(K\).
\end{proof}

\begin{proposition}\label{prop:gap_assoc}
  Taking \(K\) to be the kernel of the Markov process associated with a prelocalization \((\mu_t)_{t \ge 0}\)
  of \(\mu\) at time \(\tau\), we have 
  \[\mathfrak{gap}(K) = \inf_{\phi : \mathcal{X} \to \mathbb{R}} \frac{\mathbb{E}[\text{Var}_{\mu_\tau}[\phi]]}{\text{Var}_\mu[\phi]}.\]
\end{proposition}
\begin{proof}
  By Equation~\eqref{eq:markov_assoc} (taking \(\phi(x, y) = \phi(x)\phi(y)\)), we have 
  \begin{equation}
    \int_{\mathcal{X}^2} \phi(x)\phi(y) K(x, \dd y) \mu(\dd x) = \mathbb{E}_\mathbb{P}\left[\left(\int_\mathcal{X} \phi \dd\mu_\tau\right)^2\right],
  \end{equation}
  for any integrable \(\phi : \mathcal{X} \to \mathbb{R}\). On the other hand,  
  \[\int_{\mathcal{X}^2} \phi(y)^2 K(x, \dd y) \mu(\dd x) = \int (K \phi^2)(x) \mu(\dd x) = 
      \int \phi(x)^2 (K\mu)(\dd x) = \int \phi(x)^2 \mu(\dd x)\]
  as \(\mu\) is the stationary measure of \(K\). Thus, for any integrable \(\phi : \mathcal{X} \to \mathbb{R}\),
  by substituting the above two equations, we have
  \begin{equation}\label{eq:gap_assoc2}
    \begin{split}
      \hspace{1cm} & \frac{1}{2\text{Var}_\mu[\phi]} \int_{\mathcal{X}^2}(\phi(x) - \phi(y))^2 K(x, \dd y) \mu(\dd x)\\
      = & \frac{1}{2\text{Var}_\mu[\phi]} \left(\int\phi(x)^2\mu(\dd x) 
        - 2\int \phi(x)\phi(y) K(x, \dd y) \mu(\dd x) + \int \phi(y)^2 K(x, \dd y) \mu(\dd x)\right)\\
      = & \frac{1}{\text{Var}_\mu[\phi]}\left(\int\phi^2 \dd \mu 
        - \mathbb{E}_\mathbb{P}\left[\left(\int_\mathcal{X} \phi \dd\mu_\tau\right)^2\right]\right)\\
      = & \frac{1}{\text{Var}_\mu[\phi]} 
          \mathbb{E}_\mathbb{P}\left[\int\phi^2 \dd \mu_\tau - \left(\int_\mathcal{X} \phi \dd\mu_\tau\right)^2\right]
        = \frac{\mathbb{E}[\text{Var}_{\mu_\tau}[\phi]]}{\text{Var}_\mu[\phi]}.
    \end{split}
  \end{equation}
  Moreover, by observing that 
  \[\mathfrak{gap}(K) = 
  \inf_{\substack{\phi : \mathcal{X} \to \mathbb{R}\\ \int \phi \dd \mu = 0}} 1 
    - \frac{\int \phi K \phi \dd \mu}{\int \phi^2 \dd \mu}
  = \inf_{\phi : \mathcal{X} \to \mathbb{R}} 
    \frac{1}{2 \text{Var}_\mu[\phi]} \int (\phi(x) - \phi(y))^2 K(x, \dd y) \mu(\dd x),\]
  the result follows by taking infimum on both sides of \eqref{eq:gap_assoc2}.
\end{proof}

% This proposition has a nice intuitive interpretation. By recalling that the limit of a stochastic 
% localization as \(t \to \infty\) is a Dirac measure, we may imagine a stochastic localization zooms in 
% (in \(t\)) towards a region containing the massive point. Then, the spectral gap of the 
% associated Markov process at time \(\tau\) is simply the smallest proportion of the local variation around this 
% zoomed in region at time \(\tau\) to that of the global variation. This is achieved by a test function 
% with minimal variation within this region and maximal variation outside of it.

With regards to Theorem~\ref{thm:levin}, in the case \(\mu\) has full support on the finite state space 
\(\mathcal{X}\) (e.g. in the setting of Glauber dynamics) the above proposition provides a method for 
computing an upper bound for the mixing time. In particular, should the prelocalization \((\mu_k)_k\)
satisfy 
\[\mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k] \ge (1 - \epsilon)\text{Var}_{\mu_k}[\phi]\]
for given \(\epsilon > 0, k = 0, \ldots, m - 1\) and any integrable function \(\phi : \mathcal{X} \to \mathbb{R}\), we have the 
telescoping product
\[\frac{\mathbb{E}[\text{Var}_{\mu_m}[\phi]]}{\text{Var}_\mu[\phi]} 
  = \mathbb{E}\left[\prod_{k = 0}^{m - 1}\frac{\mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k]}{\text{Var}_{\mu_k}[\phi]}\right]
  \ge (1 - \epsilon)^m.\]
Hence, we have the bound 
\[\mathfrak{gap}(K)^{-1} \le (1 - \epsilon)^{-m}\]
which immediately provides an upper bound for the mixing time of \(K\) in light of Theorem~\ref{thm:levin}.
This motivates the following definition.

\begin{definition}[Discrete time approximate conservation of variance (ACV), \cite{Chen_2022}]
  A prelocalization process \((\mu_k)_k\) is said to conserve \((\eta_k)\)-variance 
  up to time \(m\) if for any integrable function \(\phi : \mathcal{X} \to \mathbb{R}\), \(0 \le k < m\),
  \[\mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_t] \ge (1 - \eta_k)\text{Var}_{\mu_k}[\phi].\]
\end{definition}

By the same computation above, if \((\mu_k)_k\) conserves \((\eta_k)\)-variance up to time \(m\), 
then it associated dynamics at time \(m\) has a spectral gap of at least \(\prod_{k = 0}^{m - 1} (1 - \eta_k)\).

\subsubsection{ACV of discrete time linear-tilt localization}

In the case of the discrete time linear-tilt localization \((\mu_k)_k\) of some \(\mu\), we may compute 
an explicit value \((\eta_k)\) for which \((\mu_k)_k\) conserves \((\eta_k)\)-variance. Indeed, 
given integrable \(\phi : \mathcal{X} \to \mathbb{R}\), we have
\begin{equation}\label{eq:var_bound}
  \begin{split}
    \mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k] 
    & = \mathbb{E}\left[\int\phi^2 \dd \mu_{k + 1} - \left(\int \phi \dd \mu_{k + 1}\right)^2\ \middle\vert\ \mu_k \right]\\
    & = \int \phi^2 \dd \mu_k - \mathbb{E}\left[\left(\int \phi(x) 
          (1 + \langle x - \bar{\mu}_k, Z_k\rangle)\mu_k(\dd x)\right)^2\ \middle\vert\ \mu_k\right]\\
    & = \int \phi^2 \dd \mu_k - \left(\int \phi \dd \mu_k\right)^2 -
          \mathbb{E}\left[\left(\int \phi(x) \langle x - \bar{\mu}_k, Z_k\rangle\mu_k(\dd x)\right)^2\ \middle\vert\ \mu_k\right]\\
    & = \text{Var}_{\mu_k}[\phi] - 
          \text{Var}\left[\int \phi(x) \langle x - \bar{\mu}_k, Z_k\rangle\mu_k(\dd x)\ \middle\vert \mu_k\right]
  \end{split}
\end{equation}
where the third and last equality follows as \(\mathbb{E}[Z_k \mid \mu_k] = 0\). Then, denoting 
\[C_k := \text{Cov}[Z_k \mid \mu_k] \text{ and } U_k := \int \phi(x)(x - \bar{\mu}_k) \mu_k(\dd x),\]
Equation~\eqref{eq:var_bound} becomes
\[\mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k] = \text{Var}_{\mu_k}[\phi] - \|C_k^{1 / 2}U_k\|^2.\]
With this reduction, our goal is reduced to bounding \(\|C_k^{1 / 2}U_k\|^2\) by an expression of 
the form \(\eta_k \text{Var}_{\mu_k}[\phi]\) where \(\eta_k\) is independent of \(\phi\). Indeed, 
taking \(\theta\) to be the normalized vector \(C^{1/ 2}U_k / \|C_k^{1 / 2}U_k\|\), we have
\begin{align*}
  \|C_k^{1 / 2} U_k\|^2 & = \langle C_k^{1 / 2} U_k, \theta\rangle^2 
    = \left(\int \phi(x)\langle C_k^{1 / 2}(x - \bar{\mu}_k), \theta \rangle \mu_k(\dd x)\right)^2\\
  & = \left(\int \left(\phi(x) - \int \phi \dd\mu_k\right) \langle C_k^{1 / 2}(x - \bar{\mu}_k), \theta \rangle \mu_k(\dd x)\right)^2\\
  & \le \left(\int \left(\phi(x) - \int \phi \dd\mu_k\right)^2 \mu_k(\dd x)\right)\int \langle C_k^{1 / 2}(x - \bar{\mu}_k), \theta \rangle^2 \mu_k(\dd x)\\
  & = \text{Var}_{\mu_k}[\phi]\int \langle \theta, C_k^{1 / 2}(x - \bar{\mu}_k)^{\otimes 2} C_k^{1 / 2} \theta \rangle \mu_k(\dd x)
    = \text{Var}_{\mu_k}[\phi]\langle \theta, C_k^{1 / 2}\text{Cov}(\mu_k) C_k^{1 / 2} \theta\rangle\\
  & \le \text{Var}_{\mu_k}[\phi]\|C_k^{1/ 2}\text{Cov}(\mu_k) C_k^{1 / 2}\|_{\text{op}}
\end{align*}
where the inequality is due to Cauchy-Schwarz. Hence, for any discrete linear-tilt 
localization \((\mu_k)_k\), we have
\begin{equation}\label{eq:linear-tilt_ACV}
  \mathbb{E}[\text{Var}_{\mu_{k + 1}}[\phi] \mid \mu_k] \ge 
    (1 - \|C_k^{1 / 2}\text{Cov}(\mu_k)C_k^{1 / 2}\|_{\text{op}})\text{Var}_{\mu_k}[\phi]
\end{equation}
for any integrable \(\phi : \mathcal{X} \to \mathbb{R}\) and thus, conserve \((1 - \eta_k)\)-variance 
where we define \(\eta_k := \|C_k^{1 / 2}\text{Cov}(\mu_k)C_k^{1 / 2}\|_{\text{op}}\). 

\subsubsection{ACV of linear-tilt localization driven by a Wiener process}

In the setting of the linear-tilt localization driven by a Wiener process, a similar computation 
leveraging on the Cauchy-Schwarz inequality yields a useful inequality which turns out to be very 
useful with regards to the KLS conjecture. We will present the computation now while leaving its application to 
the KLS conjecture to section~\ref{sec:KLS}.

In the continuous time setting, it no longer made sense to analyze the difference of the variance 
between subsequent time steps. Instead, we will analyze its time derivative. In particular, recalling 
the definition of the martingale \((M_t)\) from corollary~\ref{cor:lim_mart}, we may use the same 
method as the discrete time analysis to compute a bound for \(\dd[] [M_t]\).

\begin{proposition}\label{prop:linear-tilt_ACV}
  Given the linear-tilt localization \((\mu_t)_{t \ge 0}\) of \(\mu\) driven by a Wiener process as defined 
  in section~\ref{sec:construct}, for all isotropic \(\phi : \mathbb{R}^n \to \mathbb{R}\), we have 
  \[\dd[] [M]_t \le \text{Var}_{\mu_t}[\phi] \|A_t\|_\text{op} \dd t,\]
  where \(A_t := \int (x - a_t)^{\otimes 2} \dd \mu_t\) is the covariance matrix of \(\mu_t\) and \(M_t := \int \phi \dd \mu_t\).
\end{proposition}

Before proving this proposition, we first recall that for a martingale \((X_t)_t\), its quadratic variation 
\([X]_t\) is the unique adapted, continuous and non-decreasing process such that \((X_t^2 - [X]_t)_t\) 
is a martingale. In particular, if \(X_0 = 0\), we observe 
\[\mathbb{E}[X_t^2] - \mathbb{E}[X]_t = \mathbb{E}[X_0^2] = 0\]
for all \(t\). Thus, 
\[\text{Var}[X_t] = \mathbb{E}[X_t^2] - \mathbb{E}[X_t]^2 = \mathbb{E}[X]_t - \mathbb{E}[X_0]^2 = \mathbb{E}[X]_t.\]
Now, since in our case \(\phi\) is isotropic, we have \(M_0 = \int \phi \dd \mu = 0\) and so,
\(\text{Var}[M_t] = \mathbb{E}[M]_t\). Thus, by taking expectation on both sides, one observes the resemblance 
of the above equation with that of the discrete time ACV inequality. 

% To prove this proposition, we first recall that for a martingale \((X_t)_t\), its quadratic variation 
% \([X]_t\) is the unique adapted, continuous and non-decreasing process such that \((X_t^2 - [X]_t)_t\) 
% is a martingale. In particular, if \(X_0 = 0\), we observe 
% \[\mathbb{E}[X_t^2] - \mathbb{E}[X]_t = \mathbb{E}[X_0^2] = 0\]
% for all \(t\). Thus, 
% \[\text{Var}[X_t] = \mathbb{E}[X_t^2] - \mathbb{E}[X_t]^2 = \mathbb{E}[X]_t - \mathbb{E}[X_0]^2 = \mathbb{E}[X]_t.\]
% Now, as \(\phi\) is isotropic, \(M_0 = \int \phi \dd \mu = 0\) and so,
% \[\text{Var}_{\mu_t}[\phi] = \text{Var}[M_t] = [M]_t.\]
% As a result of this, it suffices to analyze \(\dd[] [M]_t\).

% To prove this proposition, we first recall the following consequence of Itô's formula: if \((X_t)\) 
% is an Itô process with no drift term, taking \(f(x) := x^2\), we have  
% \[\dd f(X_t) = 2 X_t \dd X_t + \dd[] [X]_t.\]
% Hence, taking expectation on both sides, we obtain \(\text{Var}[X_t] = [X]_t\). With this 
% in mind, by defining the martingale \((M_t)\) as in corollary~\ref{cor:lim_mart} for the given 
% \(\phi\), we have 

\begin{proof}
  We compute
  \begin{align*}
    \dd M_t & = d \int \phi(x) F_t(x) \mu(\dd x) = \int \phi(x) \langle x - a_t, \dd W_t \rangle \mu_t(\dd x)\\
    & = \left\langle \int \phi(x)(x - a_t)\mu_t(\dd x), \dd W_t\right\rangle = \langle U_t, \dd W_t\rangle
  \end{align*}
  where we denote \(U_t := \int \phi(x)(x - a_t)\mu_t(\dd x)\) as in the discrete time case.
  Hence, by considering the component-wise quadratic variation, we have
  \begin{equation}\label{eq:diff_qvar}
    \dd[] [M]_t = \left\| U_t \right\|^2 \dd t.
  \end{equation}
  The remainder of the calculation is quite similar to the discrete case. Indeed, denoting \(\theta\) the vector 
  \(U_t\) normalized to have norm 1, we obtain,
  \begin{equation}\label{eq:red_a}
    \begin{split}
      \dd[] [M]_t & = \left\langle U_t, \theta \right\rangle^2 \dd t
        = \left\langle \int \left(\phi(x) - \int \phi \dd \mu_t\right)(x - a_t)\mu_t(\dd x), \theta\right\rangle^2 \dd t\\
      & = \left(\int \left(\phi(x) - \int \phi \dd \mu_t\right) \langle x - a_t, \theta\rangle \mu_t(\dd x)\right)^2 \dd t\\
      & \le \text{Var}_{\mu_t}[\phi] \left(\int \langle\theta, (x - a_t)^{\otimes 2} \theta\rangle \mu_t(\dd x)\right) \dd t\\
      & = \text{Var}_{\mu_t}[\phi] \langle\theta, A_t \theta\rangle\dd t \le \text{Var}_{\mu_t}[\phi] \|A_t\|_{\text{op}} \dd t.
    \end{split}
  \end{equation}
  where the second equality by applying Fubini's theorem and noting that \(\int (x - a_t) \mu_t(\dd x) = 0\). 
  Moreover, the first inequality follows again by the Cauchy-Schwarz inequality.
\end{proof}

\subsection{Glauber dynamics as an associated Markov process}\label{sec:Glauber_loc}

Glauber dynamics can be constructed as an associated Markov 
chain of the coordinate by coordinate localization. Namely, taking \(\mu \in \mathcal{M}(\{-1, 1\}^n)\)
and \(\tau = n - 1\), we will show the associate Markov kernel 
\[\mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\right] = K(\sigma^1, \sigma^2)\]
for all \(\sigma^1, \sigma^2 \in \{-1, 1\}^n\) where \(K(\sigma^1, \sigma^2)\) as defined by Equation~\eqref{eq:glauber_ker}.

First, taking \(\sigma^1, \sigma^2 \in \{-1, 1\}^n\) such that \(\|\sigma^1 - \sigma^2\| = 1\), say \(\sigma^1\) 
and \(\sigma^2\) differs at the \(m\)-th coordinate, we have 
\[\mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\right] = \frac{1}{n}\sum_{k = 1}^n \mathbb{E}
    \left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\ \middle\vert\ Y_n = k\right].\]
Now, if \(Y_n = k \neq m\), the configuration is fixed at the \(m\)-th coordinate. However, as \(\sigma^1\) and 
\(\sigma^2\) differs at the \(m\)-th coordinate, either \(\mu_{n - 1}(\sigma^1) = 0\) or \(\mu_{n - 1}(\sigma^2) = 0\).
Thus, all but the \(m\)-th term in the above sum vanishes and we have the kernel equals
\begin{align*}
  & = \frac{1}{n} \mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\ \middle\vert\ Y_n = m\right]\\
  & = \frac{1}{n} \mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}
    \mathbf{1}_{\{\text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}\}} \middle\vert\ Y_n = m\right]\\
  & = \frac{\mathbb{P}(\text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\})}{n} 
    \mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}
    \middle\vert\ Y_n = m, \text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}\right]\\
\end{align*}
Then, substituting in the equalities,
\[\mathbb{P}(\text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}) = \mu(\sigma^1) + \mu(\sigma^2),\]
and
\begin{align*}
  & \mathbb{E}[\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2) \mid Y_n = m, \text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}]\\
  = & \mathbb{E}[\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2) \mid \text{supp}\ \mu_{n - 1} = \{\sigma^1, \sigma^2\}]\\
  = & \frac{\mu(\sigma^1)}{\mu(\sigma^1) + \mu(\sigma^2)} \frac{\mu(\sigma^2)}{\mu(\sigma^1) + \mu(\sigma^2)}
\end{align*}
we have 
\[\mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\right] =
  \frac{\mu(\sigma^1) + \mu(\sigma^2)}{n\mu(\sigma^1)}
  \frac{\mu(\sigma^1)}{\mu(\sigma^1) + \mu(\sigma^2)} 
  \frac{\mu(\sigma^2)}{\mu(\sigma^1) + \mu(\sigma^2)} = 
  \frac{1}{n} \frac{\mu(\sigma^2)}{\mu(\sigma^1) + \mu(\sigma^2)}\]
which is precisely the kernel of the Glauber dynamics of two neighboring configurations. 
On the other hand, if \(\|\sigma^1 - \sigma^2\| > 1\), then 
\[\mathbb{E}\left[\frac{\mu_{n - 1}(\sigma^1)\mu_{n - 1}(\sigma^2)}{\mu(\sigma^1)}\right] = 0\] 
as \(\mu_{n - 1}\) have fixed all but one coordinate. Thus, the dynamics associated with the coordinate 
is precisely the Glauber dynamics (we don't need to check the case \(\sigma^1 = \sigma^2\) as they are 
kernels and so \(K(\sigma, \sigma) = 1 - \sum_{\tilde \sigma \neq \sigma}K(\sigma, \tilde \sigma)\)).

Now, as the coordinate by coordinate localization is a discrete linear-tilt localization as presented 
by section~\ref{sec:discrete_tilt}, we have by Equation~\eqref{eq:linear-tilt_ACV} that the spectral gap of 
the Glauber dynamics is bounded below by
\begin{equation}\label{eq:gap_glauber}
  \mathfrak{gap}(K) \ge \prod_{k = 0}^{n - 2} (1 - \|C_k^{1 / 2} \text{Cov}(\mu_k) C_k^{1 / 2}\|_{\text{op}}),
\end{equation}
where \(C_k := \text{Cov}[Z_k \mid \mu_k]\) with \(Z_k\) defined by Equation~\eqref{eq:def_Z}. Furthermore, 
by observing from definition of \(Z_k\) that \((Z_k)_i (Z_k)_j = 0\) for all \(i \neq j\), 
\(C_k\) has only non-zero entries on its diagonal for which 
\begin{align*}
  (C_k)_{i, i} &= (\text{Cov}[Z_k \mid \mu_k, Y_k = i])_{i, i} \mathbb{P}(Y_k = i)\\
  & = \frac{1}{n - k}\left(\frac{1}{(1 + (\bar{\mu}_k)_i)^2}\frac{1 + (\bar{\mu}_k)_i}{2} + 
    \frac{1}{(1 - (\bar{\mu}_k)_i)^2}\frac{1 - (\bar{\mu}_k)_i}{2}\right)\\
  & = \frac{1}{n - k}\frac{1}{1 - (\bar{\mu}_k)_i^2} 
    = \frac{1}{n - k}(\text{Cov}(\mu_k)_{i, i})^{-1}.
\end{align*}
Thus, by introducing the correlation matrix \(\text{Cor}(\mu)\) with entries 
\[\text{Cor}(\mu)_{i, j} = \frac{\text{Cov}(\mu)_{i, j}}{\sqrt{\text{Cov}(\mu)_{i, i}}\sqrt{\text{Cov}(\mu)_{j, j}}},\]
we have \(C_k^{1 / 2} \text{Cov}(\mu_k) C_k^{1 / 2} = \frac{1}{n - k}\text{Cor}(\mu_k)\) and 
Equation~\eqref{eq:gap_glauber} becomes
\begin{equation}
  \mathfrak{gap}(K) \ge \prod_{k = 0}^{n - 2} \left(1 - \frac{\|\text{Cor}(\mu_k)\|_{\text{op}}}{n - k}\right).
\end{equation}

This inequality allows us to directly recover a bound for the spectral gap should \(\mu\) be spectrally 
independent. Indeed, denoting the maximum eigenvalue of a matrix \(A\) by \(\rho(A)\), as \(\text{Cor}(\mu_k)\) is symmetric, we 
have 
\[\|\text{Cor}(\mu_k)\|_{\text{op}} = \rho(\text{Cor}(\mu_k)).\] 
Furthermore, by observing 
\[\Psi(\mu_k) = \text{Cov}(\mu_k) \text{diag}(\text{Cov}(\mu_k))^{-1} - \text{Id}_n,\] 
we have 
\[\|\text{Cor}(\mu_k)\|_{\text{op}} = \rho(\text{Cov}(\mu_k) \text{diag}(\text{Cov}(\mu_k))^{-1})
  = \rho(\Psi(\mu_k) + \text{Id}_n) = \rho(\Psi(\mu_k)) + 1.\]
Thus, if \(\mu\) is a measure on \(\{-1, 1\}^n\) which is \((\eta_0, \ldots, \eta_{n - 2})\)-spectrally 
independent, we have \(\rho(\Psi(\mu_k)) \le \eta_k\) for all \(k = 0, \ldots, n - 2\) and so, by 
Equation~\eqref{eq:gap_glauber}, we have
\begin{equation}\label{eq:gap_glauber_final}
  \mathfrak{gap}(K) \ge \prod_{k = 0}^{n - 2}\left(1 - \frac{\eta_k + 1}{n - k}\right).
\end{equation}
This bound and Theorem~\ref{thm:Anari} are said to be equivalent as both equations provide polynomial 
lower bounds for the spectral gap in terms of the spectral independence coefficients.

% \subsection{Analysis of the MLSI coefficient}

% Similar to that of the spectral gap, the modified log-Sobolev inequality (MLSI) coefficient is a quantity 
% which arises in the theory of Markov mixing times. Due to the similarity of the two quantities, it is not at 
% all surprising that the exact same method of stochastic localizations can be applied to analyze the 
% MLSI coefficient. We will in this short section present a brief overview (without proof) of this 
% analysis as presented in \cite{Chen_2022}.

% \begin{definition}[Entropy, \cite{Chen_2022}]
%   Given \(\phi : \mathcal{X} \to \mathbb{R}_{\ge 0}\) and a measure \(\mu\), we define the entropy of \(\phi\) 
%   with respect to \(\mu\) to be 
%   \[\text{Ent}_\mu[\phi] := \mathbb{E}_\mu\left[\phi \log\left(\frac{1}{\mathbb{E}_\mu[\phi]} \phi\right)\right]
%    = \int \phi \log \phi \dd \mu - \int \phi \dd \mu \log\left(\int \phi \dd \mu\right).\]
% \end{definition}

% \begin{definition}[Modified log-Sobolev inequality (MLSI) coefficient, \cite{Chen_2022}]
%   The MLSI coefficient of a Markov kernel \(K\) is defined to be 
%   \[\rho_\text{LS}(K) := \inf_{\phi : \mathcal{X} \to \mathbb{R}_{\ge 0}} 1 - \frac{\text{Ent}_\mu[K\phi]}{\text{Ent}_\mu[\phi]}.\]
% \end{definition}



% By noting the similarity of this definition with that of the equivalent form of the spectral gap~\eqref{eq:gap}, 
% it is not surprising that the MLSI coefficient for linear-tilt localizations can rewritten in a similar 
% form to that of Proposition~\ref{prop:gap_assoc}
