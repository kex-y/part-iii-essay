In this section, we will take a look at an application of the stochastic localization technic 
to prove a version of the log-Sobolev inequality for the log-concave measures. As demonstrated in the 
introduction, the log-Sobolev inequality is central to the concentration of 
measures as it allows us to directly compute concentrations of specific measures. Using stochastic localization, 
we will in this section bound the log-Sobolev constant for log-concave measures 
supported in a ball of fixed diameter. Namely, we will show Theorem~\ref{thm:Lee_Vempala} which states that 
\(\rho_\mu \gtrsim D^{-1}\) for any isotropic log-concave measure \(\mu\). 
We take heavy inspiration from the proof as presented in Section~\ref{sec:KLS}
where we reduced the KLS conjecture to the thin-shell conjecture.

% \subsection{Entropy and log-Sobolev inequalities}

% Heuristically, similar to that of the variance, the entropy of a random variable is a measure of its 
% uncertainty or randomness. Formally, the entropy is defined as the following.

% \begin{definition}[Entropy]
%   Given \(\phi : \mathcal{X} \to \mathbb{R}_{\ge 0}\) and a measure \(\mu\), we define the entropy of \(\phi\) 
%   with respect to \(\mu\) to be 
%   \[\text{Ent}_\mu[\phi] := \mathbb{E}_\mu\left[\phi \log\left(\frac{1}{\mathbb{E}_\mu[\phi]} \phi\right)\right]
%    = \int \phi \log \phi \dd \mu - \int \phi \dd \mu \log\left(\int \phi \dd \mu\right)\]
%   with the convention that \(0\log 0 = 0\).
% \end{definition}
% The log-Sobolev inequality can be then formulated as the following.
% \begin{definition}[Log-Sobolev inequality, \cite{Lee_2016}]
%   For a given measure \(\mu\) on \(\mathbb{R}^n\), \(\mu\) is said to satisfy the log-Sobolev inequality with log-Sobolev 
%   constant \(\rho_\mu\) if \(\rho_\mu\) is the largest \(\rho\) such that for all smooth 
%   \(\phi : \mathbb{R}^n \to \mathbb{R}\) with \(\int \phi^2 \dd \mu = 1\), we have 
%   \[\frac{\rho}{2} \text{Ent}_\mu[\phi^2] \le \mathbb{E}_\mu[\|\nabla \phi\|^2] = \int \|\nabla \phi\|^2 \dd \mu.\]
% \end{definition}

% The log-Sobolev inequality is an incredibly useful inequality while studying the concentration of measures. In 
% particular, should a measure satisfy a log-Sobolev equality, by using a well-known method known as Herbst's argument,
% one can then obtain the exponential concentration of said measure via the Chernoff bound. 
% \begin{theorem}[Herbst's argument]
%   If \(\mu\) satisfy the log-Sobolev inequality with log-Sobolev constant \(\rho_\mu\), then for all \(\phi\) with 
%   uniformly bounded gradient \(\|\nabla \phi\| \le K\), we have 
%   \[\psi_{\phi - \mathbb{E}_\mu \phi}^\mu(\lambda) \le \frac{\rho_\mu K^2 \lambda^2}{2}\]
%   where \(\psi_{\phi - \mathbb{E}_\mu \phi}^\mu\) is the logarithmic moment generating function of 
%   \(\phi - \mathbb{E}_\mu \phi\) with respect to \(\mu\).
% \end{theorem}
% https://faculty.math.illinois.edu/~psdey/math595fa19/lec14.pdf


% We observe the similarity of the log-Sobolev inequality with that of the Poincaré inequality. Remarkably, 
% a similar inequality to the Cheeger's inequality can be defined and a correspondence similar to the correspondence 
% between the Poincaré constant and the Cheeger's constant can be made for the log-Sobolev constant and the 
% log-Cheeger's constant.

% \begin{definition}[Log-Cheeger's inequality]
%   Given a probability measure \(\mu\) on \(\mbox{R}^n\), we say \(\mu\) satisfies the log-Cheeger's inequality 
%   if there exists some \(C\) such that for all \(A\) with \(\mu(A) \le 1 / 2\),
%   \[C\mu(A)\sqrt{-\log(\mu(A))} \le \mu^+(A).\]
%   We call the least such \(C\) the log-Cheeger's constant and denote it by \(C_\mu\). It is known that \(\rho_\mu \simeq C_\mu^2\). 
% \end{definition}
  
% It does not come as a surprise that as with the Poincaré inequality, the standard Gaussian measure 
% \(\gamma^n\) in \(\mathbb{R}^n\) satisfies the log-Sobolev inequality with log-Sobolev constant 
% \(\rho_{\gamma^n} = 1\) (c.f. \cite{Gross_1975}). However, in contrast to the Poincaré inequality where 
% the KLS conjecture suggests that the Poincaré constant is bounded below by a universal constant, it is known 
% that the log-Sobolev constant cannot be bounded below by a universal constant. Nonetheless, we are 
% interested in the log-Sobolev constant of log-concave measure. More specifically, we will study the 
% log-Sobolev constant for log-concave measures supported in a ball of fixed diameter to obtain the 
% following result.

% \begin{theorem}[\cite{Lee_2016}]\label{thm:Lee_Vempala}
%   For any isotropic log-concave measure \(\mu\) on \(\mathbb{R}^n\) with support on a ball of diameter \(D\), 
%   \(\mu\) has log-Sobolev constant \(\rho_\mu \gtrsim D^{-1}\).
% \end{theorem}

\subsection{Entropy of bounded support}

Before moving directly to proving Theorem~\ref{thm:Lee_Vempala}, let us first attempt to gain some intuition 
in how the entropy of a general measure with bounded support behaves. In contrast to the variance, 
where the variance of a measure with bounded support is trivially bounded by the radius squared of 
said support, the bound for the entropy is less clear. Nonetheless, a similar result can be established 
for general measures with bounded support.

% Lemma~\ref{lem:entropy_supp} provides such a log-Sobolev bound for log-concave measures. 
% This condition can be relaxed should we forgive the gradient term. Namely, using properties of 
% sub-Gaussian measures and Hoeffding's lemma, we can arrive at the following result.

\begin{lemma}\label{lem:entropy_supp2}
  For \(\mu\) a measure on \(\mathbb{R}^n\) supported in a ball of diameter \(D \ge \epsilon\) for some fixed \(\epsilon > 0\), 
  there exists a constant \(C_\epsilon\) (depending only on \(\epsilon\)) such that 
  \[\text{Ent}_\mu[\phi^2] \le C_\epsilon D^2\]
  for all smooth 1-Lipschitz functions \(\phi : \mathbb{R}^n \to \mathbb{R}\).
\end{lemma}

\begin{definition}[Sub-Gaussian measures in \(\mathbb{R}\)]
  A measure \(\mu\) on \(\mathbb{R}\) is said to be sub-Gaussian with parameter \(\nu > 0\) if 
  \[\psi_\mu(\lambda) = \log m_{\mu}(\lambda) = \log \mathbb{E}_{X \sim \mu}[e^{\lambda X}] \le \frac{\lambda^2\nu}{2}\]
  for all \(\lambda \in \mathbb{R}\). In this case, we denote \(\mu \in \mathcal{G}(\nu)\).
\end{definition}

\begin{lemma}[Hoeffding's lemma, \cite{Jog_2023}]
  For a measure \(\mu\) on \(\mathbb{R}\) with support of diameter \(D\), we have \(\mu \in \mathcal{G}(D^2 / 4)\).
\end{lemma}

For any smooth \(\phi\) with support on \(D\), we know that \(\phi^2\) has Lipschitz coefficient 
at most \(2D \text{Lip}(\phi)\). Thus, for our measure \(\mu\) on \(\mathbb{R}^n\) which has support on a ball of diameter \(D\) 
and any smooth 1-Lipschitz \(\phi\), we have that the push-forward measure of \(\mu\) along \(\phi^2\), 
\((\phi^2)_* \mu\) is a measure on \(\mathbb{R}\) with support 
with diameter at most \(2D^2\). Consequently, Hoeffding's lemma tells us that \((\phi^2)_* \mu \in \mathcal{G}(D^4)\).  
With this in mind, we can now prove Lemma~\ref{lem:entropy_supp2}.

\begin{proof}[Proof of Lemma~\ref{lem:entropy_supp2}]
  For ease of notation, let us denote \(f = \phi^2\). Consider 
  \[\exp(\text{Ent}_\mu[f]) = \exp(\mathbb{E}_\mu[f\log f] - \mathbb{E}_\mu[f]\log\mathbb{E}[f]).\]
  By considering that the function \(x \mapsto -x\log x\) has its maximum at \(e^{-1}\), we have
  \[- \mathbb{E}_\mu[f]\log\mathbb{E}[f] \le e^{-1}\]
  and thusly \(\exp(\text{Ent}_\mu[f]) \le e^{e^{-1}}\exp(\mathbb{E}_\mu[f\log f])\) and 
  we attempt to bound \(\exp(\mathbb{E}_\mu[f\log f])\). By Jensen's inequality, we have 
  \[\exp(\mathbb{E}_\mu[f\log f]) \le \mathbb{E}_\mu[\exp(f\log f)] = \mathbb{E}_\mu[f e^f].\]
  Now, by Taylor expansion,
  \begin{equation}\label{eq:Taylor}
    \mathbb{E}_\mu[f e^f] = \mathbb{E}_\mu\left[f \sum_{n = 0}^\infty \frac{f^n}{n!}\right]
      = \sum_{n = 1}^\infty \frac{1}{(n - 1)!} \mathbb{E}[f^n]
  \end{equation}
  since the convergence is absolute. 

  On the other hand, by the above remark, \(f_* \mu\) is a sub-Gaussian measure with parameter \(D^4\).
  Thus, we can obtain a bound on the moments of \(\mu\). Indeed, since
  \[\psi_{f_*\mu}(\lambda) = \log m_{f_*\mu}(\lambda) \le \frac{\lambda^2 D^4}{2},\]
  we have \(m_{f_*\mu}(\lambda) \le e^{\lambda^2 D^4 / 2}\) for all \(\lambda \in \mathbb{R}\)
  where \(m_{f_*\mu}\) is the moment generating function of \(f_* \mu\).  

  Hence, Taylor expanding both sides, we obtain
  \[\sum_{n = 0}^\infty \frac{\lambda^n}{n!}\partial_\lambda^n m_{f_*\mu}(0) \le \sum_{n = 0}^\infty \frac{\lambda^{2n}}{n!}D^{4n}\]
  for all \(\lambda \in \mathbb{R}\). Thus, we must have 
  \[\mathbb{E}_\mu[f^{2n}] = \partial_\lambda^{2n} m_{f_*\mu}(0) \le D^{4n}\]
  and 
  \[\mathbb{E}_\mu[f^{2n + 1}] = \partial_\lambda^{2n + 1} m_{f_*\mu}(0) \le 0.\]
  Substituting the above into Equation~\eqref{eq:Taylor}, we obtain
  \[\mathbb{E}_\mu[fe^f] \le \sum_{n = 1}^\infty \frac{1}{(2n - 1)!}D^{4n}
      = D^2 \sum_{n = 1}^\infty \frac{1}{(2n - 1)!}(D^2)^{2n - 1}
      = D^2 \sinh D^2.\]
  Thus, 
  \[\text{Ent}_\mu[f] \le \log(e^{e^{-1}}\mathbb{E}_\mu[fe^f])
    \le e^{-1} + \log(D^2) + \log (\sinh D^2).\]
  Now, since \(\log x + \log (\sinh x) \le 2x\) and as we had assumed \(D \ge \epsilon\), we have 
  \(e^{-1} \le e^{-1}\epsilon^{-2} D^2\). Hence,
  \[\text{Ent}_\mu[f] \le (2 + e^{-1}\epsilon^{-2})D^2 = C_\epsilon D^2\]
  as desired.
\end{proof}

Unfortunately, this approach cannot result in a log-Sobolev inequality as we would like to control 
the entropy of \(\phi\) by \(\mathbb{E}[\|\nabla \phi\|^2]\) rather than just its Lipschitz constant.
Moreover, if \(\phi\) has a high gradient in a small region, the bound resulting from the above 
is rather inefficient. Nonetheless, by restricting to the class of log-concave measures, Kannan, Lovász and Montenegro 
\cite{Lovasz_2007} were able to provide a bound for the log-Sobolev inequality of the same order.

\begin{lemma}[\cite{Lovasz_2007}]\label{lem:entropy_supp}
  For \(\nu\) an isotropic log-concave measure on \(\mathbb{R}^n\) supported in a ball of diameter \(D\), 
  we have
  \[\text{Ent}_\nu[\phi^2] \lesssim D^2 \mathbb{E}_\nu[\|\nabla \phi\|^2]\]
  for all smooth functions \(\phi : \mathbb{R}^n \to \mathbb{R}\).
\end{lemma}

\subsection{Log-Sobolev inequality via stochastic localization}

% a bound by Lee and Vempala for the log-Sobolev constant for all log-concave measures. 
% Namely, we consider the following theorem. 
As promised in the introduction, we will in this section present a modified proof of Theorem~\ref{thm:Lee_Vempala}.
We attempt to analyze the log-Sobolev inequality using the same method as prescribed by Section~\ref{sec:KLS_to_TS}. 
As it turns out, while the initial steps can be followed verbatim, this method runs into issues when trying to 
bound the entropy of the martingale term resulted from the localization. In particular, applying the Cauchy-Schwarz 
inequality no longer provides the desired integral as it had done for the variance case. Instead, by 
utilizing the Burkholder-Davis-Gundy inequality, we are able to express the entropy of the martingale 
\(M_t := \int \phi^2 \dd \mu_t\) in terms of the quadratic 
variation of a different martingale defined by \(N_t := \int \phi^2 \log \phi^2 \dd \mu_t\). Moreover, 
by computing directly, we find that \(N_t\) has quadratic variation bounded the integral of the entropies of the 
stochastic localization up to time \(t\). Hence, by combining the two, we minimize the bound on the entropy by choosing to 
stop the stochastic localization at an appropriate time, resulting in the desired bound.

% and by normalizing, we can also assume it has Lipschitz constant 1.  
% Indeed, if the log-Sobolev inequality holds with constant \(\rho\) for 1-Lipschitz functions, denoting \(\text{Lip}(\phi)\)
% it Lipschitz constant of \(\phi\), we have 
% \[\frac{\rho}{2\text{Lip}(\phi)^2}\text{Ent}_\mu[\phi^2] = 
%   \frac{\rho}{2}\text{Ent}_\mu\left[\left(\frac{\phi}{\text{Lip}(\phi)}\right)^2\right]
%   \le \mathbb{E}\left[\left\|\nabla \frac{\phi}{\text{Lip}(\phi)}\right\|^2\right]
%   = \frac{1}{\text{Lip}(\phi)^2} \mathbb{E}[\|\nabla \phi\|^2].\]   

Let \(\phi : \mathbb{R}^n \to \mathbb{R}\) be smooth. 
Similar to before, we express the entropy of \(\phi^2\) with respect to \(\mu\) as 
the entropy of the martingale terms, in particular, denoting the martingale \(M_t := \int \phi^2 \dd \mu_t\), 
we will show that
\begin{equation}\label{eq:entropy_decomp}
  \text{Ent}_\mu[\phi^2] = \text{Ent}[M_t] + \mathbb{E}[\text{Ent}_{\mu_t}[\phi^2]],
\end{equation}
In fact, a more general fact is true. Namely, for any real-valued function \(\Psi\), defining 
the \(\Psi\)-entropy of \(\phi\) with respect to \(\mu\) as
\[\text{Ent}_\mu^\Psi[\phi] = \mathbb{E}_\mu[\Psi(\phi)] - \Psi(\mathbb{E}_\mu[\phi]),\]
we have that for \(\phi\) satisfying \(\Psi(\mathbb{E}_\mu[\phi]) = 0\),
\begin{equation}\label{eq:entropy_gen_decomp}
  \text{Ent}_\mu^\Psi[\phi] = \text{Ent}^\Psi[M_t] + \mathbb{E}[\text{Ent}_{\mu_t}^\Psi[\phi]],
\end{equation}
where \(M_t := \int \phi \dd\mu_t\) is a martingale. 
This generalizes both \eqref{eq:entropy_decomp} and \eqref{eq:bound} by taking \(\Psi(x) = x\log x\) and \(\Psi(x) = x^2\)
respectively. 

The derivation is rather simple. By definition
\[\text{Ent}_{\mu_t}^\Psi[\phi] = \mathbb{E}_{\mu_t}[\Psi(\phi)] - \Psi(\mathbb{E}_{\mu_t}[\phi])
  = \mathbb{E}_{\mu_t}[\Psi(\phi)] - \Psi(M_t).\]
Thus, taking expectation on both sides, 
\begin{equation}\label{eq:entropy_gen_decomp_deriv}
  \mathbb{E}_\mu[\Psi(\phi)] = \mathbb{E}[\mathbb{E}_{\mu_t}[\Psi(\phi)]] = \mathbb{E}[\Psi(M_t)] + \mathbb{E}[\text{Ent}_{\mu_t}^\Psi(\phi)].
\end{equation}
However, since \(\Psi(\mathbb{E}[\phi]) = 0\) by assumption, we have 
\[\text{Ent}_\mu^\Psi[\phi] = \mathbb{E}_\mu[\Psi(\phi)] - \Psi(\mathbb{E}_\mu[\phi]) = \mathbb{E}_\mu[\Psi(\phi)],\]
and since \(M_t\) is a martingale, we also have \(\mathbb{E}[M_t] = \mathbb{E}[M_0] = \mathbb{E}_\mu[\phi]\),
\[\text{Ent}^\Psi[\Psi(M_t)] = \mathbb{E}[\Psi(M_t)] - \Psi(\mathbb{E}[M_t]) = 
  \mathbb{E}[\Psi(M_t)] - \Psi(\mathbb{E}_\mu[\phi]) = \mathbb{E}[\Psi(M_t)].\]
Finally, substituting the above two equations into \eqref{eq:entropy_gen_decomp_deriv}, we recover 
Equation~\eqref{eq:entropy_gen_decomp} as required.

% Indeed, given the smooth function \(\phi : \mathbb{R}^n \to \mathbb{R}\), let us define the martingale 
% \(M_t := \int \phi^2 \dd \mu_t\) where \((\mu_t)\) is the linear-tilt localization described in Section~\ref{sec:construct}. 
% We observe
% \begin{align*}
%   \text{Ent}_\mu[\phi^2] & = \text{Ent}[M_\infty] 
%     = \mathbb{E}[M_\infty \log M_\infty] - \mathbb{E}[M_\infty]\log(\mathbb{E}[M_\infty]) \\
%   & = \mathbb{E}[\mathbb{E}[M_\infty \log M_\infty \mid \mu_t]] 
%       - \mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]]\log(\mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]]) \\
%   & = \mathbb{E}[\text{Ent}[M_\infty \mid \mu_t]] + \mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]\log(\mathbb{E}[M_\infty \mid \mu_t])]\\ 
%   & \hspace{1cm} - \mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]]\log(\mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]])\\
%   & = \mathbb{E}[\text{Ent}[M_\infty \mid \mu_t]] + \mathbb{E}[M_t \log M_t] - \mathbb{E}[M_t]\log(\mathbb{E}[M_t])\\
%   & = \mathbb{E}[\text{Ent}[M_\infty \mid \mu_t]] + \text{Ent}[M_t]
% \end{align*}
% Moreover, defining the martingale \(N_t := \int \phi^2\log \phi^2 \dd \mu_t\), we observe
% \begin{align*}
%   \text{Ent}[M_\infty \mid \mu_t] 
%   & = \mathbb{E}[M_\infty \log M_\infty \mid \mu_t] - \mathbb{E}[M_\infty \mid \mu_t]\log(\mathbb{E}[M_\infty \mid \mu_t]) \\
%   & = \mathbb{E}[N_\infty \mid \mu_t] - M_t \log M_t = N_t - M_t \log M_t\\
%   & = \mathbb{E}_{\mu_t}[\phi^2 \log \phi^2] - \mathbb{E}_{\mu_t}[\phi^2] \log \mathbb{E}_{\mu_t}[\phi^2] \\
%   & = \text{Ent}_{\mu_t}[\phi^2].
% \end{align*}
% Hence, we recover Equation~\eqref{eq:entropy_decomp} by combining the above two equations.

With this in mind, we recall that in the reduction of the KLS conjecture, we proceeded to bound the two terms on the 
right hand side by using Proposition~\ref{prop:linear-tilt_ACV} and the Brascamp-Lieb inequality respectively. 
We cannot do this for general \(\Psi\). However, in the case that \(\Psi(x) = x\log x\), 
the term \(\mathbb{E}[\text{Ent}_{\mu_t}[\phi^2]]\) can be bounded using the following lemma.

\begin{lemma}[Equation 2.17, \cite{Ledoux_1999}]\label{lem:entropy_BL}
  Let \(U : \mathbb{R}^n \to \mathbb{R}\) be a twice differentiable function such that
  there exists some \(\alpha > 0\) so that \(U''(x) \ge \alpha\text{id}_n\) for any \(x \in \mathbb{R}^n\). 
  Then, for all smooth functions
  \(\phi : \mathbb{R}^n \to \mathbb{R}\), we have 
  \[\text{Ent}_\nu[\phi^2] \le \frac{2}{\alpha}\mathbb{E}_\nu[\|\nabla\phi\|^2]\]
  where \(\dd \nu = e^{-U} \dd \text{Leb}^n\) and \(U''\) denotes the Hessian of \(U\).
\end{lemma}

Recalling Equation~\eqref{eq:stoch_loc_alt}, by substituting \(U(x) = - z_t - \langle v_t, x\rangle + \frac{t}{2}\|x\|^2 + V(x)\)
we observe that \(U''(x) \ge \frac{t}{2}\text{Id}_n + V''(x) \ge \frac{t}{2}\text{Id}_n\) as \(V\) is convex.
Thus, by the above lemma, as \(\dd \mu_t = e^{-U} \dd \text{Leb}^n\), \(\mu_t\) satisfies the log-Sobolev 
inequality of the form
\begin{equation}
  \text{Ent}_{\mu_t}[\phi^2] \le 4t^{-1} \mathbb{E}_{\mu_t}[\|\nabla \phi\|^2].
\end{equation} 
On the other hand, we cannot use the same approach to bound \(\text{Ent}[M_t]\). Indeed, while for 
Proposition~\ref{prop:linear-tilt_ACV}, we were able to invoke a clever application of the Cauchy-Schwarz inequality 
in order to bound \([M]_t\) by \(\text{Var}_{\mu_t}[M_t]\) and the operator norm of the covariance matrix, the extra logarithmic term causes problem 
when we attempt to bound the entropy%\footnote{Lemma 2.7 in \cite{Lee_2017} claims to bound \(\dd[] [M]_t\) by 
%a term involving the entropy via. a similar method invoking Hölder's inequality rather than the Cauchy-Schwarz inequality. However,
%in this case the authors had omitted to check that their choice of the exponent is greater than 1 which is required for Hölder's inequality.}. 
Instead, we shall bound \(\text{Ent}[M_t]\) by an application of the Burkholder-Davis-Gundy inequality. 

\begin{lemma}[Burkholder-Davis-Gundy, \cite{Lowther_2010}]
  For all \(1 \le p < \infty\), there exists some constant \(C_p\) such that for any martingale \((X_t)_{t \ge 0}\),
  we have 
  \[\mathbb{E}\left[\left(\sup_{s \le t} X_s\right)^p\right] \le C_p \mathbb{E}\left[[X]_t^{p / 2}\right]\]
  for all \(t \ge 0\).
\end{lemma}

With the above lemma in mind, denoting \(\psi = \phi^2 \log \phi^2\) and the martingale \(N_t = \int \psi \dd \mu_t\),
we have 
\begin{equation}\label{eq:entropy_bound}
  \begin{split}
    \text{Ent}[M_t] & = \mathbb{E}[M_t\log M_t] - \mathbb{E}[M_t]\overbrace{\log\mathbb{E}[M_t]}^{= 0}\\ 
      & = \mathbb{E}[M_t\log M_t] = \mathbb{E}[\mathbb{E}_{\mu_t}[\phi^2] \log\mathbb{E}_{\mu_t}[\phi^2]]\\
      & \le \mathbb{E}[\mathbb{E}_{\mu_t}[\phi^2 \log \phi^2]] = \mathbb{E}[N_t]\\ 
      & \le C_1 \mathbb{E}\left[[N]_t^{1 / 2}\right].
  \end{split}
\end{equation}
where the first inequality follows by Jensen's inequality. Now, similar to the variance case, we focus on 
bounding \([N]_t\). Similar to the proof of Proposition~\ref{prop:linear-tilt_ACV}, introducing the notation  
\(U_t := \int \psi(x)(x - a_t)\mu_t(\dd x)\), we obtain
\begin{align*}
  \dd N_t & = d \int \psi(x) F_t(x) \mu(\dd x) = \int \psi(x) \langle x - a_t, \dd W_t \rangle \mu_t(\dd x)\\
  & = \left\langle \int \psi(x)(x - a_t)\mu_t(\dd x), \dd W_t\right\rangle = \langle U_t, \dd W_t\rangle.
\end{align*}
Hence, as \(\int (x - a_t) \mu_t(\dd x) = 0\), we have
\begin{align*}
  \dd[] [N]_t & = \|U_t\|^2 \dd t = \left\|\int \psi(x)(x - a_t)\mu_t(\dd x)\right\|^2 \dd t\\
  & = \left\|\int (\psi(x) - M_t \log M_t)(x - a_t)\mu_t(\dd x)\right\|^2 \dd t.
\end{align*}
Now, by recalling that any linear tilt localization satisfies \(\mu_t \ll \mu\) and moreover 
\(\mu\) support in a ball of diameter \(D\), \(\mu_t\) must therefore also has support in a ball of diameter \(D\). 
Thus, \(\|x - a_t\|_{L^\infty(\mu_t)} \le D\) for any \(t \ge 0\). Then, we have  
\begin{align*}
  \dd[] [N]_t & = \left\|\int (\psi(x) - M_t \log M_t)(x - a_t)\mu_t(\dd x)\right\|^2 \dd t\\
  & \le D^2 \left\|\int (\psi(x) - M_t \log M_t)\mu_t(\dd x)\right\|^2 \dd t\\
  & = D^2 \left|\mathbb{E}_{\mu_t}[\phi^2 \log \phi^2] - \mathbb{E}_{\mu_t}[\phi^2] \log\mathbb{E}_{\mu_t}[\phi^2]\right|^2 \dd t\\
  & = D^2 \text{Ent}_{\mu_t}[\phi^2]^2 \dd t.
\end{align*}
Combining this with Equation~\eqref{eq:entropy_bound}, we obtain
\begin{equation}\label{eq:entropy_bound2}
  \text{Ent}[M_t] \lesssim \mathbb{E}\left[\int_0^t \dd[] [N]_s^{1 / 2}\right]
    \le \mathbb{E}\left[\int_0^t D \text{Ent}_{\mu_s}[\phi^2] \dd s\right].
\end{equation}
At this point, as \(\mu_t\) is only supported in a ball of diameter \(D\), 
we can utilize Lemma~\ref{lem:entropy_supp} for bounding the log-Sobolev constant.
%  which proof will be provided in the subsequent section.
% \begin{lemma}\label{lem:entropy_supp}
%   For \(\mu\) a measure on \(\mathbb{R}^n\) supported in a ball of diameter \(D \ge \epsilon\) for some fixed \(\epsilon > 0\), 
%   there exists a constant \(C_\epsilon\) (depending only on \(\epsilon\)) such that 
%   \[\text{Ent}_\mu[\phi^2] \le C_\epsilon D^2\]
%   for all smooth 1-Lipschitz functions \(\phi : \mathbb{R}^n \to \mathbb{R}\).
% \end{lemma}
Hence, combining this with Lemma~\ref{lem:entropy_BL}, we have that 
\(\text{Ent}_{\mu_t}[\phi^2] \lesssim (t^{-1} \wedge D) \mathbb{E}_{\mu_t}[\|\nabla \phi\|^2]\). Thus, 
by Fubini's theorem, Equation~\eqref{eq:entropy_bound2} becomes
\begin{align*}
  \text{Ent}[M_t] & \lesssim \mathbb{E}\left[D \int_0^t (s^{-1} \wedge D^2)\mathbb{E}_{\mu_s}[\|\nabla \phi\|^2] \dd s\right]\\
    & = D \int_0^t (s^{-1} \wedge D^2)\mathbb{E}_\mu[\|\nabla \phi\|^2] \dd s\\
    & = D \mathbb{E}_\mu[\|\nabla \phi\|^2] \left(1 + \int_{D^{-2}}^t s^{-1}\dd s\right)\\
    & = D \mathbb{E}_\mu[\|\nabla \phi\|^2] (1 + \log t + 2\log D)
\end{align*}
for all \(t > D^{-2}\).

Combining this with Equation~\eqref{eq:entropy_decomp}, we obtain
\begin{align*}
  \text{Ent}_\mu[\phi^2] & \le \text{Ent}[M_t] + \mathbb{E}[\text{Ent}_{\mu_t}[\phi^2]]\\
  & \lesssim \mathbb{E}_\mu[\|\nabla \phi\|^2](D (1 + \log t + \log D) + 4t^{-1})
\end{align*}
for all \(t > D^{-2}\). Minimizing this equation in \(t\), we find that it is minimized when \(t = 4D^{-1}\) resulting in 
\[\text{Ent}_\mu[\phi^2] \lesssim (2 + \log 4)D\mathbb{E}_\mu[\|\nabla \phi\|^2]\]
whenever \(4D^{-1} \ge D^{-2}\) which holds if and only if \(D \ge 1 / 4\). Thus, 
\[\text{Ent}_\mu[\phi^2] \lesssim D \mathbb{E}_\mu[\|\nabla \phi\|^2]\]
where for \(D \le 1\), \(D^{-2} \ge D^{-1}\) and so the conclusion remains to hold by Lemma~\ref{lem:entropy_supp}. 
Hence, we bounded the log-Sobolev constant with \(\rho_{\mu} \gtrsim D^{-1}\) recovering the result of Theorem~\ref{thm:Lee_Vempala}.
