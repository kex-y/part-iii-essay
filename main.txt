WIP Title
Kexing Ying
December 30, 2022

Contents
1 Introduction
1.1 Structure of this essay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Stochastic Localization Scheme

3
3
4

2.1 Linear-tilt localization schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

2.1.1 Linear-tilt localization driven by Wiener process . . . . . . . . . . . . . . . .

7

2.1.2 Discrete time linear-tilt localization . . . . . . . . . . . . . . . . . . . . . . . .

9

3 Application: The KLS and Thin-Shell Conjecture

10

3.1 Concentration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 Example: concentration of the Gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.3 The KLS and thin-shell conjecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.3.1 Equivalent formulation of the KLS conjecture . . . . . . . . . . . . . . . . . . 13
3.4 Reduction of KLS to thin-shell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.4.1 Differential of the quadratic variation . . . . . . . . . . . . . . . . . . . . . . . 15
3.4.2 Analysis of the covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.4.3 Stopping the process early . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
4 Application: Markov Mixing

21

4.1 Mixing bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4.1.1 Ising model and Glauber dynamics . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.2 Dynamics of stochastic localizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.3 Glauber dynamics as an associated Markov process . . . . . . . . . . . . . . . . . . . 25

1

Introduction

1.1 Structure of this essay

3

2

Stochastic Localization Scheme

We will in this section introduce the notion of stochastic localization schemes. To gain an intuition for these objects, we will present several examples which will be studied further in subsequent sections.
We will work in general Borel spaces (X , Î£) for this section while for restrict our focus to either
the Euclidean space Rn or the Boolean hypercube {âˆ’1, 1}n in subsequent sections. We take
(â„¦, F , P) our underlying probability space and we introduce the notation M (X ) for the space
of probability measures on X .
Deï¬nition 2.1 (Prelocalization process). Given Âµ âˆˆ M (X ), a measure-valued stochastic process
(Âµ t ) tâ‰¥0 is said to be a prelocalization of Âµ if
(L0) Âµ0 = Âµ.
(L1) For all t â‰¥ 0, Âµ t is a probability measure almost everywhere, i.e. P(Âµ t (X ) = 1) = 1.
(L2) For all A âˆˆ Î£, (Âµ t (A)) tâ‰¥0 is a martingale with respect to the natural ï¬ltration of (Âµ t ).
Where M (X ) is equipped with the Ïƒ-algebra generated by maps of the form
Ï€A : M (X ) 7â†’ Râ‰¥0 âˆª {âˆ} : Âµ 7â†’ Âµ(A)
for all A âˆˆ Î£. Equivalently, this is the Borel Ïƒ-algebra on M (X ) using the topology induced by
the total variation norm.
Deï¬nition 2.2 (Stochastic localization process, [CE22]). Given Âµ âˆˆ M (X ), a measure-valued
stochastic process (Âµ t ) tâ‰¥0 is said to be a stochastic localization of Âµ if in addition to being a
prelocalization of Âµ, (Âµ t ) tâ‰¥0 also satisï¬es
(L3) For all A âˆˆ Î£, Âµ t (A) converges almost everywhere to 0 or 1 as t â†’ âˆ.
Deï¬nition 2.3 (Stochastic localization scheme, [CE22]). Denoting L (Âµ) the set of all stochastic
localization processes of the measure Âµ, a stochastic localization scheme is a map
a
Î¦ : M (X ) â†’
L (Âµ)
ÂµâˆˆM (X )

such that Î¦(Âµ) âˆˆ L (Âµ) for all Âµ âˆˆ M (X ).
We say a stochastic localization is discrete if t takes value in N and continuous if t takes value
in Râ‰¥0 . For shorthand, we denote (Âµk )k for a discrete stochastic localization of Âµ.
Proposition 2.1. Straightaway, by the martingale property, if (Âµ t ) tâ‰¥0 is a stochastic localization
of Âµ, then
â€¢ E[Âµ t ] = Âµ for all t â‰¥ 0.
â€¢ taking X âˆ¼ Âµ such that Âµ t â†’ Î´X almost everywhere as t â†’ âˆ (here weak and total
variational convergence are equivalent and so â†’ can mean either).

4

Proof. The ï¬rst statement is immediate as for all A âˆˆ Î£,
E[Âµ t ](A) Â¬ E[Âµ t (A)] = E[Âµ0 (A)] = Âµ(A).
To prove the second statement, let us ï¬rst parse what the claim is. Fixing a realization Ï‰ of Âµ t ,
we have by (L3) that Âµ t converges to some Dirac measure based at some x Ï‰ âˆˆ X . Thus, deï¬ning
the random variable X : Ï‰ 7â†’ x Ï‰ , it sufï¬ces to show X âˆ¼ Âµ. Indeed, by taking Ï• : X â†’ R to be
any bounded and continuous function, by the deï¬nition of X
Z
Z
a.e.

Ï•(x)Âµ t (dx) âˆ’â†’

Ï•(x)Î´X (dx) = Ï•(X ) as t â†’ âˆ.

Thus, taking expectation on both sides, we have
Z
 Z
E[Ï•(X )] = E

Ï•dÂµ t =

Z
Ï•dE[Âµ t ] =

Ï•dÂµ

implying X âˆ¼ Âµ as required.
An example of a stochastic localization scheme is the coordinate by coordinate localization
scheme on X = {âˆ’1, 1}n . This scheme relates to the Glauber dynamics for which the stochastic
localization scheme provides a mixing bound. We shall examine the property in section 4.3,
though we will construct the scheme now.
Given a probability measure Âµ on {âˆ’1, 1}n , we introduce the random variable X âˆ¼ Âµ, and Y
a uniform random variable over all permutations of [n] = {1, Â· Â· Â· , n} independent of X . Then,
the coordinate by coordinate stochastic localization of Âµ is the process (Âµk )k such that for all
x âˆˆ {âˆ’1, 1}n ,
Âµk (x) = P(X = x | X Y1 , Â· Â· Â· , X Ynâˆ§k ).
Namely, Âµk is the law of X conditioned on X Y1 , Â· Â· Â· , X Yi .
(Âµk )k is indeed a stochastic localization of Âµ. It is clear that (L0) and (L1) are satisï¬ed. By
construction of (Âµk )k , denoting Fk := Ïƒ(X Y1 , Â· Â· Â· , X Ynâˆ§k ), we have by the tower property
E[Âµk+1 (x) | Fk ] = E[E[P(X = x | X ) | Fk+1 ] | Fk ] = E[P(X = x | X ) | Fk ] = Âµk (x)
implying (Âµk (x)) a martingale as required for (L2). Finally, it is clear that
lim Âµk (x) = Âµn (x) = P(X = x | X ) = 1{X =x} âˆˆ {0, 1}

kâ†’âˆ

implying (L3).
An analogous construction of the coordinate by coordinate stochastic localization scheme in Rn
is the random subspace localization. Similar to before, for a probability measure Âµ on Rn , we
introduce the random variable X âˆ¼ Âµ and Y a uniform random variable on O(n) (so the column
vectors {Y1 , Â· Â· Â· , Yn } form an orthonormal basis of Rn ) independent of X . Then, we deï¬ne the
random subspace stochastic localization of Âµ as (Âµk )k where Âµk is the law of X conditioned on
âŒ©X , Y1 âŒª, Â· Â· Â· , âŒ©X , Ynâˆ§k âŒª.

5

2.1 Linear-tilt localization schemes
An important class of stochastic localization schemes are the linear-tilt schemes. Introduced by
Eldan in [Eld13], linear-tilt schemes has been vital in the recent progress regarding the KLS
conjecture. More recently, a discrete version of the linear-tilt scheme was introduced in [CE22]
and is used to provide a mixing bound for Glauber dynamics. We will in this section introduce
these family of localizations and consider two speciï¬c examples of such linear-tilt schemes which
are useful for our analysis later.
Informally, given a probability measure Âµ on X âŠ† Rn , the linear-tilt scheme of Âµ is constructed
recursively in which at each step, we pick a random direction and multiply the density at this
time with a linear function along this direction (i.e. a tilt along a random direction).
Let Âµ be a probability measure on X âŠ† Rn , we introduce the following deï¬nition.
Deï¬nition 2.4 (Barycenter). The barycenter of Âµ with respect to the function F : Rn â†’ R is
Z
ÂµÌ„(F ) :=

x F (x)Âµ(dx).
X

In the case that F = id, we simply write ÂµÌ„ = ÂµÌ„(F ) = EX âˆ¼Âµ [X ].
Deï¬nition 2.5 (Linear-tilt localization). A measure-valued stochastic process (Âµ t ) tâ‰¥0 is said to
be a linear-tilt localization of the probability measure Âµ if
1. Âµ t  Âµ for each t â‰¥ 0, and
2. denoting F t := dÂµ t /dÂµ, we have F0 = 1 and
dF t (x) = âŒ©x âˆ’ ÂµÌ„(F t ), dZ t âŒªF t (x)

(1)

for some stochastic process (Z t ) tâ‰¥0 such that E[dZ t | Âµ t ] = 0 for all t â‰¥ 0.
Proposition 2.2. If (Âµ t ) t is a linear-tilt localization of Âµ, then for all A âˆˆ Î£, dÂµ t (A) = 0 for all t.
Proof. Let A âˆˆ Î£, then we have
Z



dÂµ t (A) = dE[Âµ t (A) | Âµ t ] = E

dF t (x)Âµ(dx) Âµs
A

Z
=

E[âŒ©x âˆ’ ÂµÌ„(F t ), dZ t âŒªF t (x) | Âµ t ]Âµ(dx)
ZA

=

âŒ©x âˆ’ ÂµÌ„(F t ), E[dZ t | Âµ t ]âŒªF t (x)Âµ(dx) = 0
A

as required.
Thus, as Âµ0 = Âµ is a probability measure, it follows Âµ t is a probability measure for each t.
Corollary 2.3. If (Âµ t ) t is a linear-tilt localization of Âµ then for all t, Âµ t is a probability measure.
Furthermore, as (F t (x)) t is a martingale by equation (1), it follows that (Âµ t (A)) t is a martingale
for all A âˆˆ Î£. Hence, we have:
6

Corollary 2.4. A linear-tilt localization (Âµ t ) t of Âµ is a prelocalization of Âµ.
We remark that in general, a linear-tilt localization is not necessarily a stochastic localization as
(L3) might not be satisï¬ed. It is possible to impose sufï¬cient conditions on (Z t ) for which (L3)
holds, e.g. by requiring kCov(Z t )kop to decrease sufï¬ciently fast. However, for generality, we
will not restrict ourselves to one of these conditions. Instead, we will consider (L3) case by case
in the following examples of linear-tilt schemes.
2.1.1

Linear-tilt localization driven by Wiener process

A natural choice of (Z t ) tâ‰¥0 is the standard Wiener process on Rn . Denoting (Wt ) tâ‰¥0 a standard
Wiener process on Rn , we deï¬ne the random functions (F t ) tâ‰¥0 to be the solution of the following
inï¬nite system of SDEs (existence and uniqueness is established by theorem 5.2 in [Ã˜ks03]):
F0 = 1, dF t (x) = âŒ©x âˆ’ ÂµÌ„(F t ), dWt âŒªF t (x),

(2)

for all x âˆˆ Rn . We shall from this point forward denote the random variables a t := ÂµÌ„(F t ).
By applying ItÃ´â€™s formula, we make the following useful observation: for all x âˆˆ Rn ,
d log F t (x) =

dF t (x) d[F (x)] t
1
âˆ’
= âŒ©x âˆ’ a t , dWt âŒª âˆ’ kx âˆ’ a t k2 dt
2
F t (x)
2F t (x)
2

(3)

where the second equality follows by the construction of F . Hence, as log F0 (x) = 0, we observe
Z t
Z t
1
kx âˆ’ as k2 ds
âŒ©x âˆ’ as , dWs âŒª âˆ’
log F t (x) =
2
0
0
 

Z t

Z t
Z t
t
1
2
2
âŒ©as , dWs âŒª âˆ’
= âŒ©x, Wt âŒª âˆ’
âŒ©x, as âŒªds
kas k ds âˆ’
kxk +
2
2 0
0
0
Z t

1
t
2
=âˆ’
âŒ©as , dWs âŒª + kas k ds + âŒ©x, a t + Wt âŒª âˆ’ kxk2 .
2
2
0
Thus, taking dz t := âŒ©a t , dWt âŒª + 12 ka t k2 dt and vt := a t + Wt , we observe F t (x) is of the form
t

F t (x) = ezt +âŒ©x,vt âŒªâˆ’ 2 kxk ,
2

(4)

for given ItÃ´ processes (z t ), (vt ).
With this formulation of F t (x) in mind, it follows F t is non-negative, and so, we may deï¬ne
(Âµ t ) t to be the process such that dÂµ t = F t dÂµ. It is clear that (Âµ t ) t is a linear-tilt localization of
Âµ and so, is a prelocalization of Âµ. The remainder of this section is devoted to showing (Âµ t ) t is
furthermore a stochastic localization of Âµ if Âµ is log-concave (namely we will show (L3) for this
special case), and prove some basic properties about this process useful for our analysis later.
Deï¬nition 2.6 (Log-concave measure). A measure Âµ on Rn is said to log-concave if it is of the
form dÂµ = exp(âˆ’H)dLebn for some convex function H : Rn â†’ R âˆª {âˆ}
To show (Âµ t ) satisï¬es (L3) if Âµ is log-concave, we study the limiting behavior of (Âµ t ) as t â†’ âˆ
by considering their covariances:
Z
A t := Cov[Âµ t ] =

(x âˆ’ a t ) âŠ— (x âˆ’ a t )Âµ t (dx),

7

(5)

where âŠ— denotes the Kronecker product. In particular, we will show (A t )i j â†’ 0 for all i, j âˆˆ
{1, Â· Â· Â· , n} as t â†’ âˆ allowing us to conclude (Âµ t ) converges weakly to some Dirac measure.
Indeed, this is a direct consequence of the following lemma.
Lemma 2.5 (Brascamp-Lieb inequality, [BL76]). Given V : Rn â†’ R convex and K > 0, if Î½ is an
isotropic probability measure on Rn of the form
1

dÎ½ = Z eâˆ’V (x)âˆ’ 2K kxk dLebn
2

with Z being the normalization constant, then Î½ satisfy the PoincarÃ© inequality, i.e. for all differentiable Ï•,
Z
KVarÎ½ [Ï•] â‰¤

kâˆ‡Ï•k2 dÎ½.

With this lemma in mind, by taking Î½ = Âµ t using equation (4) and deï¬ning Ï€i (x) := x i , we have
by the Cauchy-Schwarz inequality
Z
q
q
1
(A t )i j â‰¤ VarÂµ t [Ï€i ] VarÂµ t [Ï€ j ] â‰¤ max
kâˆ‡Ï€k k2 dÂµ t
k=1,Â·Â·Â· ,n t
Again, using equation (4), we note that any realizations of (F t (x)) is eventually decreasing in t
for all x 6= 0, implying
Z
Z
sup max

t>0 k=1,Â·Â·Â· ,n

kâˆ‡Ï€k k2 dÂµ t = sup max

t>0 k=1,Â·Â·Â· ,n

x k2 dÂµ t < âˆ.

Thus, by taking t â†’ âˆ we have (A t )i j â†’ 0 for all i, j âˆˆ {1, Â· Â· Â· , n} as claimed and we have (Âµ t )
satisfying (L3).
Corollary 2.6. (Âµ t ) converges set-wise to some Dirac measure almost everywhere. We denote
this limiting (random) Dirac measure by Î´aâˆ where aâˆ is some Rn -valued random variable.
As a result of 2.1, we have the following useful corollary.
Corollary 2.7. The massive point aâˆ of the limiting Dirac measure is the limit of a t as t â†’ âˆ
and has law Âµ.
Proof. Since convergence implies relatively compact, applying the Dunford-Pettis theorem it
follows that any realizations of (F t ) is uniformly integrable. Thus, the result follows by the
Vitali convergence theorem.
Corollary 2.8. Similarly, taking Ï• to be any continuous
function (not necessarily bounded as
R
we have uniform integrability), deï¬ning M t = Ï•dÂµ t , (M t ) t is a martingale and
a.e.

M t âˆ’â†’ Mâˆ âˆ¼ Ï•âˆ— Âµ
where Ï•âˆ— Âµ denotes the push-forward measure of Âµ along Ï•.

8

(6)

2.1.2

Discrete time linear-tilt localization

We may construct an analogous version of the linear-tilt localization for discrete time. By utilizing the little-o notation, equation (1) can be rewritten as
dÂµ t+h
dÂµ t
dÂµ t
(x) =
(x) + âŒ©x âˆ’ ÂµÌ„ t , hdZ t âŒª
(x) + o(h).
dÂµ
dÂµ
dÂµ
Hence, an discrete analog of the linear tilt localization is deï¬ned as the following.
Deï¬nition 2.7 (Discrete time linear-tilt localization). Given a measure Âµ âˆˆ M (X ), the discrete
time linear-tilt localization is the sequence of random measures (Âµk )k deï¬ned by Âµ0 = Âµ and
dÂµk+1 = (1 + âŒ©x âˆ’ ÂµÌ„k , Zk âŒª)dÂµk

(7)

for some sequence of random variables such that E[Zk | Âµk ] = 0 for all k âˆˆ N.
Using the discrete time linear-tilt localization, let us now provide an alternative construction of
the coordinate by coordinate localization.
Given Âµ a probability measure on {âˆ’1, 1}n , we recall that the coordinate by coordinate localization is deï¬ned by â€œpinningâ€ an additional random coordinate after each time step. To phrase
this as a linear-tilt localization, we take the random variables Zk to be
ï£±
1+(ÂµÌ„k )Yk
ï£² 1
with probability
2
1+(ÂµÌ„k )Yk
Zk := eYk Â·
(8)
1âˆ’(ÂµÌ„k )Yk
ï£³ âˆ’1
with probability
2
1âˆ’(ÂµÌ„k )Y
k

where e1 , Â· Â· Â· , en are the standard basis of Rn and again Y is a uniform random variable over all
permutations of [n]. Thus, the linear-tilt localization (Âµk ) given by this choice of (Zk ) is deï¬ned
by
Âµk+1 (Ïƒ) = (1 + kZk k(ÏƒYk âˆ’ (Âµk )Yk ))Âµk (Ïƒ)
for all k < n. Similar to before, we terminate the process at time n and so we extend the process
to all times by taking Âµk = Âµnâˆ§k .
Let us parse this deï¬nition to see why this is equivalent to the coordinate by coordinate localization. Taking k < n, we have at the k + 1-th step, Yk+1 chooses a random axis which had not
been chosen before. Then, Zk is chosen such that for each conï¬guration Ïƒ, the probability of ÏƒYk
being Â±1 is proportional the mass of Âµk on Â±eYk . This is precisely the steps needed to construct
the coordinate by coordinate localization as conditioning on an additional axis in this case is
simply a projection on to said axis.

9

3

Application: The KLS and Thin-Shell Conjecture

As an application of stochastic localizations, we will in this section introduce the KLS and thinshell conjectures and by leveraging the linear-tilt localization, reduce the KLS conjecture (up to
a logarithmic factor) into the thin-shell conjecture, i.e. we will describe a proof of theorem 2.
The method presented in this section is due to Lee and Vempala [LV16] and reformulated in the
language of concentration by Eldan [Eld18].

3.1 Concentration
Let us quickly introduce some preliminary deï¬nitions required to state the aforementioned conjectures.
Deï¬nition 3.1 (Concentration, [Eld18]). Let Âµ be a measure on Rn , then Âµ is said to be C(inversely)-concentrated if for all 1-Lipschitz function Ï• : Rn â†’ R,
VarÂµ [Ï•] = VarX âˆ¼Âµ [Ï•(X )] â‰¤ C 2 .

(9)

Âµ
We denote the least possible such C by Ccon
.

Heuristically, the concentration measures the relation between Âµ and the Euclidean metric by
providing a numerical control for the variance of its norm. This is perhaps best illustrated by
the following proposition.
Proposition 3.1. Let X be a Rn -valued random variable. Then for all K-Lipschitz function Ï• :
Rn â†’ R,
Var[Ï•(X )] â‰¤ K 2 Var[kX k2 ].
Proof. WLOG. by subtracting its expectation from X , we may assume E[X ] = 0. Let X 0 be a i.i.d.
copy of X on the same probability space. Then for all K-Lipschitz function Ï•, we have
2Var[Ï•(X )] = Var[Ï•(X ) âˆ’ Ï•(X 0 )]
0

(i.i.d.)
0

= E[(Ï•(X ) âˆ’ Ï•(X )) ] âˆ’ E[Ï•(X ) âˆ’ Ï•(X )]
2

2

= E[(Ï•(X ) âˆ’ Ï•(X 0 ))2 ]

(identically distributed)

0 2

â‰¤ K E[kX âˆ’ X k ]
2

0T

(as Ï• is K-Lipschitz)
0

0

0T

= K E[X X + X X âˆ’ X X âˆ’ X X ]
2

T

T

= 2K 2 Var[kX k2 ] âˆ’ 2K 2 Cov(X , X 0 ) = 2K 2 Var[kX k2 ].

(independence)

implying Var[Ï•(X )] â‰¤ K 2 Var[kX k2 ] as claimed.
With this proposition in mind, it is clear that for R-valued random variables X , its law Âµ has
Âµ
concentration Ccon
= Var[X ]. Furthermore, by considering the projection maps, it follows that
the standard Gaussian measure on Rn is 1-concentrated.
We note that the deï¬nition we are presenting here is slightly non-standard. However, utilising
a remarkable result due to Milman, we show that this deï¬nition is equivalent to the following
deï¬nitions in a speciï¬c sense.

10

Deï¬nition 3.2 (Exponential concentration, [Mil18]). Given a measure Âµ on Rn , we say Âµ has
exponential concentration if there exists some c, D > 0 such that for all 1-Lipschitz function
Ï• : Rn â†’ R, t > 0, we have
Âµ(|Ï• âˆ’ EÂµ [Ï•]| â‰¥ t) â‰¤ ceâˆ’Dt .
(10)
Âµ
Fixing c = 1, we denote the largest possible D as Dexp
.

Deï¬nition 3.3 (First-moment concentration, [Mil18]). Again, given Âµ a measure on Rn , we say
Âµ has ï¬rst-moment concentration if there exists some D > 0 such that for all 1-Lipschitz function
Ï• : Rn â†’ R, we have
1
(11)
EÂµ [|Ï• âˆ’ EÂµ [Ï•]|] â‰¤ .
D
Âµ

We denote the largest possible D by DFM .
It is clear that exponential concentration implies ï¬rst-moment concentration. Indeed, if Âµ has
exponential concentration with constant D (taking c = 1), then by the tail probability formula,
Zâˆ
Zâˆ
1
Âµ(|Ï• âˆ’ EÂµ [Ï•]| â‰¥ t)dt â‰¤
eâˆ’Dt dt = .
EÂµ [|Ï• âˆ’ EÂµ [Ï•]|] =
D
0
0
On the other hand, Milman showed that for log-concave measures on Rn , exponential concentration and ï¬rst-moment concentration are equivalent in the following sense.
Theorem 1 (Milman, [Mil08]). For all log-concave measure Âµ on Rn , Âµ has exponential conÂµ
Âµ
centration if and only if Âµ has ï¬rst-moment concentration. Furthermore, Dexp
' DFM where we
write A ' B if there exists universal constants C1 , C2 > 0 such that C1 A â‰¤ B â‰¤ C2 A.
With this theorem in mind, we establish the following correspondence.
Proposition 3.2. For all measures Âµ on Rn , we have
Exponentially concentrated =â‡’ Concentrated =â‡’ First-moment concentrated
p
Âµ
Âµ
Âµ
Âµ âˆ’1
Âµ âˆ’1
Âµ
Âµ âˆ’1
and Dexp
â‰¤ 2(Ccon
) and (2Ccon
) â‰¤ DFM . Hence, if Âµ is log-concave, Dexp
' DFM ' (Ccon
) .
Proof. Assume ï¬rst that Âµ is C-concentrated. Then by the Chebyshev inequality, we have
Âµ(|Ï• âˆ’ EÂµ [Ï•]| â‰¥ t) â‰¤

1
C2
Var
[Ï•]
â‰¤
,
Âµ
t2
t2

for all 1-Lipschitz Ï•. Thus, by tail probability,
Zâˆ
EÂµ [|Ï• âˆ’ EÂµ [Ï•]|] =

Âµ(|Ï• âˆ’ EÂµ [Ï•]| â‰¥ t)dt
0

â‰¤ inf

a>0

Âµ(|Ï• âˆ’ EÂµ [Ï•]| â‰¥ t)dt + C


0

â‰¤ inf a +
a>0

Zâˆ

Z a

2

C
a

2
a



1
dt
t2

= 2C,

implying Âµ is ï¬rst-moment concentrated with respect to the constant (2C)âˆ’1 .
11



On the other hand, if Âµ is exponential concentration with some constant D, then again by the
tail probability,
Zâˆ
Zâˆ
p
2
2
VarÂµ [Ï•] =
Âµ((Ï• âˆ’ EÂµ [Ï•]) â‰¥ t)dt â‰¤
eâˆ’D t dt = 2
D
0
0
p âˆ’1
implying Âµ is 2D -concentrated.

3.2 Example: concentration of the Gaussian
3.3 The KLS and thin-shell conjecture
Informally, the KLS conjecture suggests that any log-concave measure on Rn admits the same
concentration as that of the Gaussian measure. However, unlike the Gaussian, as the concentration of measures is not invariant under linear functions, it is clear that the KLS conjecture would
not hold without a suitable normalization. This leads us to the following formulation of the KLS
conjecture.
n
Conjecture 1 (Kannan-LovÃ¡sz-Simonovitz, [Eld18]). Denoting Mcon
the set of all log-concave
n
probability measures Âµ on R satisfying VarÂµ [T ] â‰¤ 1 for all 1-Lipschitz linear maps T : Rn â†’ R,
n
there exists a universal constant C such that for all Âµ âˆˆ Mcon
, Âµ is C-concentrated.

We remark that C is universal in the sense that it does not depend on any parameter and in
particular is independent of the dimension n.
n
Conjecture 2 (Thin-shell, [Eld13]). Taking Mcon
as above, there exists a universal constant C
n
such that for all Âµ âˆˆ Mcon , we have
q
VarÂµ [k Â· k] â‰¤ C.

As the norm function is 1-Lipschitz, it is a priori that the thin-shell conjecture is weaker than
that of the KLS conjecture. On the other hand, as we shall describe in the next section, as a
consequence of the stochastic localization scheme, Eldan [Eld13] provides a reduction of the
KLS conjecture to the thin-shell conjecture up to logarithmic factors.
n
Theorem 2 (Eldan, [Eld13]). Denoting Mcon
as above, we deï¬ne

n
n
Ccon
:= inf C | âˆ€Âµ âˆˆ Mcon
, Âµ is C-concentrated ,

and

Â¦
Â©
q
q
n
n
CTS
:= inf C | âˆ€Âµ âˆˆ Mcon
, VarÂµ [k Â· k] â‰¤ C = sup
VarÂµ [k Â· k],
n
ÂµâˆˆMcon

we have,
n
n
n
CTS
â‰¤ Ccon
Â® CTS
log n.

The stochastic localization scheme has been wildly successful in making progress towards the
KLS conjecture. Modifying the original arguments by Eldan, Lee and Vempala [LV16] obtained
n
the bound Ccon
Â® nâˆ’1/4 . Further modifying their arguments, a recent breakthrough by Chen
[Che20] improves the bound providing the following theorem.
p
n
n
Theorem 3 (Chen, [Che20]). log Ccon
Â® log n log log n and so Ccon
= nâˆ’o(1) .
12

3.3.1

Equivalent formulation of the KLS conjecture

While we have formulated the KLS conjecture using the language of concentration, the conjecture itself is originally formulated as an isoperimetric problem. The isoperimetric problem is
the problem in ï¬nding the set of unit volume with minimum surface area. In the case of the
Rn equipped with the Lebesgue measure, we have known since the ancient Greeks [Bl05] that
the solution is the unit ball. With this in mind, it is natural for us to generalize the problem for
arbitrary measures.
Deï¬nition 3.4 (Minkowskiâ€™s boundary measure). Given a measure Âµ on Rn and a Borel set
A âŠ†< Rn , the Minkowskiâ€™s boundary measure of A,
Âµ+ (âˆ‚ A) := lim inf
Îµâ†“0

Âµ(AÎµ ) âˆ’ Âµ(A)
.
Îµ

where AÎµ := {x âˆˆ Rn | dist(x, A) â‰¤ Îµ} is the Îµ-thickening of some Borel set A.
The isoperimetric problem for the measure Âµ then becomes the problem of ï¬nding the set A
satisfying Âµ(A) = 1 with minimum Âµ+ (âˆ‚ A).
Deï¬nition 3.5 (Cheegerâ€™s inequality, [Mil08]). Given a measure Âµ on Rn , we say Âµ satisfy
Cheegerâ€™s inequality if there exists some D such that for all A,
Âµ(A) âˆ§ Âµ(Ac ) â‰¤ DÂµ+ (A).
We call the largest such D the inverse Cheegerâ€™s constant (or the inverse isoperimetric constant)
Âµ
and denote it by DC .
With this, the KLS conjecture can be reformulated as the following.
n
Conjecture 3 (KLS, [Eld13]). Denoting Miso
the set of all log-concave and isotropic (i.e.
EX âˆ¼Âµ [X ] = 0 and CovX âˆ¼Âµ (X ) = id) probability measures Âµ on Rn , there exists a universal
n
constant D such that for all Âµ âˆˆ Miso
, Âµ satisfy the Cheegerâ€™s inequality with constant D.

The equivalence of the reformulation follows by completing theorem 3.2 with two additional
equivalences.
Theorem 4 (Milman, [Mil08]). For all log-concave measure Âµ on Rn , the following are equivalent
Âµ
â€¢ Âµ has exponential concentration with constant Dexp
.
Âµ

â€¢ Âµ has ï¬rst-moment concentration with constant DFM .
Âµ

â€¢ Âµ satisfy the Cheegerâ€™s inequality with constant DC .
â€¢ Âµ satisfy the PoincarÃ© Rinequality: there exists some D > 0 such that for all differentiable
Ï• : Rn â†’ R satisfying Ï•dÂµ = 0, we have
Z
D Â· VarÂµ [Ï•] â‰¤
Âµ

We denote the largest such D by DP .
13

kâˆ‡Ï•k2 dÂµ.

Âµ

Âµ

Âµ

Âµ
Furthermore, Dexp
' DFM ' DP ' DC .

With this theorem and proposition 3.2 in mind, it is clear that the KLS conjecture can be instead
formulated with any of these inequalities instead. Thus, the KLS conjecture can also be phrased
using the constant provided by the PoincarÃ© inequality.
n
Conjecture 4 (KLS, [Eld13]). Denoting Miso
as above, there exist a universal constant D such
n
that for all Âµ âˆˆ Miso , Âµ satisfy the PoincarÃ© inequality with constant D.

We remark that isotropic measures satisfy the normalization condition in 1. Indeed, if T : Rn â†’
R is a 1-Lipschitz linear function, i.e. is of the form v 7â†’ w T v + d for some w âˆˆ S nâˆ’1 and d âˆˆ R,
then we have
Â™
Â– n
n
n
X
X
X
w i w j CovX âˆ¼Âµ (X i , X j ) =
w2i = 1,
VarÂµ [T ] = VarX âˆ¼Âµ
wi X i + d =
i, j=1

i=1

i=1

as CovX âˆ¼Âµ (X ) = id.

3.4 Reduction of KLS to thin-shell
We will now present a proof of theorem 2. As a high level overview, recall that the linear-tilt
localization of a given measure is a measure-valued martingale for which the original measure
is recovered in the limit. Then, as the concentration of the measure relates to the covariance of
said measure, we will stop the martingale before the covariance grows too large. This allows
us to analyze the martingale in a more tractable manner. However, as the the sequence is a
martingale, some properties are invariant in time and hence allowing us to conclude that these
properties also hold for the original measure.
We recall the goal of theorem 2 is to control VarÂµ [Ï•] by a logarithmic factor of VarÂµ [k Â· k]. As
translating the barycenter of Âµ does not affect its variance, we may assume Âµ has its barycenter
Âµ at the origin. Furthermore, we may assume Âµ is supported on Bn (0) âŠ† Rn with Bn (0) the ball
at the origin of radius n. Thus, we also have
supp Âµ t = supp F t Âµ âŠ† supp Âµ âŠ† Bn (0)
for all t > 0.
Fix Ï• : Rn â†’ R some 1-Lipschitz function and let (M t ) be the martingale as described above and
a.e.
in particular we recall equation (2.7) and so, VarÂµ [Ï•] = Var[Mâˆ ] where M t âˆ’â†’ Mâˆ . Hence,
for all t > 0, by the martingale property we have


2
Var[M t ] + E[Var[Mâˆ | F t ]] = (E[M t2 ] âˆ’ E[M t ]2 ) + E E[Mâˆ
| F t ] âˆ’ E[Mâˆ | F t ]2
2
= E[M t2 ] + (E[E[Mâˆ
| F t ]] âˆ’ E[M t2 ])
2
= E[Mâˆ
] = Var[Mâˆ ],

where the second equality follows as E[M t ] = E[Mâˆ ] = EÂµ [Ï•] = 0 as a linear map
On the other hand, as (M t ) is a martingale, M t2 âˆ’ [M ] t is also a martingale implying E[M t2 ] =
E[M ] t and so Var[M t ] = E[M t2 ] âˆ’ E[M t ]2 = E[M ] t âˆ’ Âµ2 = E[M ] t . Hence, combining this with
the above, we obtain the bound
VarÂµ [Ï•] = Var[Mâˆ ] = Var[M t ] + E[Var[Mâˆ | F t ]] = E[M ] t + E[VarÂµ t [Ï•]].
14

(12)

explain

Now, observing that Ï• is 1-Lipschitz implies kâˆ‡Ï•k2 â‰¤ 1, we have by lemma 2.5 the bound
VarÂµ t [Ï•] â‰¤ t âˆ’1 (in fact VarÂµ t [Ï•] â‰¤ t âˆ’1 âˆ§ n2 as we have assumed supp Âµ t âŠ† Bn (0)). Thus, the
second term E[VarÂµ t [Ï•]] is bounded by t âˆ’1 . With this in mind, by choosing an appropriate
random time Ï„ to stop the process such that E[M ]Ï„ is nicely bounded, the result follow by
bounding E[Ï„âˆ’1 ]. We dedicate the remainder of this section to describe said procedure in detail.
3.4.1

Differential of the quadratic variation

To bound the term E[M ]Ï„ we will compute its differential and bound it sufï¬ciently such that
we reobtain a bound for [M ]Ï„ after integration. We will show d[M ] t is bounded by a quantity
concerning A t . This should not be at all surprising as both d[M ] t and A t describes the variation
of M t in a inï¬nitesimal time neighborhood of t.
We compute
Z

Z

dM t = d

Ï•(x)F t (x)Âµ(dx) =

Ï•(x)âŒ©x âˆ’ a t , dWt âŒªÂµ t (dx)

Z



=

Ï•(x)(x âˆ’ a t )Âµ t (dx), dWt

and so, by considering the component-wise quadratic variation, we have
Z

Then, denoting Î¸ the vector


R

2

Ï•(x)(x âˆ’ a t )Âµ t (dx)

d[M ] t =

(13)

Ï•(x)(x âˆ’ a t )Âµ t (dx) normalized to have norm 1, so

Z
Î¸,

dt.



Z

Ï•(x)(x âˆ’ a t )Âµ t (dx) =

Ï•(x)(x âˆ’ a t )Âµ t (dx)

we observe,

d[M ] t = Î¸ ,

Z

2
Ï•(x)(x âˆ’ a t )Âµ t (dx)


dt = Î¸ ,

Z
=

2
(Ï•(x) âˆ’ a t )(x âˆ’ a t )Âµ t (dx)

dt

2
(Ï•(x) âˆ’ a t )âŒ©Î¸ , x âˆ’ a t âŒªÂµ t (dx)

Z
â‰¤

Z

dt

 Z
(Ï•(x) âˆ’ a t ) Âµ t (dx)

2

Z
= VarÂµ t [Ï•]


âŒ©Î¸ , x âˆ’ a t âŒª Âµ t (dx) dt

2

(14)


âŠ—2

Î¸ (x âˆ’ a t ) Î¸ Âµ t (dx) dt = VarÂµ t [Ï•](Î¸ T A t Î¸ )dt
T

â‰¤ VarÂµ t [Ï•]kA t kop dt.
where the inequality follows by the Cauchy-Schwarz inequality and k Â· kop denotes the operator
norm. Thus, as we know VarÂµ t [Ï•] â‰¤ t âˆ’1 , the problem is now reduced to that of bounding kA t kop .

15

3.4.2

Analysis of the covariance matrix

As demonstrated in section 2.1.1, we know the limiting behavior of the covariance matrices,
namely A t â†’ 0 point-wise as t â†’ âˆ. This was important for us to establish the existence of the
limit of (a t ) and (M t ). However, as shown above, we now require some quantitative bounds for
the operator norm of A t . For this purpose, we ï¬rst compute some useful properties of A t .
Observing
Z

Z
dF t (x)Âµ(dx) =

we have

Z
âŒ©x âˆ’ a t , dWt âŒªÂµ t (dx) =

Z
da t = d

= 0,

Z
xdF t (x)Âµ(dx) =

Z
=

xÂµ t (dx) âˆ’ a t , dWt

Z
x F t (x)Âµ(dx) =



(x âˆ’ a t )dF t (x)Âµ(dx)

Z

(15)
âŠ—2

(x âˆ’ a t )âŒ©x âˆ’ a t , dWt âŒªF t (x)Âµ(dx) =

(x âˆ’ a t ) dWt Âµ t (dx) = A t dWt

where the second to last equality used the fact that vâŒ©v, wâŒª = v âŠ—2 w for any appropriate v, w.
Similarly, computing using ItÃ´â€™s formula, we have
Z
(x âˆ’ a t )âŠ—2 F t (x)Âµ(dx)

dA t = d
Z
=

(16)

(x âˆ’ a t )âŠ—2 dF t (x) + F t (x) d(x âˆ’ a t )âŠ—2
âˆ’ 2(x âˆ’ a t ) âŠ— d[a t , F t (x)] t + F t (x)d[a t ] t Âµ(dx).

The second term vanishes as
z
Z

Z
F t (x) d(x âˆ’ a t )âŠ—2 Âµ(dx) = âˆ’2da t âŠ—

=0

}|

{

(x âˆ’ a t )Âµ t (dx) = 0.

Also, by equation (15), da t = A t dWt implying d[a t ] t = A2t dt. Finally, as both (a t ) and (F t (x))
are martingales, d[a t , F t (x)] t = F t (x)A t xdt and the third term becomes
Z
Z

âˆ’2

(x âˆ’ a t ) âŠ— d[a t , F t (x)]Âµ t (dx) = âˆ’2A t

(x âˆ’ a t ) âŠ— xÂµ t (dx) dt
ï£«

ï£¬
= âˆ’2A t ï£¬
ï£­

A

z
Z

}|t

{

(x âˆ’ a t )âŠ—2 Âµ t (dx) +

z
Z

=0

Hence, combining these and equation (2) together in (16), we have
Z
(x âˆ’ a t )âŠ—2 âŒ©x âˆ’ a t , dWt âŒªÂµ t (dx) âˆ’ A2t dt
16

{

ï£¶

ï£·
(x âˆ’ a t )Âµ t (dx) âŠ—a t ï£·
ï£¸ dt

= âˆ’2A2t dt.

dA t =

}|

However, since we wish to bound A t from above, as the drift term âˆ’A2t dt only contributes negaR
tively, an upper bound for the process of the form (x âˆ’a t )âŠ—2 âŒ©x âˆ’a t , dWt âŒªÂµ t (dx) is also sufï¬cient
for A t . Hence, we proceed by ignoring the drift term and redeï¬ne the process A t such that
Z
(x âˆ’ a t )âŠ—2 âŒ©x âˆ’ a t , dWt âŒªÂµ t (dx).

dA t =

(17)

With this justiï¬cation, we now proceed to bound the operator norm of this new A t . In particn
ular, as A t is symmetric, we recall that kA t kop = maxi=1,Â·Â·Â· ,n Î»i (t) = k(Î»i (t))i=1
kâˆ where Î»i (t)
denotes the distinct eigenvalues of A t . Hence, it sufï¬ces to ï¬nd a bound for the potential
Î¦Î± (t) =

n
X

|Î»i (t)|Î± = k(Î»i (t))ni=1 kÎ±Î±

(18)

i=1

for some Î± > 0. Furthermore,
as A t is positive semi-deï¬nite, Î»i (t) â‰¥ 0 for all i = 1, Â· Â· Â· , n and
Pn
thus we have Î¦Î± (t) = i=1 Î»i (t)Î± . Again, to proceed, we will attempt to compute dÎ¦Î± (t) at
some t = t 0 > 0 utilizing the following lemma.
Lemma 3.3. If A = [ai j ] is a diagonal matrix with distinct eigenvalues Î»i , Â· Â· Â· , Î»n , then for all
i, j, k, l, m âˆˆ 1, Â· Â· Â· n, we have
âˆ‚Î»

â€¢ âˆ‚ a jki = Î´i j Î´ik ;
âˆ‚ 2Î»

â€¢ whenever i 6= j, âˆ‚ a2i = 2(Î»i âˆ’ Î» j )âˆ’1 ;
ij

âˆ‚ 2Î»

â€¢ and for j 6= l, k 6= m or i 6= j and i 6= k, âˆ‚ a jk âˆ‚ ai l m = 0,
where Î´i j denotes the Kronecker delta function.
As this lemma requires the matrix to be diagonal, denoting e1 , Â· Â· Â· , en as the normalized eigenbasis of A t 0 (they are in fact orthonormal as A t 0 is positive semi-deï¬nite), we will consider A t
with respect to this basis by considering the entries
ai j (t) := âŒ©ei , A t e j âŒª.
Using equation (17), we compute
 Z

 
âŠ—2

dai j (t) = ei ,

(x âˆ’ a t ) âŒ©x âˆ’ a t , dWt âŒªÂµ t (dx) e j

Z
=


âŠ—2

âŒ©ei , (x âˆ’ a t ) e j âŒª(x âˆ’ a t )Âµ t (dx), dWt

= âŒ©Î¾i j , dWt âŒª

R
where we introduce the notation Î¾i j = âŒ©ei , (x âˆ’ a t )âŠ—2 e j âŒª(x âˆ’ a t )Âµ t (dx). Thus, combining this
with lemma 3.3, denoting Î»i = Î»i (t 0 ), we have by ItÃ´â€™s formula
dÎ»i (t) =

n
X
âˆ‚Î»
j,k=1

âˆ‚ 2 Î»i
1 X X
d[a jk , alm ] t
2 j,k=1 l,m=1 âˆ‚ a jk âˆ‚ alm
n

i

âˆ‚ a jk

da jk (t) +

= âŒ©Î¾ii , dWt âŒª +

X d[ai j ] t
j6=i

Î»i âˆ’ Î» j
17

n

= âŒ©Î¾ii , dWt âŒª +

X kÎ¾i j k2
j6=i

Î»i âˆ’ Î» j

dt.

(19)

at t = t 0 . As a result, it is also clear that d[Î»i (t)] t 0 = kÎ¾ii k2 dt.
Again applying ItÃ´â€™s formula, we may ï¬nally compute
dÎ¦Î± (t) =

n
X
âˆ‚ Î¦Î±
i=1

âˆ‚ Î»i

1 X âˆ‚ 2 Î¦Î±
d[Î»i , Î» j ] t
2 i, j=1 âˆ‚ Î»i âˆ‚ Î» j t=t 0
n

t=t 0

dÎ»i (t) +

n
X
1
Î»Î±âˆ’2
d[Î»i (t)] t
Î±(Î±
âˆ’
1)
Î»Î±âˆ’1
dÎ»
(t)
+
i
i
i
2
i=1
i=1
!
n
n
X kÎ¾i j k2
X
X
1
Î±âˆ’1
Î»i
âŒ©Î¾ii , dWt âŒª +
Î»Î±âˆ’2
d[Î»i (t)] t
=Î±
dt + Î±(Î± âˆ’ 1)
i
Î»
âˆ’
Î»
2
i
j
i=1
i=1
j6=i
*
+
n
n
X
X
X
kÎ¾i j k2
1
Î±âˆ’1
Î±âˆ’2
2
Î±âˆ’1
=Î±
Î»i
dt + Î±(Î± âˆ’ 1)
Î»i kÎ¾ii k dt + Î±
Î»i Î¾ii , dWt
Î»i âˆ’ Î» j
2
i=1
i=1
i6= j
|
{z
}

=Î±

n
X

=:vt

Î»Î±âˆ’1 âˆ’ Î»Î±âˆ’1
j
2 i

=

1 X
Î±
kÎ¾i j k
2 i6= j

â‰¤

n
X
X
1
1
kÎ¾i j k2 (Î»i âˆ¨ Î» j )Î±âˆ’2 dt + Î±(Î± âˆ’ 1)
Î±(Î± âˆ’ 1)
Î»i (t)Î±âˆ’2 kÎ¾ii k2 dt + âŒ©vt , dWt âŒª
2
2
i=1
i6= j

=

n
n
X
X
1
Î±(Î± âˆ’ 1)
kÎ¾i j k2 (Î»i âˆ¨ Î» j )Î±âˆ’2 dt + âŒ©vt , dWt âŒª â‰¤ Î±2
kÎ¾i j k2 Î»Î±âˆ’2
dt + âŒ©vt , dWt âŒª,
i
2
i, j=1
i, j=1

Î»i âˆ’ Î» j

n
X

1
dt + Î±(Î± âˆ’ 1)
Î»i (t)Î±âˆ’2 kÎ¾ii k2 dt + âŒ©vt , dWt âŒª
2
i=1

where the ï¬rst inequality holds as
Î»Î±âˆ’1
âˆ’ Î»Î±âˆ’1
i
j
Î»i âˆ’ Î» j

= Î»Î±âˆ’2
+ Î»Î±âˆ’3
Î» j + Â· Â· Â· + Î»Î±âˆ’2
â‰¤ (Î± âˆ’ 1)(Î»i âˆ¨ Î» j )Î±âˆ’2 .
i
i
i

Thus, we have shown
dÎ¦Î± (t) â‰¤ Î±2

n
X

Î»i (t)Î±âˆ’2

i=1

where vt := Î±

n
X

kÎ¾i j k2 dt + âŒ©vt , dWt âŒª

(20)

j=1

Pn

Î±âˆ’1
Î¾ii .
i=1 Î»i

By recalling that our goal is to bound kA t kop from above (c.f. equation (12) and (14)), we may
assume without loss of generality that kA t kop â‰¥ 1. Thus, applying the reverse Cauchy-Schwarz
inequality to equation (20), we have
dÎ¦Î± (t) â‰¤ 2Î±2

n
X

Î»i (t)Î±âˆ’2

n
X
j=1

i=1

â‰¤ 2Î±2 kA t k2op

n
X

Î»i (t)Î±âˆ’2

n
X
i=1

Î»i (t)Î±

n
X

kÎ¾i j k2 dt + âŒ©vt , dWt âŒª

j=1

i=1

Â® 2Î±2

kÎ¾i j k2 dt + âŒ©vt , dWt âŒª

n
X

kÎ¾i j k2 dt + âŒ©vt , dWt âŒª.

j=1

18

Thus, deï¬ning K t := supi

Pn

2
j=1 kÎ¾i j k , we have the bound

dÎ¦Î± (t) Â® 2Î±2 K t Î¦Î± (t)dt + âŒ©vt , dWt âŒª.
3.4.3

(21)

Stopping the process early

As outlined in the beginning of this section, we will stop the process early in order to provide
a bound for the right hand side of equation (12). By observing equation (14), we hypothesize
that we should stop the process once kA t kop grows too large. As a result we deï¬ne the stopping
time
Ï„ := inf{t > 0 | kA t kop > 2} âˆ§ 1.
By the optional stopping theorem we have
ZÏ„
[M ]Ï„ =

d[M ] t â‰¤

â‰¤2
âˆ§n2
Z Ï„ zâ‰¤t âˆ’1
}| { z }| {

0

ZÏ„

â‰¤2

VarÂµ [Ï•] kA t kop dt

0

t

âˆ’1

Z1
t âˆ’1 âˆ§ n2 dt = 2 + 4 log n.

âˆ§ n dt â‰¤ 2
2

0

0

Combining this with equation (12), we obtain
VarÂµ [Ï•] â‰¤ 2 + 4 log n + E[Ï„âˆ’1 ],

(22)

and it remains to ï¬nd an upper bound for E[Ï„âˆ’1 ]. Observing that t < Ï„ whenever Î¦Î± (t) < 2Î± ,
we deï¬ne the Ïƒ the ï¬rst time for which the potential Î¦Î± (t) reaches 2Î± , namely
Ïƒ := inf{t > 0 | Î¦Î± (t) = 2Î± },
we have Ïƒâˆ’1 â‰¥ Ï„âˆ’1 and so it sufï¬ces to bound Ïƒ from below.
For simplicity, let us ignore the stochastic term in equation (21) and regard it as an ODE. Then,
by Gronwallâ€™s inequality, if we can ï¬nd some constant K such that K t â‰¤ K for all t â‰¤ Ï„, we have
the bound
2
S t â‰¤ ne2Î± K t .
Thus, substituting Ïƒ into the above, we have
2Î± = SÏƒ â‰¤ ne2Î± KÏƒ
2

implying

Î± log 2 âˆ’ log n
â‰¤ Ïƒ â‰¤ Ï„.
2Î±2 K
Then, taking Î± = 10K log n, it is easy to check that
Î± log 2 âˆ’ log n
1
â‰¤
10K log n
2Î±2 K
implying E[Ï„âˆ’1 ] â‰¤ 10K log n. Of course, this deduction only holds while ignoring the stochastic
term âŒ©vt , dWt âŒª. Nonetheless, this is justiï¬ed as one can show that kvt k2 is bounded Î±Î¦Î± (t) and
so the same analysis holds by applying the stochastic Gronwallâ€™s inequality (c.f. second part of
lemma 34 in [LV18]).
Finally, to ï¬nd a bound for (K t ), we employ the following lemma.
19

Maybe not
ignore the
martingale
term if we
want a complete proof

n
Lemma 3.4 (Lemma 1.6 in [Eld13]). Denoting CTS
as in theorem 2, there exists a constant C
such that for any log-concave, isotropic probability measure Âµ, we have
n
X

sup
Î¸ âˆˆS nâˆ’1 i, j=1

EX âˆ¼Âµ [âŒ©X , ei âŒªâŒ©X , e j âŒªâŒ©X , Î¸ âŒª]2 â‰¤ C

n
X
(C n )2
TS

k=1

k

,

where {e1 , Â· Â· Â· , en } is any orthonormal basis on Rn .
Recalling that

Î¾i j = EX +a t âˆ¼Âµ t [âŒ©ei , X âŠ—2 e j âŒªX ] = EX +a t âˆ¼Âµ t [âŒ©X , ei âŒªâŒ©X , e j âŒªX ],

we have by Parsevalâ€™s identity
K t = sup
i

n
X

i

j=1

= sup
i

n X
n
X

n
X

kEX +a t âˆ¼Âµ t [âŒ©X , ei âŒªâŒ©X , e j âŒªX ]k2

j=1

EX +at âˆ¼Âµ t [âŒ©X , ei âŒªâŒ©X , e j âŒªX ], ek

2

j=1 k=1
n
X

= sup
i

kÎ¾i j k = sup
2

EX +a t âˆ¼Âµ t [âŒ©X , ei âŒªâŒ©X , e j âŒªâŒ©X , ek âŒª]2

j,k=1

â‰¤ sup

n
X

Î¸ âˆˆS nâˆ’1 i, j=1

EX +at âˆ¼Âµ [âŒ©X , ei âŒªâŒ©X , e j âŒªâŒ©X , Î¸ âŒª]2 .

We note that we cannot direct apply lemma 3.4 at this point since the measure Âµ t might not
be isotropic. Hence, to be able to use the lemma, we need to normalize the covariance of Âµ t .
Namely, taking X + a t âˆ¼ Âµ t , we deï¬ne Y = Aâˆ’1/2 X which by construction is isotropic. Thus, by
observing that
EX +a t âˆ¼Âµ [âŒ©X , ei âŒªâŒ©X , e j âŒªâŒ©X , Î¸ âŒª]2 â‰¤ kA t k3op EX +a t âˆ¼Âµ [âŒ©Y, ei âŒªâŒ©Y, e j âŒªâŒ©Y, Î¸ âŒª]2 ,
we have
K t â‰¤ sup

Î¸ âˆˆS nâˆ’1

n
X

EX +a t âˆ¼Âµ [âŒ©X , ei âŒªâŒ©X , e j âŒªâŒ©X , Î¸ âŒª]2

i, j=1

â‰¤ kA t k3op

n
X

sup
Î¸ âˆˆS nâˆ’1 i, j=1

EX +a t âˆ¼Âµ [âŒ©Y, ei âŒªâŒ©Y, e j âŒªâŒ©Y, Î¸ âŒª] â‰¤ 8C
2

n
X
(C n )2

(23)

TS

k=1

k

where the last inequality follows as kA t kop â‰¤ 2 for all t < Ï„.
At last, combining equation (23) and (22), we have
ï£«
ï£¶
Î˜(log n)
z }| {
n
ï£¬
ï£·
X
1 n 2ï£·
ï£¬
n
VarÂµ [Ï•] â‰¤ 2 + log n ï£¬4 + 80C
log n)2 )
(CTS ) ï£· = Î˜n ((CTS
k
ï£­
ï£¸
k=1
implying there exists a constant R > 0 such that for all 1-Lipschitz Ï•,
n
n
n
i.e. Âµ is RCTS
log n-concentrated and so, Ccon
â‰¤ RCTS
log n as required.
20

Ã†

n
VarÂµ [Ï•] â‰¤ RCTS
log n,

4

Application: Markov Mixing

An application of stochastic localizations is used to prove mixing bounds for Markov processes.
Expanding on the work of Anari, Liu and Oveis in [ALG20], Chen and Eldan in [CE22] established a framework in which mixing bounds of a special class of Markov processes arises,
namely Markov chains associated with stochastic localizations. We will in this section present
this framework and describe its application to the Glauber dynamics.

4.1 Mixing bounds
The motivation for Markov mixing bounds fundamentally comes from sampling. Suppose we
wish to sample from some probability distribution Âµ. A common method to achieve this to
through the use of the Markov chain Monte Carlo (MCMC):
Theorem 5. Given (X n ) an irreducible positively recurrent homogenous Markov process on X
with stationary distribution Âµ, for any Ï• : X â†’ R integrable,
Z
n
1X
lim
Ï•(X k ) =
f dÂµ
nâ†’âˆ n
k=1
almost everywhere.
With this theorem in mind, MCMC allows us to sample Âµ by sampling from (X n ) instead. It is
in general not difï¬cult to come up with such Markov processes, although it is often difï¬cult to
show its rate of convergence. This motivates the notion of mixing bounds which quantiï¬es the
time for which the Markov process takes before its law is approximately stationary.
Deï¬nition 4.1 (Total variation mixing time). Given a probability measure Î½ âˆˆ M (X ), a Markov
kernel K with stationary distribution Âµ and some Îµ > 0, the Îµ-total variation mixing time is
deï¬ned as
t mix (P, Îµ, Âµ) := inf{t â‰¥ 0 | kP t Î½ âˆ’ ÂµkTV < Îµ},
Furthermore, we denote
t mix (P, Îµ) = sup t mix (P, Îµ, Î´ x )
xâˆˆX

the worst mixing time starting at a point.
A standard method of analyzing the mixing times of Markov chains is through the use of the
spectral gap.
Deï¬nition 4.2 (Spectral gap, [Lev17]). The spectral gap of a Markov kernel K is deï¬ned to be

gap(K) := 1 âˆ’ sup{Î» | Î» is an eigenvalue of K, Î» 6= 1}.
It is not difï¬cult to show that
R
Z
Ï•KÏ•dÂµ
1
gap(K) = inf 1 âˆ’ R
(Ï•(x) âˆ’ Ï•( y))2 K(x, d y)Âµ(dx),
= inf
2 dÂµ
Ï•:X
â†’R
Ï•:X
â†’R
2Var
[Ï•]
Ï•
Âµ
R

(24)

Ï•dÂµ=0

R
where Âµ is the stationary measure of K and KÏ• = Ï•( y)K(Â·, d y). We will take equation (24)
to be the deï¬ning property of the spectral gap in the case K is deï¬ned on a general state space.
21

Fix!

Theorem 6 ([Lev17]). Given a reversible and irreducible Markov chain with kernel K on the
state space X with stationary distribution Âµ, denoting Âµmin = inf xâˆˆX Âµ(x), we have
Â
Â
Â¡
Â‹
Â Â‹Â‹Â¤
1
1
1
1
t mix (K, Îµ) â‰¤
log
+ log
.
gap(K) 2
Âµmin
2Îµ
We remark that this inequality is only meaningful whenever Âµmin > 0 and thus, this theorem is
only meaningful for Markov chains on ï¬nite state space (and we can replace inf with min).
A similar bound can also be established using the modiï¬ed log-Sobolev inequality (MLSI).
Deï¬nition 4.3 (Entropy, [CLV20]). Given Ï• : X â†’ Râ‰¥0 and a measure Âµ, we deï¬ne the entropy
of Ï• with respect to Âµ to be


 Z
Z
Z

1
EntÂµ [Ï•] := EÂµ Ï• log
Ï• = Ï• log Ï•dÂµ âˆ’ Ï•dÂµ log
Ï•dÂµ .
EÂµ [Ï•]
Deï¬nition 4.4.
4.1.1

TODO

Ising model and Glauber dynamics

Deï¬nition 4.5 (Ising model). Given a graph G = (V, E), Î² > 0 and h âˆˆ R, the Ising model on
G with inverse temperature Î² and external ï¬eld h is the probability measure ÂµÎ²,h on {âˆ’1, 1}V
deï¬ned such that for all Ïƒ âˆˆ {âˆ’1, 1}V ,
(
)
X
X
1
ÂµÎ²,h (Ïƒ) := exp Î²
Ïƒx Ïƒ y + h
Ïƒx
Z
x yâˆˆE
xâˆˆV
where Z > 0 is the normalizing constant.
Heuristically, the Ising model measures the probability that a graph is in a speciï¬c conï¬guration
of up and down spins in which neighboring vertices are more likely to have the same spin. This
â€œlikelinessâ€ is controlled by Î² in which a larger Î² means that neighboring vertices are more likely
to align.
As illustrated by theorem 5, in order to sample from the Ising model, we can construct a Markov
chain which has the Ising model as its stationary distribution. One such Markov chain is known
as the Glauber dynamics.
Deï¬nition 4.6 (Glauber dynamics). Given a measure Âµ of {âˆ’1, 1}n , the Glauber dynamics of Âµ
is the Markov chain with kernel
X
Âµ(Ïƒ2 )
Âµ(Ïƒ1 )
1
1
K(Ïƒ1 , Ïƒ2 ) := 1{kÏƒ1 âˆ’Ïƒ2 k1 =1}
+
1
,
(25)
1
2
{Ïƒ =Ïƒ }
n Âµ(Ïƒ1 ) + Âµ(Ïƒ2 )
n kÏƒÌƒâˆ’Ïƒ1 k =1 Âµ(Ïƒ1 ) + Âµ(ÏƒÌƒ)
1

Pn
1

where we deï¬ne kÏƒ1 âˆ’ Ïƒ2 k1 := 2

1
2
1
2
n
i=1 |Ïƒi âˆ’ Ïƒi | for all Ïƒ , Ïƒ âˆˆ {âˆ’1, 1} .

Parsing this deï¬nition, we see that the Glauber dynamics is the Markov chain such that, starting
at a conï¬guration Ïƒ1 âˆˆ {âˆ’1, 1}n , the conï¬guration at the next time step either remains the same
or change at one vertex. Furthermore, the probability of this occurring is weighted according
to Âµ. As we hoped, the Glauber dynamics of the Ising model ÂµÎ²,h has ÂµÎ²,h as its stationary
distribution.
22

4.2 Dynamics of stochastic localizations
As alluded to previously, one may associate a Markov process at each time step of a stochastic localization process for which the original process is stationary. We will in this section deï¬ne these
Markov processes and show that the Glauber dynamics can be constructed using this method.
Deï¬nition 4.7 (Markov process associated with a stochastic localization, [CE22]). Let (Âµ t ) tâ‰¥0
be a stochastic localization of Âµ such that Âµ t is absolutely continuous (almost everywhere) with
respect to Âµ for all t. For all Ï„ > 0, we deï¬ne the dynamics associated with (Âµ t ) t at Ï„ to be the
Markov process with kernel
Â˜
Â•
dÂµÏ„
(x)ÂµÏ„ (A)
K(x, A) := EP
dÂµ
for all x âˆˆ X , A âˆˆ Î£.
As alluded by the notation, rather than a deterministic time Ï„, Ï„ can be taken to be an appropriate stopping time. In this case, the theorems below will remain to hold by invoking the optional
stopping theorem whenever necessary.
This is indeed a kernel since for each x âˆˆ X , as EP [ÂµÏ„ ] = Âµ by (L2),
Â•
Â˜
dÂµÏ„
dÂµ
d
K(x, â„¦) = EP
(x) =
EP [ÂµÏ„ ](x) =
(x) = 1
dÂµ
dÂµ
dÂµ
where the third equality follows by the uniqueness of the Radon-Nikodym derivative as for all
A âˆˆ Î£, we have
Z
Z

Â•
Â˜
dÂµÏ„
dÂµÏ„
EP
dÂµ = EP
dÂµ = EP [ÂµÏ„ ](A).
dÂµ
dÂµ
A
A
Proposition 4.1. The Markov process associated with a stochastic localization (Âµ t ) tâ‰¥0 of Âµ is
reversible and has stationary distribution Âµ.
Proof. Taking Ï• : X 2 â†’ R integrable, we have by Fubiniâ€™s theorem
Z
Z
Â•
Â˜
dÂµÏ„
Ï•(x, y)K(x, d y)Âµ(dx) = Ï•(x, y)EP
(x)ÂµÏ„ (d y) Âµ(dx)
dÂµ
X2

Z
dÂµÏ„
(x)Âµ(dx)
= EP
Ï•(x, y)ÂµÏ„ (d y)
dÂµ
Z

= EP

Ï•(x, y)ÂµÏ„ (d y)ÂµÏ„ (dx) .

Similarly, by the same calculation,
Z
X2

Thus,

Z

Ï•(x, y)K( y, dx)Âµ(d y) = EP

Z


Ï•(x, y)ÂµÏ„ (d y)ÂµÏ„ (dx) .

Z
Ï•(x, y)K(x, d y)Âµ(dx) =
X2

Ï•(x, y)K( y, dx)Âµ(d y)
X2

for any integrable Ï• : X 2 â†’ R implying K is reversible.
23

(26)

On the other hand, for all A âˆˆ Î£, we compute using the martingale property
Z
Z

Z

Â•
Â˜
dÂµÏ„
dÂµÏ„
KÂµ(A) = K(x, A)Âµ(dx) = EP
(x)ÂµÏ„ (A) = EP ÂµÏ„ (A)
dÂµ
dÂµ
dÂµ
= EP [ÂµÏ„ (A)ÂµÏ„ (â„¦)] = EP [ÂµÏ„ (A)] = Âµ(A)
implying Âµ is the stationary measure of K.
Proposition 4.2. Taking K to be the kernel of the Markov process associated with a stochastic
localization (Âµ t ) tâ‰¥0 of Âµ at time Ï„, we have

gap(K) =

inf

Ï•:X â†’R

E[VarÂµÏ„ [Ï•]]
VarÂµ [Ï•]

.

Proof. By equation (26) (where we take Ï•(x, y) = Ï•(x)Ï•( y)), we have
Â–Z
Z
2 Â™
X2

Ï•(x)Ï•( y)K(x, d y)Âµ(dx) = EP

X

Ï•dÂµÏ„

,

for any integrable Ï• : X â†’ R. On the other hand, we observe
Z
Z
Z
Ï•( y)2 K(x, d y)Âµ(dx) =

(KÏ• 2 )(x)Âµ(dx) =

Ï•(x)2 (KÂµ)(dx) =

(27)

Z
Ï•(x)2 Âµ(dx)

X2

as Âµ is the stationary measure of K. Thus, for any integrable Ï• : X â†’ R, by substituting the
above two equations, we have
Z
1
(Ï•(x) âˆ’ Ï•( y))2 K(x, d y)Âµ(dx)
2VarÂµ [Ï•] X 2
Z
Z
Z

1
=
Ï•(x)2 Âµ(dx) âˆ’ 2 Ï•(x)Ï•( y)K(x, d y)Âµ(dx) + Ï•( y)2 K(x, d y)Âµ(dx)
2VarÂµ [Ï•]
Â‚Z
Â–Z
2 Â™ÂŒ
1
2
=
Ï•dÂµÏ„
Ï• dÂµ âˆ’ EP
VarÂµ [Ï•]
X
Â–Z
Z
2 Â™
E[VarÂµÏ„ [Ï•]]
1
=
EP
Ï•dÂµÏ„
.
Ï• 2 dÂµÏ„ âˆ’
=
VarÂµ [Ï•]
VarÂµ [Ï•]
X
Hence, recalling the equivalent form of the spectral gap as described by (24), the result follows
by taking inï¬mum on both sides.
This proposition has a nice intuitive interpretation. By recalling that the limit of a stochastic
localization as t â†’ âˆ is a Dirac measure, we may imagine a stochastic localization zooms in
(in t) towards a region containing the massive point. Then, the spectral gap of the associated
Markov process at time Ï„ is simply the smallest proportion of the local variation around this
zoomed in region at time Ï„ to that of the global variation. This is achieved by a test function
with minimal variation within this region and maximal variation outside of it.

24

With regards to theorem 6, in the case Âµ has full support on the ï¬nite state space X (e.g. in the
setting of Glauber dynamics) the above proposition provides a method for computing an upper
bound for the mixing time. In particular, should the stochastic localization (Âµk )k satisfy
E[VarÂµk+1 [Ï•] | Âµ t ] â‰¥ (1 âˆ’ Îµ)VarÂµk [Ï•]
for given Îµ > 0 and any integrable function Ï• : X â†’ R, we have the telescoping product
Â– k
Â™
Y E[VarÂµ [Ï•] | Âµiâˆ’1 ]
E[VarÂµk [Ï•]]
i
=E
â‰¥ (1 âˆ’ Îµ)k .
VarÂµ [Ï•]
Var
[Ï•]
Âµ
iâˆ’1
i=1
Hence, we have the bound

gap(K)âˆ’1 â‰¤ (1 âˆ’ Îµ)âˆ’k

which immediately provides an upper bound for the mixing time of K in light of theorem 6. This
motivates the following deï¬nition.
Deï¬nition 4.8 (Approximate conservation of variance, [CE22]). A stochastic localization process (Âµk )k is said satisfy conserve (Îºk )-variance up to time n if for any integrable function
Ï• : X â†’ R, 1 â‰¤ k â‰¤ n,
E[VarÂµk+1 [Ï•] | Âµ t ] â‰¥ (1 âˆ’ Îºk )VarÂµk [Ï•].
By the same computation above, if (Âµk )kQconserves (Îºk )-variance up to time n, then it associated
n
dynamics has a spectral gap of at least i=1 (1 âˆ’ Îºi ).

4.3 Glauber dynamics as an associated Markov process
As alluded to in section 2, Glauber dynamics can be constructed as an associated Markov chain
of the coordinate by coordinate localization. Namely, taking Âµ âˆˆ M ({âˆ’1, 1}n ) and Ï„ = n âˆ’ 1,
we will show the associate Markov kernel


Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 )
E
= K(Ïƒ1 , Ïƒ2 )
Âµ(Ïƒ1 )
for all Ïƒ1 , Ïƒ2 âˆˆ {âˆ’1, 1}n where K(Ïƒ1 , Ïƒ2 ) as deï¬ned by equation (25).
First, taking Ïƒ1 , Ïƒ2 âˆˆ {âˆ’1, 1}n such that kÏƒ1 âˆ’ Ïƒ2 k = 1, say Ïƒ1 and Ïƒ2 differs at the m-th
coordinate, we have




n
Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 )
Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 )
1X
E
=
E
Y
=
k
.
n
Âµ(Ïƒ1 )
n k=1
Âµ(Ïƒ1 )
Now, if Yn = k 6= m, the conï¬guration is ï¬xed at the m-th coordinate. However, as Ïƒ1 and Ïƒ2
differs at the m-th coordinate, either Âµnâˆ’1 (Ïƒ1 ) = 0 or Âµnâˆ’1 (Ïƒ2 ) = 0. Thus, all but the m-th term
in the above sum vanishes and we have the kernel equals


Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 )
1
Yn = m
= E
n
Âµ(Ïƒ1 )


Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 )
1
= E
1{supp Âµnâˆ’1 ={Ïƒ1 ,Ïƒ2 }} Yn = m
n
Âµ(Ïƒ1 )


P(supp Âµnâˆ’1 = {Ïƒ1 , Ïƒ2 })
Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 )
1
2
=
E
Yn = m, supp Âµnâˆ’1 = {Ïƒ , Ïƒ }
n
Âµ(Ïƒ1 )

25

Then, substituting in the following equalities:
P(supp Âµnâˆ’1 = {Ïƒ1 , Ïƒ2 }) = Âµ(Ïƒ1 ) + Âµ(Ïƒ2 ),
and
E[Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 ) | Yn = m, supp Âµnâˆ’1 = {Ïƒ1 , Ïƒ2 }]
=E[Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 ) | supp Âµnâˆ’1 = {Ïƒ1 , Ïƒ2 }]
=

Âµ(Ïƒ1 )

Âµ(Ïƒ2 )

Âµ(Ïƒ1 ) + Âµ(Ïƒ2 ) Âµ(Ïƒ1 ) + Âµ(Ïƒ2 )

we have


Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 )
Âµ(Ïƒ1 ) + Âµ(Ïƒ2 )
Âµ(Ïƒ1 )
Âµ(Ïƒ2 )
Âµ(Ïƒ2 )
1
E
=
=
Âµ(Ïƒ1 )
nÂµ(Ïƒ1 )
Âµ(Ïƒ1 ) + Âµ(Ïƒ2 ) Âµ(Ïƒ1 ) + Âµ(Ïƒ2 )
n Âµ(Ïƒ1 ) + Âµ(Ïƒ2 )
which is precisely the kernel of the Glauber dynamics of two neighboring conï¬gurations. On the
other hand, if kÏƒ1 âˆ’ Ïƒ2 k > 1, then


Âµnâˆ’1 (Ïƒ1 )Âµnâˆ’1 (Ïƒ2 )
E
=0
Âµ(Ïƒ1 )
as Âµnâˆ’1 have ï¬xed all but one coordinate. Thus, the dynamics associated with the coordinate is
precisely the Glauber P
dynamics (we donâ€™t need to check the case Ïƒ1 = Ïƒ2 as they are kernels
and so K(Ïƒ, Ïƒ) = 1 âˆ’ ÏƒÌƒ6=Ïƒ K(Ïƒ, ÏƒÌƒ)).

26

References
[ALG20] Nima Anari, Kuikui Liu, and Shayan Oveis Gharan. Spectral independence in highdimensional expanders and applications to the hardcore model, 2020.
[BL76]

Herm Jan Brascamp and Elliott H Lieb. On extensions of the brunn-minkowski and
prÃ©kopa-leindler theorems, including inequalities for log concave functions, and with
an application to the diffusion equation. Journal of Functional Analysis, 22(4):366â€“
389, 1976.

[Bl05]

Viktor BlÃ¥sjÃ¶. The isoperimetric problem.
112(6):526â€“566, 2005.

[CE22]

Yuansi Chen and Ronen Eldan. Localization schemes: A framework for proving mixing
bounds for markov chains, 2022.

The American Mathematical Monthly,

[Che20] Yuansi Chen. An almost constant lower bound of the isoperimetric coefï¬cient in the
kls conjecture, 2020.
[CLV20] Zongchen Chen, Kuikui Liu, and Eric Vigoda. Optimal mixing of glauber dynamics:
Entropy factorization via high-dimensional expansion, 2020.
[Eld13] Ronen Eldan. Thin shell implies spectral gap up to polylog via a stochastic localization
scheme. Geometric and Functional Analysis, 23(2):532â€“569, mar 2013.
[Eld18] Ronen Eldan. Lecture notes - from stochastic calculus to geometric inequalities, 2018.
[Lev17] David Asher Levin. Markov chains and mixing times / David A. Levin, Yuval Peres ; with
contributions by Elizabeth L. Wilmer. American Mathematical Society, second edition.
edition, 2017.
[LV16]

Yin Tat Lee and Santosh S. Vempala. Eldanâ€™s stochastic localization and the kls conjecture: Isoperimetry, concentration and mixing, 2016.

[LV18]

Yin Tat Lee and Santosh S. Vempala. The kannan-lovÃ¡sz-simonovits conjecture, 2018.

[Mil08] Emanuel Milman. Uniform tail-decay of lipschitz functions implies cheegerâ€™s isoperimetric inequality under convexity assumptions. Comptes Rendus Mathematique,
346(17):989â€“994, 2008.
[Mil18] Emanuel Milman. On the role of convexity in isoperimetry, spectral-gap and concentration. 2018.
[Ã˜ks03] Bernt Ã˜ksendal. Stochastic Differential Equations, pages 65â€“84. Springer Berlin Heidelberg, Berlin, Heidelberg, 2003.

27

