In this section, we will take a look at an application of the stochastic localization technic 
to prove a version of the log-Sobolev inequality for log-concave measures. The log-Sobolev inequality 
is a class of inequalities central to the concentration of measures and has applications to bounding 
Markov mixing times (c.f.\todo{TODO}). We will in particular\dots

\subsection{Entropy and log-Sobolev inequalities}

Heuristically, similar to that of the variance, the entropy of a random variable is a measure of its 
uncertainty or randomness. Formally, the entropy is defined as the following.

\begin{definition}[Entropy]
  Given \(\phi : \mathcal{X} \to \mathbb{R}_{\ge 0}\) and a measure \(\mu\), we define the entropy of \(\phi\) 
  with respect to \(\mu\) to be 
  \[\text{Ent}_\mu[\phi] := \mathbb{E}_\mu\left[\phi \log\left(\frac{1}{\mathbb{E}_\mu[\phi]} \phi\right)\right]
   = \int \phi \log \phi \dd \mu - \int \phi \dd \mu \log\left(\int \phi \dd \mu\right)\]
  with the convention that \(0\log 0 = 0\).
\end{definition}
The log-Sobolev inequality can be then formulated as the following.
\begin{definition}[Log-Sobolev inequality, \cite{Lee_2016}]
  For a given measure \(\mu\) on \(\mathbb{R}^n\), \(\mu\) is said to satisfy the log-Sobolev inequality with log-Sobolev 
  constant \(\rho_\mu\) if \(\rho_\mu\) is the largest \(\rho\) such that for all smooth 
  \(\phi : \mathbb{R}^n \to \mathbb{R}\) with \(\int \phi^2 \dd \mu = 1\), we have 
  \[\frac{\rho}{2} \text{Ent}_\mu[\phi^2] \le \mathbb{E}_\mu[\|\nabla \phi\|^2] = \int \|\nabla \phi\|^2 \dd \mu.\]
\end{definition}

The log-Sobolev inequality\todo{Motivation and Herbst.}
% https://faculty.math.illinois.edu/~psdey/math595fa19/lec14.pdf

We will in this section present a bound by Lee and Vempala for the log-Sobolev constant for all log-concave measures. 
Namely, we consider the following theorem. 

\begin{theorem}[\cite{Lee_2016}]
  For any isotropic log-concave measure \(\mu\) on \(\mathbb{R}^n\) with support on a ball of diameter \(D\), 
  \(\mu\) has log-Sobolev constant \(\rho_\mu \gtrsim D^{-1}\).
\end{theorem}

Before presenting Lee and Vempala's proof of above theorem, let us first remark the similarity of the 
log-Sobolev inequality with that of the Poincaré inequality. As a result, one might first attempt to analyze 
the log-Sobolev inequality using the same method as prescribed by section~\ref{sec:KLS_to_TS}. As it turns out,
this method quickly runs into issues as one struggles to bound the entropy of the martingale terms resulted 
from the localization. \todo{Fix.}

Similar to before, for any smooth function \(\phi : \mathbb{R}^n \to \mathbb{R}\), 
I claim that one can express the entropy of \(\phi^2\) with respect to \(\mu\) as 
the entropy of the martingale terms, in particular, 
\begin{equation}\label{eq:entropy_decomp}
  \text{Ent}_\mu[\phi^2] = \text{Ent}[M_t] + \mathbb{E}[\text{Ent}_{\mu_t}[\phi^2]],
\end{equation}
Indeed, given the smooth function \(\phi : \mathbb{R}^n \to \mathbb{R}\), let us define the martingale 
\(M_t := \int \phi^2 \dd \mu_t\) where \((\mu_t)\) is the linear-tilt localization described in section~\ref{sec:construct}. 
We observe
\begin{align*}
  \text{Ent}_\mu[\phi^2] & = \text{Ent}[M_\infty] 
    = \mathbb{E}[M_\infty \log M_\infty] - \mathbb{E}[M_\infty]\log(\mathbb{E}[M_\infty]) \\
  & = \mathbb{E}[\mathbb{E}[M_\infty \log M_\infty \mid \mu_t]] 
      - \mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]]\log(\mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]]) \\
  & = \mathbb{E}[\text{Ent}[M_\infty \mid \mu_t]] + \mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]\log(\mathbb{E}[M_\infty \mid \mu_t])]\\ 
  & \hspace{1cm} - \mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]]\log(\mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]])\\
  & = \mathbb{E}[\text{Ent}[M_\infty \mid \mu_t]] + \mathbb{E}[M_t \log M_t] - \mathbb{E}[M_t]\log(\mathbb{E}[M_t])\\
  & = \mathbb{E}[\text{Ent}[M_\infty \mid \mu_t]] + \text{Ent}[M_t]
\end{align*}
Moreover, defining the martingale \(N_t := \int \phi^2\log \phi^2 \dd \mu_t\), we observe
\begin{align*}
  \text{Ent}[M_\infty \mid \mu_t] 
  & = \mathbb{E}[M_\infty \log M_\infty \mid \mu_t] - \mathbb{E}[M_\infty \mid \mu_t]\log(\mathbb{E}[M_\infty \mid \mu_t]) \\
  & = \mathbb{E}[N_\infty \mid \mu_t] - M_t \log M_t = N_t - M_t \log M_t\\
  & = \mathbb{E}_{\mu_t}[\phi^2 \log \phi^2] - \mathbb{E}_{\mu_t}[\phi^2] \log \mathbb{E}_{\mu_t}[\phi^2] \\
  & = \text{Ent}_{\mu_t}[\phi^2].
\end{align*}
Hence, we recover equation~\eqref{eq:entropy_decomp} by combining the above two equations.

At this point, we recall that in the Poincaré case, we proceeded to bound the two terms on the 
right hand side by using proposition~\ref{prop:linear-tilt_ACV} and the Brascamp-Lieb inequality respectively. 
In this case, the term \(\mathbb{E}[\text{Ent}_{\mu_t}[\phi^2]]\) can be bounded using the following lemma.
% Unfortunately, we do not have a version of the Brascamp-Lieb inequality for entropy for general log-concave measures.
% Instead, additional restrictions on the measure is required to obtain a log-Sobolev inequality.

\begin{lemma}[Euqation 2.17, \cite{Ledoux_1999}]
  Let \(U : \mathbb{R}^n \to \mathbb{R}\) be a twice differentiable function such that
  there exists some \(c > 0\) so that \(U''(x) \ge c\text{id}_n\) for any \(x \in \mathbb{R}^n\). 
  Then, for all smooth functions
  \(\phi : \mathbb{R}^n \to \mathbb{R}\), we have 
  \[\text{Ent}_\nu[\phi^2] \le \frac{2}{c}\mathbb{E}_\nu[\|\nabla\phi\|^2]\]
  where \(\dd \nu = e^{-U} \dd \text{Leb}^n\) and \(U''\) denotes the Hessian of \(U\).
\end{lemma}

Recalling equation~\eqref{eq:stoch_loc_alt}, by substituting \(U(x) = - z_t - \langle v_t, x\rangle + \frac{t}{2}\|x\|^2 + V(x)\)
we observe that \(U''(x) \ge \frac{t}{2}\text{Id}_n + V''(x) \ge \frac{t}{2}\text{Id}_n\) as \(V\) is convex.
Thus, by the above lemma, as \(\dd \mu_t = e^{-U} \dd \text{Leb}^n\), \(\mu_t\) satisfies the log-Sobolev 
inequality of the form 
\[\text{Ent}_{\mu_t}[\phi^2] \le 4t^{-1} \mathbb{E}_{\mu_t}[\|\nabla \phi\|^2].\] 
