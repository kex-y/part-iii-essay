In this section, we will take a look at an application of the stochastic localization technic 
to prove a version of the log-Sobolev inequality for log-concave measures. The log-Sobolev inequality 
is a class of inequalities central to the concentration of measures and has applications to bounding 
Markov mixing times (c.f.\todo{TODO}). We will in particular\dots

\subsection{Entropy and log-Sobolev inequalities}

Heuristically, similar to that of the variance, the entropy of a random variable is a measure of its 
uncertainty or randomness. Formally, the entropy is defined as the following.

\begin{definition}[Entropy]
  Given \(\phi : \mathcal{X} \to \mathbb{R}_{\ge 0}\) and a measure \(\mu\), we define the entropy of \(\phi\) 
  with respect to \(\mu\) to be 
  \[\text{Ent}_\mu[\phi] := \mathbb{E}_\mu\left[\phi \log\left(\frac{1}{\mathbb{E}_\mu[\phi]} \phi\right)\right]
   = \int \phi \log \phi \dd \mu - \int \phi \dd \mu \log\left(\int \phi \dd \mu\right)\]
  with the convention that \(0\log 0 = 0\).
\end{definition}
The log-Sobolev inequality can be then formulated as the following.
\begin{definition}[Log-Sobolev inequality, \cite{Lee_2016}]
  For a given measure \(\mu\) on \(\mathbb{R}^n\), \(\mu\) is said to satisfy the log-Sobolev inequality with log-Sobolev 
  constant \(\rho_\mu\) if \(\rho_\mu\) is the largest \(\rho\) such that for all smooth 
  \(\phi : \mathbb{R}^n \to \mathbb{R}\) with \(\int \phi^2 \dd \mu = 1\), we have 
  \[\frac{\rho}{2} \text{Ent}_\mu[\phi^2] \le \mathbb{E}_\mu[\|\nabla \phi\|^2] = \int \|\nabla \phi\|^2 \dd \mu.\]
\end{definition}

The log-Sobolev inequality\todo{Motivation and Herbst.}
% https://faculty.math.illinois.edu/~psdey/math595fa19/lec14.pdf


We observe the similarity of the log-Sobolev inequality with that of the Poincaré inequality. Remarkably, 
a similar inequality to the Cheeger's inequality can be defined and a correspondence similar to the correspondence 
between the Poincaré constant and the Cheeger's constant can be made for the log-Sobolev constant and the 
log-Cheeger's constant.

\begin{definition}[Log-Cheeger's inequality]
  Given a probability measure \(\mu\) on \(\mbox{R}^n\), we say \(\mu\) satisfies the log-Cheeger's inequality 
  if there exists some \(C\) such that for all \(A\) with \(\mu(A) \le 1 / 2\),
  \[C\mu(A)\sqrt{-\log(\mu(A))} \le \mu^+(A).\]
  We call the least such \(C\) the log-Cheeger's constant and denote it by \(C_\mu\).
\end{definition}

In general, one can show that \(\rho_\mu \simeq C_\mu\). However, in contrast to the Poincaré inequality where 
the KLS conjecture suggests that the Poincaré constant is bounded below by a universal constant, it is known 
that the log-Sobolev constant cannot be bounded below by a universal constant. 

\subsection{Log-Sobolev inequality via stochastic localization}

Inspired by the proof of the reduction of the KLS conjecture to the thin-shell conjecture, we will in this section 
present a modified proof of a bound by Lee and Vempala for the log-Sobolev constant for all log-concave measures. 
Namely, we consider the following theorem. 

\begin{theorem}[\cite{Lee_2016}]
  For any isotropic log-concave measure \(\mu\) on \(\mathbb{R}^n\) with support on a ball of diameter \(D\), 
  \(\mu\) has log-Sobolev constant \(\rho_\mu \gtrsim D^{-1}\).
\end{theorem}

We attempt to analyze the log-Sobolev inequality using the same method as prescribed by section~\ref{sec:KLS_to_TS}. 
As it turns out, while the initial steps can be follow verbatim, this method runs into issues when trying to 
bound the entropy of the martingale term resulted from the localization. As a result, instead of attempting 
bound the entropy using the operator norm of the covariance matrix as we had done so previously, we will 
to this end utilize the correspondence between the log-Sobolev constant and the log-Chegger's constant.

Let \(\phi : \mathbb{R}^n \to \mathbb{R}\) be smooth.  
Similar to before, we express the entropy of \(\phi^2\) with respect to \(\mu\) as 
the entropy of the martingale terms, in particular, we will show that
\begin{equation}\label{eq:entropy_decomp}
  \text{Ent}_\mu[\phi^2] = \text{Ent}[M_t] + \mathbb{E}[\text{Ent}_{\mu_t}[\phi^2]],
\end{equation}
Indeed, given the smooth function \(\phi : \mathbb{R}^n \to \mathbb{R}\), let us define the martingale 
\(M_t := \int \phi^2 \dd \mu_t\) where \((\mu_t)\) is the linear-tilt localization described in section~\ref{sec:construct}. 
We observe
\begin{align*}
  \text{Ent}_\mu[\phi^2] & = \text{Ent}[M_\infty] 
    = \mathbb{E}[M_\infty \log M_\infty] - \mathbb{E}[M_\infty]\log(\mathbb{E}[M_\infty]) \\
  & = \mathbb{E}[\mathbb{E}[M_\infty \log M_\infty \mid \mu_t]] 
      - \mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]]\log(\mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]]) \\
  & = \mathbb{E}[\text{Ent}[M_\infty \mid \mu_t]] + \mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]\log(\mathbb{E}[M_\infty \mid \mu_t])]\\ 
  & \hspace{1cm} - \mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]]\log(\mathbb{E}[\mathbb{E}[M_\infty \mid \mu_t]])\\
  & = \mathbb{E}[\text{Ent}[M_\infty \mid \mu_t]] + \mathbb{E}[M_t \log M_t] - \mathbb{E}[M_t]\log(\mathbb{E}[M_t])\\
  & = \mathbb{E}[\text{Ent}[M_\infty \mid \mu_t]] + \text{Ent}[M_t]
\end{align*}
Moreover, defining the martingale \(N_t := \int \phi^2\log \phi^2 \dd \mu_t\), we observe
\begin{align*}
  \text{Ent}[M_\infty \mid \mu_t] 
  & = \mathbb{E}[M_\infty \log M_\infty \mid \mu_t] - \mathbb{E}[M_\infty \mid \mu_t]\log(\mathbb{E}[M_\infty \mid \mu_t]) \\
  & = \mathbb{E}[N_\infty \mid \mu_t] - M_t \log M_t = N_t - M_t \log M_t\\
  & = \mathbb{E}_{\mu_t}[\phi^2 \log \phi^2] - \mathbb{E}_{\mu_t}[\phi^2] \log \mathbb{E}_{\mu_t}[\phi^2] \\
  & = \text{Ent}_{\mu_t}[\phi^2].
\end{align*}
Hence, we recover equation~\eqref{eq:entropy_decomp} by combining the above two equations.

At this point, we recall that in the reduction of the KLS conjecture, we proceeded to bound the two terms on the 
right hand side by using proposition~\ref{prop:linear-tilt_ACV} and the Brascamp-Lieb inequality respectively. 
In this case, the term \(\mathbb{E}[\text{Ent}_{\mu_t}[\phi^2]]\) can be bounded using the following lemma.
% Unfortunately, we do not have a version of the Brascamp-Lieb inequality for entropy for general log-concave measures.
% Instead, additional restrictions on the measure is required to obtain a log-Sobolev inequality.

\begin{lemma}[Euqation 2.17, \cite{Ledoux_1999}]
  Let \(U : \mathbb{R}^n \to \mathbb{R}\) be a twice differentiable function such that
  there exists some \(c > 0\) so that \(U''(x) \ge c\text{id}_n\) for any \(x \in \mathbb{R}^n\). 
  Then, for all smooth functions
  \(\phi : \mathbb{R}^n \to \mathbb{R}\), we have 
  \[\text{Ent}_\nu[\phi^2] \le \frac{2}{c}\mathbb{E}_\nu[\|\nabla\phi\|^2]\]
  where \(\dd \nu = e^{-U} \dd \text{Leb}^n\) and \(U''\) denotes the Hessian of \(U\).
\end{lemma}

Recalling equation~\eqref{eq:stoch_loc_alt}, by substituting \(U(x) = - z_t - \langle v_t, x\rangle + \frac{t}{2}\|x\|^2 + V(x)\)
we observe that \(U''(x) \ge \frac{t}{2}\text{Id}_n + V''(x) \ge \frac{t}{2}\text{Id}_n\) as \(V\) is convex.
Thus, by the above lemma, as \(\dd \mu_t = e^{-U} \dd \text{Leb}^n\), \(\mu_t\) satisfies the log-Sobolev 
inequality of the form
\begin{equation}
  \text{Ent}_{\mu_t}[\phi^2] \le 4t^{-1} \mathbb{E}_{\mu_t}[\|\nabla \phi\|^2].
\end{equation} 
On the other hand, it is difficult to bound \(\text{Ent}[M_t]\) by the operator norm of the covariance 
matrix. In contrast to bounding the variance, we cannot express the entropy as a neat expression of 
the quadratic variation of \((M_t)\). Instead, we will bound the log-Chegger's constant of \(M_t\).

