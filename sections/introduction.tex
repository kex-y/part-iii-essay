The notion of stochastic localization was first introduced in the 2013 paper by Eldan \cite{Eldan_2013}
in order to make progress regarding an isoperimetric problem known as the 
\textit{Kannan-Lov√°sz-Simonovitz} (KLS) conjecture. As it turns out, stochastic localization has been also 
useful in many other adjacent areas, in particular, in sampling and Markov mixing. This essay will 
provide an introduction to stochastic localization and describe its applications in Markov mixing and 
the KLS conjecture. Furthermore, using stochastic localization, this essay will also provide an alternative 
proof of a known bound for the log-Sobolev constant of log-concave measures. 

Stochastic localization in its most general form describes a sequence of random measures (random variables 
taking values in the space of measures) which begins at a given measure and converges to dirac 
measures almost everywhere, namely it ``localizes'', and moreover, satisfy a certain martingale 
condition. These sequences of random measures are useful in studying specific measures. Namely, 
by evolving the stochastic localization in time, particular properties of the structure collapses 
allowing us say something about them. On the other hand, the martingale property allows us to preserve 
these properties (possibly up to some time). Hence, by balancing the two, i.e. allowing the sequence 
to evolve so it is close to a dirac measure while not evolve too long such that we lose the properties 
of the original measure, we are able to obtain results about the original measure.

In our case, we will mostly focus on one specific type of stochastic localization known as the 
linear-tilt localization. The linear-tilt localization is a special case of stochastic localizations 
in which at each time step, the measure is ``tilted'' in a random direction. This random direction 
can be chosen in a variety of ways however one choice of interest is when the random direction is 
chosen according to a Wiener process. A stochastic localization constructed this way accumulates a 
Gaussian component which becomes more and more significant as the process evolves. This is particularly 
helpful as Gaussian measures are well understood and we can use properties of the Gaussian to obtain 
results about our original measure.
\subsection{Structure of this essay}

This essay consists of four sections. We now give a brief overview of them.

\begin{itemize}
  \item The first section of this essay consists of setting up the theory of stochastic localization.
    We also introduce some specific examples of stochastic localizations which will be used in subsequent 
    sections most importantly the discrete and continuous linear-tilt localization. 
  \item We then move on to discuss the application of the stochastic localization to bounding Markov 
    mixing time. Motivated by the Markov chain Monte Carlo, the Markov mixing time provides a quantitative 
    measure on how fast a Markov chain converges to its stationary distribution. It turns out that 
    the mixing time of Markov chains which kernels can be described using stochastic localizations 
    is related to how the variance of the stochastic localization evolves. This method is then applied  
    to the Glauber dynamics on the Boolean hypercube to obtain a mixing time bound to the Ising model. 
  \item We also discuss a proof of Eldan's original 2013 result reducing the KLS conjecture to another seemingly 
    weaker conjecture known as the thin-shell conjecture. Roughly speaking, the KLS conjecture asserts 
    that the class of measures known as the \textit{isotropic log-concave} measures are concentrated in the similar 
    way to the Gaussian. While we shall mostly consider the KLS conjecture using the language 
    of concentrations, it can be equivalently phrased with a more geometric point of view, that the 
    maximum proportion of volume by surface area of a log-concave measure is bounded by a universal constant. 
    As a result, the KLS conjecture has many important consequences in convex geometry. 
    In particular, by noticing that any uniform measure on a convex body of unit volume is log-concave, 
    the KLS conjecture also implies the Bourgain slicing conjecture which asserts that any convex body 
    of unit volume in \(\mathbb{R}^n\) has a intersection with some hyperplane with volume bounded below 
    by a universal constant. Moreover, the KLS conjecture has applications in bounding mixing times for 
    ball walks (c.f. \cite{Lee_2016}), the central limit theorem for convex bodies (c.f. \cite{Giannopoulos}) 
    and directly implies the aforementioned thin-shell conjecture.
  \item Finally, inspired by the proof from the third section, we will use stochastic localization methods to 
    provide a bound for the log-Sobolev constant of log-concave measures.
\end{itemize}

